{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35864c39",
   "metadata": {},
   "source": [
    "# Train DL\n",
    "\n",
    "> Deep neural nets for PSSM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35be57ea",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "**Utilities**\n",
    "\n",
    "`seed_everything(seed=123)` — Sets random seeds across Python, NumPy, and PyTorch for reproducibility. Ensures deterministic behavior on CUDA.\n",
    "\n",
    "```python\n",
    "seed_everything(\n",
    "    seed=42,  # random seed for reproducibility\n",
    ")\n",
    "```\n",
    "\n",
    "`init_weights(m, leaky=0.)` — Applies Kaiming initialization to Conv layers. Pass to `model.apply()` for weight initialization.\n",
    "\n",
    "```python\n",
    "model = CNN1D(ni=1024, nf=230).apply(\n",
    "    init_weights,  # initializes Conv layers with Kaiming normal\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Layer Builders**\n",
    "\n",
    "`lin_wn(ni, nf, dp=0.1, act=nn.SiLU)` — Creates a weight-normalized linear layer with BatchNorm, Dropout, and activation.\n",
    "\n",
    "```python\n",
    "layer = lin_wn(\n",
    "    ni=1024,       # input features\n",
    "    nf=512,        # output features  \n",
    "    dp=0.1,        # dropout probability\n",
    "    act=nn.SiLU,   # activation function (None to disable)\n",
    ")\n",
    "```\n",
    "\n",
    "`conv_wn(ni, nf, ks=3, stride=1, padding=1, dp=0.1, act=nn.ReLU)` — Creates a weight-normalized 1D convolution with BatchNorm, Dropout, and activation.\n",
    "\n",
    "```python\n",
    "layer = conv_wn(\n",
    "    ni=256,        # input channels\n",
    "    nf=512,        # output channels\n",
    "    ks=5,          # kernel size\n",
    "    stride=1,      # stride\n",
    "    padding=2,     # padding\n",
    "    dp=0.1,        # dropout probability\n",
    "    act=nn.ReLU,   # activation function\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Models**\n",
    "\n",
    "`MLP(num_features, num_targets, hidden_units=[512, 218], dp=0.2)` — Builds a multi-layer perceptron with BatchNorm and PReLU activations.\n",
    "\n",
    "```python\n",
    "model = MLP(\n",
    "    num_features=1024,          # input dimension (e.g., T5 embeddings)\n",
    "    num_targets=230,            # output dimension (23 AA × 10 positions)\n",
    "    hidden_units=[512, 256],    # list of hidden layer sizes\n",
    "    dp=0.2,                     # dropout rate (currently commented out)\n",
    ")\n",
    "```\n",
    "\n",
    "`CNN1D(ni, nf, amp_scale=16)` — 1D CNN that amplifies input features, applies convolutions with skip connections, then projects to output.\n",
    "\n",
    "```python\n",
    "model = CNN1D(\n",
    "    ni=1024,        # input features\n",
    "    nf=230,         # output features (flattened PSSM)\n",
    "    amp_scale=16,   # amplification factor for feature expansion\n",
    ").apply(init_weights)\n",
    "```\n",
    "\n",
    "`PSSM_model(n_features, n_targets, model='MLP')` — Wrapper that reshapes flat output to `(batch, 23, positions)` PSSM format with softmax-ready logits.\n",
    "\n",
    "```python\n",
    "model = PSSM_model(\n",
    "    n_features=1024,   # input feature dimension\n",
    "    n_targets=230,     # total targets (must be divisible by 23)\n",
    "    model='CNN',       # 'MLP' or 'CNN' architecture\n",
    ")\n",
    "# Output shape: (batch, 23, 10) for 10 positions\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Dataset**\n",
    "\n",
    "`GeneralDataset(df, feat_col, target_col=None, A=23, dtype=np.float32)` — PyTorch Dataset that extracts features and reshapes targets to `(23, L)` PSSM matrices.\n",
    "\n",
    "```python\n",
    "ds = GeneralDataset(\n",
    "    df=train_df,           # DataFrame with features and targets\n",
    "    feat_col=feat_col,     # Index/list of feature column names\n",
    "    target_col=target_col, # Index/list of target columns (None for test mode)\n",
    "    A=23,                  # number of amino acids (including pS, pT, pY)\n",
    "    dtype=np.float32,      # data type for tensors\n",
    ")\n",
    "# Returns (X, y) where y.shape = (23, L)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Loss Function**\n",
    "\n",
    "`CE(logits, target_probs)` — Cross-entropy loss with soft labels. Applies log_softmax to logits and computes against target probability distributions.\n",
    "\n",
    "```python\n",
    "loss = CE(\n",
    "    logits=model_output,      # (B, 23, 10) raw logits\n",
    "    target_probs=target_pssm, # (B, 23, 10) target probabilities (sum to 1 per position)\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Metrics**\n",
    "\n",
    "`KLD(logits, target_probs)` — Kullback-Leibler divergence between target distribution (p) and predicted softmax distribution (q).\n",
    "\n",
    "```python\n",
    "kl_div = KLD(\n",
    "    logits=model_output,      # (B, 23, 10) raw logits\n",
    "    target_probs=target_pssm, # (B, 23, 10) target probabilities\n",
    ")\n",
    "```\n",
    "\n",
    "`JSD(logits, target_probs)` — Jensen-Shannon divergence (symmetric metric) between target and predicted distributions.\n",
    "\n",
    "```python\n",
    "js_div = JSD(\n",
    "    logits=model_output,      # (B, 23, 10) raw logits  \n",
    "    target_probs=target_pssm, # (B, 23, 10) target probabilities\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Training**\n",
    "\n",
    "`train_dl(df, feat_col, target_col, split, model_func, ...)` — Trains a model on a single train/valid split using fastai's `Learner` with one-cycle policy.\n",
    "\n",
    "```python\n",
    "target, pred = train_dl(\n",
    "    df=df,                     # full DataFrame\n",
    "    feat_col=feat_col,         # feature column names\n",
    "    target_col=target_col,     # target column names\n",
    "    split=split0,              # (train_idx, valid_idx) tuple\n",
    "    model_func=get_cnn,        # callable returning fresh model\n",
    "    n_epoch=10,                # number of training epochs\n",
    "    bs=32,                     # batch size\n",
    "    lr=3e-3,                   # learning rate\n",
    "    loss=CE,                   # loss function\n",
    "    save='my_model',           # save to models/my_model.pth\n",
    "    sampler=None,              # optional custom sampler\n",
    "    lr_find=True,              # run lr_find before training\n",
    ")\n",
    "# Returns (target_df, pred_df) for validation set\n",
    "```\n",
    "\n",
    "`train_dl_cv(df, feat_col, target_col, splits, model_func, save=None, **kwargs)` — Cross-validation wrapper that trains across multiple folds and concatenates OOF predictions.\n",
    "\n",
    "```python\n",
    "oof = train_dl_cv(\n",
    "    df=df,                     # full DataFrame\n",
    "    feat_col=feat_col,         # feature column names\n",
    "    target_col=target_col,     # target column names\n",
    "    splits=splits,             # list of (train_idx, valid_idx) tuples\n",
    "    model_func=get_cnn,        # callable returning fresh model\n",
    "    save='cnn',                # saves as cnn_fold0.pth, cnn_fold1.pth, ...\n",
    "    n_epoch=10,                # passed to train_dl\n",
    "    lr=3e-3,                   # passed to train_dl\n",
    ")\n",
    "# Returns DataFrame with all OOF predictions + 'nfold' column\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Prediction**\n",
    "\n",
    "`predict_dl(df, feat_col, target_col, model_func, model_pth)` — Loads a saved model and generates predictions for a DataFrame.\n",
    "\n",
    "```python\n",
    "preds = predict_dl(\n",
    "    df=test_df,                # DataFrame to predict\n",
    "    feat_col=feat_col,         # feature column names\n",
    "    target_col=target_col,     # used for output column names\n",
    "    model_func=get_cnn,        # must match saved architecture\n",
    "    model_pth='cnn_fold0',     # model name (without .pth)\n",
    ")\n",
    "# Returns DataFrame with softmax probabilities, same shape as target_col\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6c25e4",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a005c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf7f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import fastcore.all as fc\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "import os, random\n",
    "from katlas.data import *\n",
    "from katlas.train import *\n",
    "from katlas.pssm import *\n",
    "from fastai.vision.all import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb999f91",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b727ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def seed_everything(seed=123):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6b1688",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c815c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def_device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f927bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e645e330",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511102a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_parquet('paper/kinase_domain/train/pspa_t5.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd555c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# info=Data.get_kinase_info()\n",
    "\n",
    "# info = info[info.pseudo=='0']\n",
    "\n",
    "# info = info[info.kd_ID.notna()]\n",
    "\n",
    "# subfamily_map = info[['kd_ID','subfamily']].drop_duplicates().set_index('kd_ID')['subfamily']\n",
    "\n",
    "# pspa_info = pd.DataFrame(df.index.tolist(),columns=['kinase'])\n",
    "\n",
    "# pspa_info['subfamily'] = pspa_info.kinase.map(subfamily_map)\n",
    "\n",
    "# splits = get_splits(pspa_info, group='subfamily',nfold=5)\n",
    "\n",
    "# split0 = splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6f9b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f7c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f63679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # column name of feature and target\n",
    "# feat_col = df.columns[df.columns.str.startswith('T5_')]\n",
    "# target_col = df.columns[~df.columns.isin(feat_col)][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d7b6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d02a9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41753042",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e04ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GeneralDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 df,\n",
    "                 feat_col,            # list/Index of feature columns (e.g., 100 cols)\n",
    "                 target_col=None,     # list/Index of flattened PSSM cols; AA-first; A=23\n",
    "                 A: int = 23,\n",
    "                 dtype=np.float32):\n",
    "        \"\"\"\n",
    "        If target_col is None -> test mode, returns only X.\n",
    "        Otherwise returns (X, y) where y has shape (23, L), L inferred from target columns.\n",
    "        \"\"\"\n",
    "        self.test = target_col is None\n",
    "        self.aa = A\n",
    "\n",
    "        # Features\n",
    "        self.X = df[feat_col].to_numpy(dtype=dtype, copy=True)\n",
    "\n",
    "        self.y = None\n",
    "        if not self.test:\n",
    "            y_flat = df[target_col].to_numpy(dtype=dtype, copy=True)\n",
    "\n",
    "            total = y_flat.shape[1]\n",
    "            if total % A != 0:\n",
    "                raise ValueError(f\"Target columns ({total}) not divisible by A={A}; cannot infer L.\")\n",
    "            self.position = total // self.aa\n",
    "\n",
    "            # AA-first: reshape to (N, 23, L)\n",
    "            self.y = y_flat.reshape(-1, A, self.position) # reshape from row-major flatten\n",
    "            # if column-major as pandas.unstack is column major\n",
    "            # self.y = y_flat.reshape(-1, self.position,self.aa).transpose(0, 2, 1) \n",
    "\n",
    "        self.len = len(df)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = torch.from_numpy(self.X[index])        # (feat_dim,)\n",
    "        if self.test: return X\n",
    "        y = torch.from_numpy(self.y[index])        # (23, L)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a0f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataset\n",
    "# ds = GeneralDataset(df,feat_col,target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d762e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc795061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl = DataLoader(ds, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071bec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xb,yb = next(iter(dl))\n",
    "\n",
    "# xb.shape,yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35781600",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fe5fae",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9028ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def MLP(num_features, \n",
    "          num_targets,\n",
    "          hidden_units = [512, 218],\n",
    "          dp = 0.2):\n",
    "    \n",
    "    # Start with the first layer from num_features to the first hidden layer\n",
    "    layers = [\n",
    "        nn.Linear(num_features, hidden_units[0]),\n",
    "        nn.BatchNorm1d(hidden_units[0]),\n",
    "        # nn.Dropout(dp),\n",
    "        nn.PReLU()\n",
    "    ]\n",
    "    \n",
    "    # Loop over hidden units to create intermediate layers\n",
    "    for i in range(len(hidden_units) - 1):\n",
    "        layers.extend([\n",
    "            nn.Linear(hidden_units[i], hidden_units[i+1]),\n",
    "            nn.BatchNorm1d(hidden_units[i+1]),\n",
    "            # nn.Dropout(dp),\n",
    "            nn.PReLU()\n",
    "        ])\n",
    "    \n",
    "    # Add the output layer\n",
    "    layers.append(nn.Linear(hidden_units[-1], num_targets))\n",
    "    \n",
    "    model = nn.Sequential(*layers)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4091f767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_feature = len(feat_col)\n",
    "# n_target = len(target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056dfb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MLP(n_feature, n_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4fcd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c2993a",
   "metadata": {},
   "source": [
    "### CNN1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89e8451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lin_wn(ni,nf,dp=0.1,act=nn.SiLU):\n",
    "    \"Weight norm of linear.\"\n",
    "    layers = [\n",
    "            nn.BatchNorm1d(ni),\n",
    "            nn.Dropout(dp),\n",
    "            nn.utils.parametrizations.weight_norm(nn.Linear(ni, nf)) \n",
    "    ]\n",
    "    if act: layers.append(act())\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8852fbb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (1): Dropout(p=0.1, inplace=False)\n",
       "  (2): ParametrizedLinear(\n",
       "    in_features=10, out_features=3, bias=True\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): _WeightNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): SiLU()\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_wn(10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0ebce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def conv_wn(ni, nf, ks=3, stride=1, padding=1, dp=0.1,act=nn.ReLU):\n",
    "    \"Weight norm of conv.\"\n",
    "    layers = [\n",
    "        nn.BatchNorm1d(ni),\n",
    "        nn.Dropout(dp),\n",
    "        nn.utils.parametrizations.weight_norm(nn.Conv1d(ni, nf, ks, stride, padding)) \n",
    "        ]\n",
    "    if act: layers.append(act())\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc8b5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CNN1D(nn.Module):\n",
    "    \n",
    "    def __init__(self, ni, nf, amp_scale = 16):\n",
    "        super().__init__()\n",
    "\n",
    "        cha_1,cha_2,cha_3 = 256,512,512\n",
    "        hidden_size = cha_1*amp_scale\n",
    "\n",
    "        cha_po_1 = hidden_size//(cha_1*2)\n",
    "        cha_po_2 = (hidden_size//(cha_1*4)) * cha_3\n",
    "        \n",
    "        self.lin = lin_wn(ni,hidden_size)\n",
    "        \n",
    "        # bs, 256, 16\n",
    "        self.view = View(-1,cha_1,amp_scale)\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            conv_wn(cha_1, cha_2, ks=5, stride=1, padding=2, dp=0.1),\n",
    "            nn.AdaptiveAvgPool1d(output_size = cha_po_1),\n",
    "            conv_wn(cha_2, cha_2, ks=3, stride=1, padding=1, dp=0.1))\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            conv_wn(cha_2, cha_2, ks=3, stride=1, padding=1, dp=0.3),\n",
    "            conv_wn(cha_2, cha_3, ks=5, stride=1, padding=2, dp=0.2))\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=4, stride=2, padding=1),\n",
    "            nn.Flatten(),\n",
    "            lin_wn(cha_po_2,nf,act=None) )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # amplify features to 4096\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        # reshape to bs,256,16 for conv1d\n",
    "        x = self.view(x) \n",
    "\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x_s = x  # for skip connection (multiply)\n",
    "        x = self.conv2(x)\n",
    "        x = x * x_s\n",
    "\n",
    "        # Final block\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4de267",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def init_weights(m, leaky=0.):\n",
    "    \"Initiate any Conv layer with Kaiming norm.\"\n",
    "    if isinstance(m, (nn.Conv1d,nn.Conv2d,nn.Conv3d)): nn.init.kaiming_normal_(m.weight, a=leaky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eba767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CNN1D(n_feature,n_target).apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0819455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(xb).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac8902d",
   "metadata": {},
   "source": [
    "## Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c6097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PSSM_model(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_features,\n",
    "                 n_targets,\n",
    "                 model='MLP'):\n",
    "        super().__init__()\n",
    "        self.n_features=n_features\n",
    "        self.n_targets=n_targets\n",
    "        self.n_aa = 23\n",
    "        if self.n_targets % self.n_aa != 0: raise ValueError(f\"n_targets ({n_targets}) must be divisible by n_aa ({self.n_aa}).\")\n",
    "        self.n_positions = self.n_targets//self.n_aa\n",
    "        \n",
    "        if model =='MLP': self.model=MLP(self.n_features, self.n_targets)\n",
    "        elif model =='CNN': self.model=CNN1D(self.n_features, self.n_targets).apply(init_weights)\n",
    "        else: raise ValueError('model must be MLP or CNN.')\n",
    "    def forward(self,x):\n",
    "        logits = self.model(x).reshape(-1, self.n_aa,self.n_positions)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd7d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PSSM_model(n_feature,n_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1134414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits= model(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42731e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4963181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_mlp(): return PSSM_model(n_feature,n_target,model='MLP')\n",
    "\n",
    "# def get_cnn(): return PSSM_model(n_feature,n_target,model='CNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdbdf86",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a837a709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def CE(logits: torch.Tensor,\n",
    "       target_probs: torch.Tensor,\n",
    "      ):\n",
    "    \"\"\"\n",
    "    Cross-entropy with soft labels.\n",
    "    logits:       (B, 20, 10)\n",
    "    target_probs: (B, 20, 10), each column (over AA) sums to 1\n",
    "    \"\"\"\n",
    "    logp = F.log_softmax(logits, dim=1)              # (B, 20, 10)\n",
    "    ce   = -(target_probs * logp).sum(dim=1)         # (B, 10)\n",
    "    return ce.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c42c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CE(logits,yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b2390f",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f678ccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def KLD(logits: torch.Tensor,\n",
    "          target_probs: torch.Tensor,\n",
    "         ):\n",
    "    \"\"\"\n",
    "    Averaged KL divergence across positions between target_probs (p) and softmax(logits) (q).\n",
    "    \n",
    "    logits:       (B, 20, 10)\n",
    "    target_probs: (B, 20, 10), each column (over AA) sums to 1\n",
    "    \"\"\"\n",
    "    logq = F.log_softmax(logits, dim=1)    # log q(x)\n",
    "    logp = torch.log(target_probs + 1e-8) # log p(x), safe for zeros\n",
    "    kl   = (target_probs * (logp - logq)).sum(dim=1)   # (B, 10)\n",
    "    return kl.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85970bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KLD(logits,yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc24bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def JSD(logits: torch.Tensor,\n",
    "        target_probs: torch.Tensor,\n",
    "       ):\n",
    "    \"\"\"\n",
    "    Averaged Jensen-Shannon Divergence across positions between target_probs (p) and softmax(logits) (q).\n",
    "\n",
    "    logits:       (B, 20, 10)\n",
    "    target_probs: (B, 20, 10), each column (over AA) sums to 1\n",
    "    \"\"\"\n",
    "    # p, q distributions\n",
    "    q = F.softmax(logits, dim=1)                # q(x)\n",
    "    p = target_probs\n",
    "    m = 0.5 * (p + q)                           # midpoint distribution\n",
    "\n",
    "    # logs (with epsilon for stability)\n",
    "    logp = torch.log(p + 1e-8)\n",
    "    logq = torch.log(q + 1e-8)\n",
    "    logm = torch.log(m + 1e-8)\n",
    "\n",
    "    # KL(p||m) and KL(q||m)\n",
    "    kld_pm = (p * (logp - logm)).sum(dim=1)\n",
    "    kld_qm = (q * (logq - logm)).sum(dim=1)\n",
    "\n",
    "    jsd = 0.5 * (kld_pm + kld_qm)               # (B, 10)\n",
    "    return jsd.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c55e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSD(logits,yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6da9c31",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8a1c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_dl(df, \n",
    "            feat_col, \n",
    "            target_col,\n",
    "            split, # tuple of numpy array for split index\n",
    "            model_func, # function to get pytorch model\n",
    "             n_epoch = 4, # number of epochs\n",
    "             bs = 32, # batch size\n",
    "             lr = 1e-2, # will be useless if lr_find is True\n",
    "            loss = CE, # loss function\n",
    "            save = None, # models/{save}.pth\n",
    "             sampler = None,\n",
    "             lr_find=False, # if true, will use lr from lr_find\n",
    "              ):\n",
    "    \"A DL trainer.\"\n",
    "    \n",
    "    train = df.loc[split[0]]\n",
    "    valid = df.loc[split[1]]\n",
    "    \n",
    "    train_ds = GeneralDataset(train, feat_col, target_col)\n",
    "    valid_ds = GeneralDataset(valid, feat_col, target_col)\n",
    "    \n",
    "    dls = DataLoaders.from_dsets(train_ds, valid_ds, bs=bs, num_workers=min(fc.defaults.cpus, 4))\n",
    "\n",
    "    model = model_func()\n",
    "    learn = Learner(dls.to(def_device), model.to(def_device), loss, \n",
    "                    metrics= [KLD,JSD]\n",
    "                    # cbs = [GradientClip(1.0)] # prevent overfitting\n",
    "                   )\n",
    "    \n",
    "    if lr_find:\n",
    "        # get learning rate\n",
    "        lr = learn.lr_find()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        print(lr)\n",
    "\n",
    "        \n",
    "    print('lr in training is', lr)\n",
    "    learn.fit_one_cycle(n_epoch,lr) #cbs = [SaveModelCallback(fname = 'best')] # save best model\n",
    "    \n",
    "    if save is not None:\n",
    "        learn.save(save)\n",
    "        \n",
    "    pred,target = learn.get_preds()\n",
    "\n",
    "    # row first\n",
    "    pred  = F.softmax(pred, dim=1).reshape(len(valid),-1)\n",
    "    target = target.reshape(len(valid),-1)\n",
    "\n",
    "    # column first\n",
    "    # pred  = F.softmax(pred, dim=1).permute(0, 2, 1).reshape(len(valid),-1)\n",
    "    # target = target.permute(0, 2, 1).reshape(len(valid),-1)\n",
    "    \n",
    "    pred = pd.DataFrame(pred.detach().cpu().numpy(),index=valid.index,columns=target_col)\n",
    "    target = pd.DataFrame(target.detach().cpu().numpy(),index=valid.index,columns=target_col)\n",
    "    \n",
    "    return target, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83faca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target, pred = train_dl(df, \n",
    "#                         feat_col, \n",
    "#                         target_col,\n",
    "#                         split0, \n",
    "#                         model_func=get_cnn,\n",
    "#                         n_epoch=1,\n",
    "#                         lr = 3e-3,\n",
    "#                         lr_find=True,\n",
    "#                         save = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f39b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d245ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_pssm = recover_pssm(pred.iloc[0])\n",
    "# pred_pssm.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b4256c",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce0d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def predict_dl(df, \n",
    "               feat_col, \n",
    "               target_col,\n",
    "               model_func, # model architecture\n",
    "               model_pth, # only name, not with .pth\n",
    "              ):\n",
    "    \n",
    "    \"Predict dataframe given a deep learning model\"\n",
    "    \n",
    "    test_dset = GeneralDataset(df,feat_col)\n",
    "    test_dl = DataLoader(test_dset,bs=512)\n",
    "    \n",
    "    model = model_func()\n",
    "    \n",
    "    learn = Learner(None, model.to(def_device), loss_func=1)\n",
    "    learn.load(model_pth,weights_only=False)\n",
    "    \n",
    "    learn.model.eval()\n",
    "    \n",
    "    preds = []\n",
    "    for data in test_dl:\n",
    "        inputs = data.to(def_device)\n",
    "        pred = learn.model(inputs)\n",
    "\n",
    "        pred  = F.softmax(pred, dim=1).reshape(len(pred),-1)\n",
    "        # pred  = F.softmax(pred, dim=1).permute(0, 2, 1).reshape(len(pred),-1)\n",
    "\n",
    "        preds.append(pred.detach().cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "    preds = pd.DataFrame(preds,index=df.index,columns=target_col)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f74e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = df.loc[split0[1]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134e5a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pred = predict_dl(test, \n",
    "#                feat_col, \n",
    "#                target_col,\n",
    "#                model_func=get_cnn, # model architecture\n",
    "#                model_pth='test', # only name, not with .pth\n",
    "#               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a9be04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pred.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1114785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pssm_pred = recover_pssm(test_pred.iloc[0])\n",
    "# pssm_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2221dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_heatmap(pssm_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ba94c2",
   "metadata": {},
   "source": [
    "## CV train\n",
    "> cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8132357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_dl_cv(df, \n",
    "                feat_col, \n",
    "                target_col, \n",
    "                splits, # list of tuples\n",
    "                model_func, # functions like lambda x: return MLP_1(num_feat, num_target)\n",
    "                save:str=None,\n",
    "                **kwargs\n",
    "                ):\n",
    "    \n",
    "    OOF = []\n",
    "    \n",
    "    for fold,split in enumerate(splits):\n",
    "\n",
    "        print(f'------fold{fold}------')\n",
    "        \n",
    "        fname = f'{save}_fold{fold}' if save is not None else None\n",
    "        \n",
    "        \n",
    "        # train model\n",
    "        target, pred = train_dl(df,feat_col,target_col, split, model_func ,save=fname,**kwargs)\n",
    "\n",
    "        pred['nfold'] = fold\n",
    "        OOF.append(pred)\n",
    "        \n",
    "\n",
    "    # Concatenate OOF from each fold to a new dataframe\n",
    "    oofs = pd.concat(OOF).sort_index()\n",
    "    \n",
    "    return oofs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7a0882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oof = train_dl_cv(df,feat_col,target_col,\n",
    "#                   splits = splits,\n",
    "#                   model_func = get_cnn,\n",
    "#                   n_epoch=1,lr=3e-3,save='cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b32c5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oof.nfold.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fbf97c",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8bd623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b514994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
