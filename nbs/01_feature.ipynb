{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "> In this module, we develop tools to extract features from compounds, proteins, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import sys\n",
    "sys.path.append(\"/notebooks/katlas\")\n",
    "from nbdev.showdoc import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from katlas.core import Data\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "import pandas as pd\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import Descriptors\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features from amino acid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDKit descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastbook import *\n",
    "from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP\n",
    "from fairscale.nn.wrap import enable_wrap, wrap\n",
    "import esm\n",
    "from tqdm.notebook import tqdm; tqdm.pandas()\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def smi2prop(df, # df needs to have SMILES an ID columns\n",
    "             smi_colname = \"SMILES\", # column name of smiles\n",
    "             id_colname = \"ID\", # column name of ID\n",
    "             remove_duplicate=False, # remove features that are highly correlated\n",
    "             thr = 0.95, # threshold of Pearson correlation\n",
    "             normalize = True, # normalize features using StandardScaler()\n",
    "            ):\n",
    "    \"Extract ~209 features from smiles via rdkit.Chem.Descriptors, and remove duplicate features\"\n",
    "    \n",
    "    mols = [Chem.MolFromSmiles(smi) for smi in df[smi_colname]]\n",
    "    desc_names = [desc_name[0] for desc_name in Descriptors.descList]\n",
    "    desc_calc = MoleculeDescriptors.MolecularDescriptorCalculator(desc_names)\n",
    "    desc_values = [desc_calc.CalcDescriptors(mol) for mol in mols]\n",
    "    feature_df = pd.DataFrame(np.stack(desc_values), index=df[id_colname],columns=desc_names)\n",
    "    if remove_duplicate:\n",
    "        # remove compound that has same value across features\n",
    "        # feature_df = feature_df.loc[feature_df.std(axis=1) != 0] \n",
    "        print(f'number of {feature_df.shape[1]} features are detected')\n",
    "        #femove features with zero std\n",
    "        feature_std = feature_df.std()\n",
    "        zero_std_features = np.where(feature_std == 0)[0]\n",
    "        to_drop = feature_df.columns[zero_std_features]\n",
    "        feature_df = feature_df.drop(columns=to_drop).copy()\n",
    "        print(f'dropping {len(to_drop)} features, as they have zero std variance:{to_drop.tolist()}')\n",
    "        \n",
    "        corr_matrix = feature_df.corr().abs()\n",
    "        # Select upper triangle of correlation matrix\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        # Find index of feature columns with correlation greater than a threshold (e.g., 0.95)\n",
    "        to_drop = [column for column in upper.columns if any(upper[column] > thr)]\n",
    "        # Drop the highly correlated features \n",
    "        feature_df = feature_df.drop(to_drop, axis=1).copy()\n",
    "        print(f'dropping {len(to_drop)} features, as they have Pearson corr > {thr}:{to_drop}')\n",
    "        print(f'number of {feature_df.shape[1]} features are left')\n",
    "        \n",
    "\n",
    "    if normalize:\n",
    "        scaler = StandardScaler()\n",
    "        transformed = scaler.fit_transform(feature_df.iloc[:,1:])\n",
    "        feature_df.iloc[:,1:] = transformed\n",
    "        \n",
    "    feature_df = feature_df.reset_index()\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(smi2prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Data.get_aa_info()[['aa','SMILES']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature = smi2prop(df, id_colname='aa',remove_duplicate=True,thr=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_feature.to_csv('aa_feature.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morgan fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def smi2morgan(df, # a dataframe contains ID and SMILES columns\n",
    "               smi_colname = \"SMILES\", # set smiles columne name\n",
    "               id_colname = \"ID\", # set ID column name\n",
    "              ):\n",
    "    \"Like `smi2prop`, get 2048 morgan feature (0/1) given a dataframe that contains ID&smiles\"\n",
    "    mols = [Chem.MolFromSmiles(smi) for smi in df[smi_colname]]\n",
    "    morgan_fps = [AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048) for mol in mols]\n",
    "    fp_df = pd.DataFrame(np.array(morgan_fps), index=df[id_colname])\n",
    "    colnames = [f'morgan_{i}' for i in fp_df.columns]\n",
    "    fp_df.columns = colnames\n",
    "    fp_df = fp_df.reset_index()\n",
    "    return fp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(smi2morgan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features from protein sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def esm_embeddings(df: pd.DataFrame, \n",
    "                   seq_colname: str, #The name of the column containing the sequences.\n",
    "                   model_name: str = \"esm2_t33_650M_UR50D\", #The name of the ESM model to use for the embeddings.\n",
    "                  ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract 1280 esmfold2 embeddings from protein sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize distributed world with world_size 1\n",
    "    if not torch.distributed.is_initialized():\n",
    "        url = \"tcp://localhost:23456\"\n",
    "        torch.distributed.init_process_group(backend=\"nccl\", init_method=url, world_size=1, rank=0)\n",
    "    \n",
    "    #get number of repr layers\n",
    "    match = re.search(r'_t(\\d+)_', model_name)\n",
    "    number = int(match.group(1))\n",
    "    print(f\"repr_layers number for model {model_name} is {number}.\")\n",
    "    print(\"You can also choose other esm2 models:\",\n",
    "          \"\\nesm2_t48_15B_UR50D\\nesm2_t36_3B_UR50D\\nesm2_t33_650M_UR50D\\nesm2_t30_150M_UR50D\\nesm2_t12_35M_UR50D\\nesm2_t6_8M_UR50D\\n\")\n",
    "\n",
    "    # Download model data from the hub\n",
    "    model_data, regression_data = esm.pretrained._download_model_and_regression_data(model_name)\n",
    "\n",
    "    # Initialize the model with FSDP wrapper\n",
    "    fsdp_params = dict(\n",
    "        mixed_precision=True,\n",
    "        flatten_parameters=True,\n",
    "        state_dict_device=torch.device(\"cpu\"),  # reduce GPU mem usage\n",
    "        cpu_offload=True,  # enable cpu offloading\n",
    "    )\n",
    "\n",
    "    with enable_wrap(wrapper_cls=FSDP, **fsdp_params):\n",
    "        model, vocab = esm.pretrained.load_model_and_alphabet_core(\n",
    "            model_name, model_data, regression_data\n",
    "        )\n",
    "        batch_converter = vocab.get_batch_converter()\n",
    "        model.eval()\n",
    "\n",
    "        # Wrap each layer in FSDP separately\n",
    "        for name, child in model.named_children():\n",
    "            if name == \"layers\":\n",
    "                for layer_name, layer in child.named_children():\n",
    "                    wrapped_layer = wrap(layer)\n",
    "                    setattr(child, layer_name, wrapped_layer)\n",
    "        model = wrap(model)\n",
    "\n",
    "        # Define the feature extraction function\n",
    "        def get_feature(r, colname=seq_colname) -> np.ndarray:\n",
    "            data = [('protein', r[colname])]\n",
    "            labels, strs, tokens = batch_converter(data)\n",
    "            with torch.no_grad():\n",
    "                results = model(tokens.cuda(), repr_layers=[number], return_contacts=False)\n",
    "            rpr = results[\"representations\"][number].squeeze()\n",
    "            rpr = rpr[1 : len(r[colname]) + 1].mean(0).detach().cpu().numpy()\n",
    "\n",
    "            del results, labels, strs, tokens, data #especially need to delete those on cuda: tokens, results\n",
    "            gc.collect()\n",
    "\n",
    "            return rpr\n",
    "        \n",
    "        # Apply the feature extraction function to each row in the DataFrame\n",
    "        series = df.progress_apply(get_feature, axis=1)\n",
    "        df_feature = pd.DataFrame(series.tolist())\n",
    "\n",
    "        return df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(esm_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Data.get_esm_pca32()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
