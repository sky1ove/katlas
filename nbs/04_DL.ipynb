{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DL\n",
    "\n",
    "> A collection of deep learning tools via Fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "import sys\n",
    "sys.path.append(\"/notebooks/katlas\")\n",
    "from nbdev.showdoc import *\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastbook import *\n",
    "import fastcore.all as fc,torch.nn.init as init\n",
    "from fastai.callback.training import GradientClip\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "# katlas\n",
    "from katlas.core import Data\n",
    "from katlas.feature import *\n",
    "from katlas.train import *\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import spearmanr,pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def seed_everything(seed=123):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def_device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=5, random_state=123, shuffle=True)\n",
      "# kinase group in train set: 9\n",
      "# kinase group in test set: 9\n",
      "---------------------------\n",
      "# kinase in train set: 312\n",
      "---------------------------\n",
      "# kinase in test set: 78\n",
      "---------------------------\n",
      "test set: ['EPHA3' 'FES' 'FLT3' 'FYN' 'EPHB1' 'EPHB3' 'FER' 'EPHB4' 'FLT4' 'FGFR1' 'EPHA5' 'TEK' 'DDR2' 'ZAP70' 'LIMK1' 'ULK3' 'JAK1' 'WEE1' 'TESK1' 'MAP2K3' 'AMPKA2' 'ATM' 'CAMK1D' 'CAMK2D' 'CAMK4' 'CAMKK1'\n",
      " 'CK1D' 'CK1E' 'DYRK2' 'DYRK4' 'HGK' 'IKKE' 'JNK2' 'JNK3' 'KHS1' 'MAPKAPK5' 'MEK2' 'MSK2' 'NDR1' 'NEK6' 'NEK9' 'NIM1' 'NLK' 'OSR1' 'P38A' 'P38B' 'P90RSK' 'PAK1' 'PERK' 'PKCH' 'PKCI' 'PKN1' 'ROCK2'\n",
      " 'RSK2' 'SIK' 'STLK3' 'TAK1' 'TSSK1' 'ALPHAK3' 'BMPR2' 'CDK10' 'CDK13' 'CDK14' 'CDKL5' 'GCN2' 'GRK4' 'IRE1' 'KHS2' 'MASTL' 'MLK4' 'MNK1' 'MRCKA' 'PRPK' 'QSK' 'SMMLCK' 'SSTK' 'ULK2' 'VRK1']\n"
     ]
    }
   ],
   "source": [
    "# read training data\n",
    "df = pd.read_parquet('train_data/combine_t5_kd.parquet').reset_index()\n",
    "\n",
    "# read data contains info for split\n",
    "info_df = Data.get_kinase_info().query('pseudo!=\"1\"') # get non-pseudo kinase\n",
    "\n",
    "# merge info with training data\n",
    "info = df[['kinase']].merge(info_df)\n",
    "info.head()\n",
    "\n",
    "# splits\n",
    "splits = get_splits(info,stratified='group')\n",
    "split0 = splits[0]\n",
    "\n",
    "\n",
    "# column name of feature and target\n",
    "feat_col = df.columns[df.columns.str.startswith('T5_')]\n",
    "target_col = df.columns[~df.columns.isin(feat_col)][1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GeneralDataset:\n",
    "    def __init__(self, \n",
    "                 df, # a dataframe of values\n",
    "                 feat_col, # feature columns\n",
    "                 target_col=None # Will return test set for prediction if target col is None\n",
    "                ):\n",
    "        \"A general dataset that can be applied to any dataframe\"\n",
    "        \n",
    "        self.test = False if target_col is not None else True\n",
    "        \n",
    "        self.X = df[feat_col].values \n",
    "        self.y = df[target_col].values if not self.test else None\n",
    "        \n",
    "        self.len = df.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = torch.Tensor(self.X[index])\n",
    "        if self.test:\n",
    "            return X\n",
    "        else:\n",
    "            y = torch.Tensor(self.y[index])\n",
    "            return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "ds = GeneralDataset(df,feat_col,target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_sampler(info,col):\n",
    "    \n",
    "    \"For imbalanced data, get higher weights for less-represented samples\"\n",
    "    \n",
    "    # get value counts\n",
    "    group_counts = info[col].value_counts()\n",
    "    \n",
    "    # to reduce the difference through log\n",
    "    # group_counts = group_counts.apply(lambda x: np.log(x+1.01))\n",
    "    \n",
    "    weights = 1. / group_counts[info[col]]\n",
    "\n",
    "    sample_weights = torch.from_numpy(weights.to_numpy())\n",
    "    sample_weights = torch.clamp_min(sample_weights,0.01)\n",
    "\n",
    "    sampler = WeightedRandomSampler(sample_weights, len(sample_weights),replacement=True)\n",
    "    \n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = get_sampler(info,'subfamily')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "dl = DataLoader(ds, batch_size=64, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1024]), torch.Size([64, 210]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb,yb = next(iter(dl))\n",
    "\n",
    "xb.shape,yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def MLP_1(num_features, \n",
    "          num_targets,\n",
    "          hidden_units = [512, 218],\n",
    "          dp = 0.2):\n",
    "    \n",
    "    # Start with the first layer from num_features to the first hidden layer\n",
    "    layers = [\n",
    "        nn.Linear(num_features, hidden_units[0]),\n",
    "        nn.BatchNorm1d(hidden_units[0]),\n",
    "        nn.Dropout(dp),\n",
    "        nn.PReLU()\n",
    "    ]\n",
    "    \n",
    "    # Loop over hidden units to create intermediate layers\n",
    "    for i in range(len(hidden_units) - 1):\n",
    "        layers.extend([\n",
    "            nn.Linear(hidden_units[i], hidden_units[i+1]),\n",
    "            nn.BatchNorm1d(hidden_units[i+1]),\n",
    "            nn.Dropout(dp),\n",
    "            nn.PReLU()\n",
    "        ])\n",
    "    \n",
    "    # Add the output layer\n",
    "    layers.append(nn.Linear(hidden_units[-1], num_targets))\n",
    "    \n",
    "    model = nn.Sequential(*layers)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feature = len(feat_col)\n",
    "n_target = len(target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_1(n_feature, n_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1115, -0.3755, -0.3818,  ..., -0.1483, -0.0387, -0.1111],\n",
       "        [ 0.8555,  0.9352, -0.9642,  ..., -0.4723,  0.7757, -0.0121],\n",
       "        [ 0.3422,  0.3537, -0.1441,  ...,  0.5467, -0.4535,  0.2103],\n",
       "        ...,\n",
       "        [-0.4287,  0.6751,  0.1797,  ...,  0.0192,  0.0692, -0.0573],\n",
       "        [-0.0206, -0.1953,  0.7445,  ..., -0.2206, -0.1188,  0.4579],\n",
       "        [ 0.2342, -0.0243,  0.4630,  ...,  0.8393,  0.5747, -0.6881]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Version 1***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CNN1D_1(Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_features, # this does not matter, just for format\n",
    "                 num_targets):\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=3, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=3, out_channels=8, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = nn.Linear(in_features = int(8 * num_features/4), out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=num_targets)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1) # need shape (bs, 1, num_features) for CNN\n",
    "        x = self.pool1(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool2(nn.functional.relu(self.conv2(x)))\n",
    "        # x = torch.flatten(x, 1)\n",
    "        x = self.flatten(x)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN1D_1(n_feature, n_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0193,  0.0690,  0.0138,  ..., -0.0428, -0.0026,  0.0840],\n",
       "        [ 0.0203,  0.0693,  0.0136,  ..., -0.0422, -0.0023,  0.0846],\n",
       "        [ 0.0198,  0.0703,  0.0148,  ..., -0.0424, -0.0029,  0.0839],\n",
       "        ...,\n",
       "        [ 0.0197,  0.0694,  0.0147,  ..., -0.0429, -0.0019,  0.0841],\n",
       "        [ 0.0193,  0.0687,  0.0146,  ..., -0.0429, -0.0017,  0.0843],\n",
       "        [ 0.0191,  0.0692,  0.0148,  ..., -0.0425, -0.0028,  0.0834]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Version 2***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def init_weights(m, leaky=0.):\n",
    "    \"Initiate any Conv layer with Kaiming norm.\"\n",
    "    if isinstance(m, (nn.Conv1d,nn.Conv2d,nn.Conv3d)): init.kaiming_normal_(m.weight, a=leaky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lin_wn(ni,nf,dp=0.1,act=nn.SiLU):\n",
    "    \"Weight norm of linear.\"\n",
    "    layers =  nn.Sequential(\n",
    "            nn.BatchNorm1d(ni),\n",
    "            nn.Dropout(dp),\n",
    "            nn.utils.weight_norm(nn.Linear(ni, nf)) )\n",
    "    if act: layers.append(act())\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def conv_wn(ni, nf, ks=3, stride=1, padding=1, dp=0.1,act=nn.ReLU):\n",
    "    \"Weight norm of conv.\"\n",
    "    layers =  nn.Sequential(\n",
    "        nn.BatchNorm1d(ni),\n",
    "        nn.Dropout(dp),\n",
    "        nn.utils.weight_norm(nn.Conv1d(ni, nf, ks, stride, padding)) )\n",
    "    if act: layers.append(act())\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CNN1D_2(nn.Module):\n",
    "    \n",
    "    def __init__(self, ni, nf, amp_scale = 16):\n",
    "        super().__init__()\n",
    "\n",
    "        cha_1,cha_2,cha_3 = 256,512,512\n",
    "        hidden_size = cha_1*amp_scale\n",
    "\n",
    "        cha_po_1 = hidden_size//(cha_1*2)\n",
    "        cha_po_2 = (hidden_size//(cha_1*4)) * cha_3\n",
    "        \n",
    "        self.lin = lin_wn(ni,hidden_size)\n",
    "        \n",
    "        # bs, 256, 16\n",
    "        self.view = View(-1,cha_1,amp_scale)\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            conv_wn(cha_1, cha_2, ks=5, stride=1, padding=2, dp=0.1),\n",
    "            nn.AdaptiveAvgPool1d(output_size = cha_po_1),\n",
    "            conv_wn(cha_2, cha_2, ks=3, stride=1, padding=1, dp=0.1))\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            conv_wn(cha_2, cha_2, ks=3, stride=1, padding=1, dp=0.3),\n",
    "            conv_wn(cha_2, cha_3, ks=5, stride=1, padding=2, dp=0.2))\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=4, stride=2, padding=1),\n",
    "            nn.Flatten(),\n",
    "            lin_wn(cha_po_2,nf,act=None) )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # amplify features to 4096\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        # reshape to bs,256,16 for conv1d\n",
    "        x = self.view(x) \n",
    "\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x_s = x  # for skip connection (multiply)\n",
    "        x = self.conv2(x)\n",
    "        x = x * x_s\n",
    "\n",
    "        # Final block\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN1D_2(n_feature,n_target).apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5740,  0.0151, -0.0819,  ...,  0.2636,  0.3405, -0.1404],\n",
       "        [-0.6800,  0.5530, -0.0958,  ..., -0.3752, -0.6124,  0.7171],\n",
       "        [ 0.4427, -0.3204, -0.3243,  ..., -0.2290,  0.1070,  0.1504],\n",
       "        ...,\n",
       "        [-0.3660, -0.2667, -0.6036,  ..., -0.3130,  0.5462, -0.0055],\n",
       "        [ 0.4511,  0.6824,  0.8659,  ..., -0.0171,  0.2362, -0.3475],\n",
       "        [-0.0746, -0.1699,  0.6895,  ...,  1.1522, -0.3472,  0.6422]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_dl(df, \n",
    "            feat_col, \n",
    "            target_col,\n",
    "            split, # tuple of numpy array for split index\n",
    "            model_func, # function to get pytorch model\n",
    "             n_epoch = 4, # number of epochs\n",
    "             bs = 32, # batch size\n",
    "             lr = 1e-2, # will be useless if lr_find is True\n",
    "            loss = mse, # loss function\n",
    "            save = None, # models/{save}.pth\n",
    "             sampler = None,\n",
    "             lr_find=False, # if true, will use lr from lr_find\n",
    "              ):\n",
    "    \"A DL trainer.\"\n",
    "    \n",
    "    train = df.loc[split[0]]\n",
    "    valid = df.loc[split[1]]\n",
    "    \n",
    "    train_ds = GeneralDataset(train, feat_col, target_col)\n",
    "    valid_ds = GeneralDataset(valid, feat_col, target_col)\n",
    "    \n",
    "    n_workers = fc.defaults.cpus\n",
    "\n",
    "    if sampler is not None:\n",
    "        \n",
    "        train_dl = DataLoader(train_ds, batch_size=bs, sampler=sampler,num_workers=n_workers)\n",
    "        valid_dl = DataLoader(valid_ds, batch_size=bs, sampler=sampler,num_workers=n_workers)\n",
    "        \n",
    "        dls = DataLoaders(train_dl, valid_dl)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        dls = DataLoaders.from_dsets(train_ds, valid_ds, bs=bs, num_workers=n_workers)\n",
    "    \n",
    "    model = model_func()\n",
    "    \n",
    "    learn = Learner(dls.to(def_device), model.to(def_device), loss, \n",
    "                    metrics= [PearsonCorrCoef(),SpearmanCorrCoef()],\n",
    "                    cbs = [GradientClip(1.0)] # prevent overfitting\n",
    "                   )\n",
    "    \n",
    "    if lr_find:\n",
    "        # get learning rate\n",
    "        lr = learn.lr_find()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        print(lr)\n",
    "\n",
    "        \n",
    "    print('lr in training is', lr)\n",
    "    learn.fit_one_cycle(n_epoch,lr) #cbs = [SaveModelCallback(fname = 'best')] # save best model\n",
    "    \n",
    "    if save is not None:\n",
    "        learn.save(save)\n",
    "        \n",
    "    pred,target = learn.get_preds()\n",
    "    \n",
    "    pred = pd.DataFrame(pred.detach().cpu().numpy(),index=valid.index,columns=target_col)\n",
    "    target = pd.DataFrame(target.detach().cpu().numpy(),index=valid.index,columns=target_col)\n",
    "    \n",
    "    return target, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    return CNN1D_2(n_feature, n_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr in training is 0.01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>pearsonr</th>\n",
       "      <th>spearmanr</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.194746</td>\n",
       "      <td>2.176958</td>\n",
       "      <td>-0.104578</td>\n",
       "      <td>-0.053141</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target, pred = train_dl(df, \n",
    "                        feat_col, \n",
    "                        target_col,\n",
    "                        split0, \n",
    "                        get_model,\n",
    "                        n_epoch=1,\n",
    "                        lr = 1e-2,\n",
    "                        save = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall MSE: 2.1770\n",
      "Average Pearson: 0.2149 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.176958,\n",
       " 0.21488270397776432,\n",
       "       Pearson\n",
       " 3   -0.442855\n",
       " 8   -0.490345\n",
       " 10  -0.401885\n",
       " 19  -0.428557\n",
       " 24  -0.383956\n",
       " ..        ...\n",
       " 359 -0.127000\n",
       " 361  0.005761\n",
       " 366  0.095977\n",
       " 367  0.335805\n",
       " 373 -0.230842\n",
       " \n",
       " [78 rows x 1 columns])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_each(target,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@fc.delegates(train_dl)\n",
    "def train_dl_cv(df, \n",
    "                feat_col, \n",
    "                target_col, \n",
    "                splits, # list of tuples\n",
    "                model_func, # functions like lambda x: return MLP_1(num_feat, num_target)\n",
    "                save:str=None,\n",
    "                **kwargs\n",
    "                ):\n",
    "    \n",
    "    OOF = []\n",
    "    metrics = []\n",
    "    \n",
    "    for fold,split in enumerate(splits):\n",
    "\n",
    "        print(f'------fold{fold}------')\n",
    "        \n",
    "        \n",
    "        fname=None\n",
    "        # save best model for each fold\n",
    "        if save is not None:\n",
    "            fname = f'{save}_fold{fold}'\n",
    "        \n",
    "        # train model\n",
    "        target, pred = train_dl(df,feat_col,target_col, split, model_func ,save=fname,**kwargs)\n",
    "\n",
    "        #------------get scores--------------\n",
    "        # get score metrics\n",
    "        mse, pearson_avg, _ = score_each(target,pred)\n",
    "        \n",
    "        # store metrics in a dictionary for the current fold\n",
    "        fold_metrics = {\n",
    "            'fold': fold,\n",
    "            'mse': mse,\n",
    "            'pearson_avg': pearson_avg\n",
    "        }\n",
    "        metrics.append(fold_metrics)\n",
    "\n",
    "        OOF.append(pred)\n",
    "        \n",
    "\n",
    "    # Concatenate OOF from each fold to a new dataframe\n",
    "    oof = pd.concat(OOF).sort_index()\n",
    "    \n",
    "    # Get metrics into a dataframe\n",
    "    metrics = pd.DataFrame(metrics)\n",
    "    \n",
    "    return oof, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    return CNN1D_2(n_feature, n_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------fold0------\n",
      "lr in training is 0.003\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>pearsonr</th>\n",
       "      <th>spearmanr</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.165076</td>\n",
       "      <td>0.997911</td>\n",
       "      <td>0.091948</td>\n",
       "      <td>0.058285</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall MSE: 0.9979\n",
      "Average Pearson: 0.1634 \n",
      "------fold1------\n",
      "lr in training is 0.003\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>pearsonr</th>\n",
       "      <th>spearmanr</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.180757</td>\n",
       "      <td>0.992539</td>\n",
       "      <td>0.102852</td>\n",
       "      <td>0.084205</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall MSE: 0.9925\n",
      "Average Pearson: 0.1617 \n",
      "------fold2------\n",
      "lr in training is 0.003\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>pearsonr</th>\n",
       "      <th>spearmanr</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.159264</td>\n",
       "      <td>0.987170</td>\n",
       "      <td>0.119972</td>\n",
       "      <td>0.098912</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall MSE: 0.9872\n",
      "Average Pearson: 0.2364 \n",
      "------fold3------\n",
      "lr in training is 0.003\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>pearsonr</th>\n",
       "      <th>spearmanr</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.184666</td>\n",
       "      <td>1.001829</td>\n",
       "      <td>0.077155</td>\n",
       "      <td>0.047876</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall MSE: 1.0018\n",
      "Average Pearson: 0.1415 \n",
      "------fold4------\n",
      "lr in training is 0.003\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>pearsonr</th>\n",
       "      <th>spearmanr</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.178444</td>\n",
       "      <td>0.992547</td>\n",
       "      <td>0.109969</td>\n",
       "      <td>0.100576</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall MSE: 0.9925\n",
      "Average Pearson: 0.2014 \n"
     ]
    }
   ],
   "source": [
    "oof,metrics = train_dl_cv(df,feat_col,target_col,splits,get_model,n_epoch=1,lr=3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>mse</th>\n",
       "      <th>pearson_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.997911</td>\n",
       "      <td>0.163423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.992539</td>\n",
       "      <td>0.161654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.987170</td>\n",
       "      <td>0.236363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.001829</td>\n",
       "      <td>0.141464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.992547</td>\n",
       "      <td>0.201375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fold       mse  pearson_avg\n",
       "0     0  0.997911     0.163423\n",
       "1     1  0.992539     0.161654\n",
       "2     2  0.987170     0.236363\n",
       "3     3  1.001829     0.141464\n",
       "4     4  0.992547     0.201375"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18085578818910147"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.pearson_avg.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall MSE: 0.9944\n",
      "Average Pearson: 0.1809 \n"
     ]
    }
   ],
   "source": [
    "target = df[target_col]\n",
    "_,_,corr = score_each(target,oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.183429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.178564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.225202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.117838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.153463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>0.109792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>0.269238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>0.079601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>0.063310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>0.343237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>390 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Pearson\n",
       "0   -0.183429\n",
       "1   -0.178564\n",
       "2   -0.225202\n",
       "3   -0.117838\n",
       "4   -0.153463\n",
       "..        ...\n",
       "385  0.109792\n",
       "386  0.269238\n",
       "387  0.079601\n",
       "388  0.063310\n",
       "389  0.343237\n",
       "\n",
       "[390 rows x 1 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def predict_dl(df, \n",
    "               feat_col, \n",
    "               target_col,\n",
    "               model, # model architecture\n",
    "               model_pth, # only name, not with .pth\n",
    "              ):\n",
    "    \n",
    "    \"Predict dataframe given a deep learning model\"\n",
    "    \n",
    "    test_dset = GeneralDataset(df,feat_col)\n",
    "    test_dl = DataLoader(test_dset,bs=512)\n",
    "    \n",
    "    \n",
    "    learn = Learner(None, model.to(def_device), loss_func=1)\n",
    "    learn.load(model_pth)\n",
    "    \n",
    "    learn.model.eval()\n",
    "    \n",
    "    preds = []\n",
    "    for data in test_dl:\n",
    "        inputs = data.to(def_device)\n",
    "        outputs = learn.model(inputs) #learn.model(x).sigmoid().detach().cpu().numpy()\n",
    "\n",
    "        preds.append(outputs.detach().cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "    preds = pd.DataFrame(preds,index=df.index,columns=target_col)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df.loc[split0[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-5P</th>\n",
       "      <th>-5G</th>\n",
       "      <th>-5A</th>\n",
       "      <th>-5C</th>\n",
       "      <th>-5S</th>\n",
       "      <th>-5T</th>\n",
       "      <th>-5V</th>\n",
       "      <th>-5I</th>\n",
       "      <th>-5L</th>\n",
       "      <th>-5M</th>\n",
       "      <th>-5F</th>\n",
       "      <th>-5Y</th>\n",
       "      <th>-5W</th>\n",
       "      <th>-5H</th>\n",
       "      <th>-5K</th>\n",
       "      <th>-5R</th>\n",
       "      <th>-5Q</th>\n",
       "      <th>-5N</th>\n",
       "      <th>-5D</th>\n",
       "      <th>-5E</th>\n",
       "      <th>-5s</th>\n",
       "      <th>-5t</th>\n",
       "      <th>-5y</th>\n",
       "      <th>-4P</th>\n",
       "      <th>-4G</th>\n",
       "      <th>-4A</th>\n",
       "      <th>-4C</th>\n",
       "      <th>-4S</th>\n",
       "      <th>-4T</th>\n",
       "      <th>-4V</th>\n",
       "      <th>-4I</th>\n",
       "      <th>-4L</th>\n",
       "      <th>-4M</th>\n",
       "      <th>-4F</th>\n",
       "      <th>-4Y</th>\n",
       "      <th>-4W</th>\n",
       "      <th>-4H</th>\n",
       "      <th>-4K</th>\n",
       "      <th>-4R</th>\n",
       "      <th>-4Q</th>\n",
       "      <th>-4N</th>\n",
       "      <th>-4D</th>\n",
       "      <th>-4E</th>\n",
       "      <th>-4s</th>\n",
       "      <th>-4t</th>\n",
       "      <th>-4y</th>\n",
       "      <th>-3P</th>\n",
       "      <th>-3G</th>\n",
       "      <th>-3A</th>\n",
       "      <th>-3C</th>\n",
       "      <th>-3S</th>\n",
       "      <th>-3T</th>\n",
       "      <th>-3V</th>\n",
       "      <th>-3I</th>\n",
       "      <th>-3L</th>\n",
       "      <th>-3M</th>\n",
       "      <th>-3F</th>\n",
       "      <th>-3Y</th>\n",
       "      <th>-3W</th>\n",
       "      <th>-3H</th>\n",
       "      <th>-3K</th>\n",
       "      <th>-3R</th>\n",
       "      <th>-3Q</th>\n",
       "      <th>-3N</th>\n",
       "      <th>-3D</th>\n",
       "      <th>-3E</th>\n",
       "      <th>-3s</th>\n",
       "      <th>-3t</th>\n",
       "      <th>-3y</th>\n",
       "      <th>-2P</th>\n",
       "      <th>-2G</th>\n",
       "      <th>-2A</th>\n",
       "      <th>-2C</th>\n",
       "      <th>-2S</th>\n",
       "      <th>-2T</th>\n",
       "      <th>-2V</th>\n",
       "      <th>-2I</th>\n",
       "      <th>-2L</th>\n",
       "      <th>-2M</th>\n",
       "      <th>-2F</th>\n",
       "      <th>-2Y</th>\n",
       "      <th>-2W</th>\n",
       "      <th>-2H</th>\n",
       "      <th>-2K</th>\n",
       "      <th>-2R</th>\n",
       "      <th>-2Q</th>\n",
       "      <th>-2N</th>\n",
       "      <th>-2D</th>\n",
       "      <th>-2E</th>\n",
       "      <th>-2s</th>\n",
       "      <th>-2t</th>\n",
       "      <th>-2y</th>\n",
       "      <th>-1P</th>\n",
       "      <th>-1G</th>\n",
       "      <th>-1A</th>\n",
       "      <th>-1C</th>\n",
       "      <th>-1S</th>\n",
       "      <th>-1T</th>\n",
       "      <th>-1V</th>\n",
       "      <th>-1I</th>\n",
       "      <th>-1L</th>\n",
       "      <th>-1M</th>\n",
       "      <th>-1F</th>\n",
       "      <th>-1Y</th>\n",
       "      <th>-1W</th>\n",
       "      <th>-1H</th>\n",
       "      <th>-1K</th>\n",
       "      <th>-1R</th>\n",
       "      <th>-1Q</th>\n",
       "      <th>-1N</th>\n",
       "      <th>-1D</th>\n",
       "      <th>-1E</th>\n",
       "      <th>-1s</th>\n",
       "      <th>-1t</th>\n",
       "      <th>-1y</th>\n",
       "      <th>1P</th>\n",
       "      <th>1G</th>\n",
       "      <th>1A</th>\n",
       "      <th>1C</th>\n",
       "      <th>1S</th>\n",
       "      <th>1T</th>\n",
       "      <th>1V</th>\n",
       "      <th>1I</th>\n",
       "      <th>1L</th>\n",
       "      <th>1M</th>\n",
       "      <th>1F</th>\n",
       "      <th>1Y</th>\n",
       "      <th>1W</th>\n",
       "      <th>1H</th>\n",
       "      <th>1K</th>\n",
       "      <th>1R</th>\n",
       "      <th>1Q</th>\n",
       "      <th>1N</th>\n",
       "      <th>1D</th>\n",
       "      <th>1E</th>\n",
       "      <th>1s</th>\n",
       "      <th>1t</th>\n",
       "      <th>1y</th>\n",
       "      <th>2P</th>\n",
       "      <th>2G</th>\n",
       "      <th>2A</th>\n",
       "      <th>2C</th>\n",
       "      <th>2S</th>\n",
       "      <th>2T</th>\n",
       "      <th>2V</th>\n",
       "      <th>2I</th>\n",
       "      <th>2L</th>\n",
       "      <th>2M</th>\n",
       "      <th>2F</th>\n",
       "      <th>2Y</th>\n",
       "      <th>2W</th>\n",
       "      <th>2H</th>\n",
       "      <th>2K</th>\n",
       "      <th>2R</th>\n",
       "      <th>2Q</th>\n",
       "      <th>2N</th>\n",
       "      <th>2D</th>\n",
       "      <th>2E</th>\n",
       "      <th>2s</th>\n",
       "      <th>2t</th>\n",
       "      <th>2y</th>\n",
       "      <th>3P</th>\n",
       "      <th>3G</th>\n",
       "      <th>3A</th>\n",
       "      <th>3C</th>\n",
       "      <th>3S</th>\n",
       "      <th>3T</th>\n",
       "      <th>3V</th>\n",
       "      <th>3I</th>\n",
       "      <th>3L</th>\n",
       "      <th>3M</th>\n",
       "      <th>3F</th>\n",
       "      <th>3Y</th>\n",
       "      <th>3W</th>\n",
       "      <th>3H</th>\n",
       "      <th>3K</th>\n",
       "      <th>3R</th>\n",
       "      <th>3Q</th>\n",
       "      <th>3N</th>\n",
       "      <th>3D</th>\n",
       "      <th>3E</th>\n",
       "      <th>3s</th>\n",
       "      <th>3t</th>\n",
       "      <th>3y</th>\n",
       "      <th>4P</th>\n",
       "      <th>4G</th>\n",
       "      <th>4A</th>\n",
       "      <th>4C</th>\n",
       "      <th>4S</th>\n",
       "      <th>4T</th>\n",
       "      <th>4V</th>\n",
       "      <th>4I</th>\n",
       "      <th>4L</th>\n",
       "      <th>4M</th>\n",
       "      <th>4F</th>\n",
       "      <th>4Y</th>\n",
       "      <th>4W</th>\n",
       "      <th>4H</th>\n",
       "      <th>4K</th>\n",
       "      <th>4R</th>\n",
       "      <th>4Q</th>\n",
       "      <th>4N</th>\n",
       "      <th>4D</th>\n",
       "      <th>4E</th>\n",
       "      <th>4s</th>\n",
       "      <th>4t</th>\n",
       "      <th>4y</th>\n",
       "      <th>0s</th>\n",
       "      <th>0t</th>\n",
       "      <th>0y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.124083</td>\n",
       "      <td>-1.333018</td>\n",
       "      <td>0.370999</td>\n",
       "      <td>-1.482608</td>\n",
       "      <td>1.065671</td>\n",
       "      <td>2.435333</td>\n",
       "      <td>-1.181065</td>\n",
       "      <td>0.243360</td>\n",
       "      <td>0.541074</td>\n",
       "      <td>0.526481</td>\n",
       "      <td>0.789112</td>\n",
       "      <td>-1.935882</td>\n",
       "      <td>1.474911</td>\n",
       "      <td>0.908850</td>\n",
       "      <td>-2.656013</td>\n",
       "      <td>0.462956</td>\n",
       "      <td>-0.006826</td>\n",
       "      <td>1.895988</td>\n",
       "      <td>0.339413</td>\n",
       "      <td>0.476922</td>\n",
       "      <td>-0.983320</td>\n",
       "      <td>1.270476</td>\n",
       "      <td>1.475200</td>\n",
       "      <td>-1.669044</td>\n",
       "      <td>-1.078790</td>\n",
       "      <td>-1.765836</td>\n",
       "      <td>0.821250</td>\n",
       "      <td>0.570236</td>\n",
       "      <td>-0.302717</td>\n",
       "      <td>1.042980</td>\n",
       "      <td>-1.587289</td>\n",
       "      <td>0.182884</td>\n",
       "      <td>2.623421</td>\n",
       "      <td>-2.359888</td>\n",
       "      <td>-0.646681</td>\n",
       "      <td>1.590253</td>\n",
       "      <td>2.212846</td>\n",
       "      <td>-2.315615</td>\n",
       "      <td>-0.418003</td>\n",
       "      <td>1.428890</td>\n",
       "      <td>0.929068</td>\n",
       "      <td>1.008114</td>\n",
       "      <td>-0.425482</td>\n",
       "      <td>-2.041824</td>\n",
       "      <td>2.350778</td>\n",
       "      <td>-0.230695</td>\n",
       "      <td>0.340297</td>\n",
       "      <td>0.432018</td>\n",
       "      <td>0.793356</td>\n",
       "      <td>1.710751</td>\n",
       "      <td>-0.199124</td>\n",
       "      <td>0.316310</td>\n",
       "      <td>-2.101466</td>\n",
       "      <td>-1.823912</td>\n",
       "      <td>-1.321935</td>\n",
       "      <td>-1.437125</td>\n",
       "      <td>0.130883</td>\n",
       "      <td>2.243388</td>\n",
       "      <td>1.785522</td>\n",
       "      <td>-1.228163</td>\n",
       "      <td>-1.860953</td>\n",
       "      <td>-2.221823</td>\n",
       "      <td>-0.398452</td>\n",
       "      <td>-1.200318</td>\n",
       "      <td>1.220733</td>\n",
       "      <td>-0.437580</td>\n",
       "      <td>-1.395456</td>\n",
       "      <td>-1.520077</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>-0.452956</td>\n",
       "      <td>-2.240156</td>\n",
       "      <td>1.864775</td>\n",
       "      <td>2.193667</td>\n",
       "      <td>-0.092991</td>\n",
       "      <td>-0.991873</td>\n",
       "      <td>-0.043211</td>\n",
       "      <td>-0.026852</td>\n",
       "      <td>-1.612026</td>\n",
       "      <td>0.882562</td>\n",
       "      <td>-1.191970</td>\n",
       "      <td>0.247461</td>\n",
       "      <td>-1.808025</td>\n",
       "      <td>0.248498</td>\n",
       "      <td>-2.131180</td>\n",
       "      <td>-2.775856</td>\n",
       "      <td>-2.635972</td>\n",
       "      <td>-0.260469</td>\n",
       "      <td>1.875052</td>\n",
       "      <td>-0.374094</td>\n",
       "      <td>-1.802452</td>\n",
       "      <td>1.710730</td>\n",
       "      <td>0.500554</td>\n",
       "      <td>-2.568784</td>\n",
       "      <td>-1.137813</td>\n",
       "      <td>1.847648</td>\n",
       "      <td>-1.176928</td>\n",
       "      <td>-0.784231</td>\n",
       "      <td>-0.335855</td>\n",
       "      <td>-0.998383</td>\n",
       "      <td>-1.808661</td>\n",
       "      <td>0.702443</td>\n",
       "      <td>-0.933349</td>\n",
       "      <td>-1.970836</td>\n",
       "      <td>1.824810</td>\n",
       "      <td>-0.550751</td>\n",
       "      <td>-1.536322</td>\n",
       "      <td>-0.277769</td>\n",
       "      <td>-0.246322</td>\n",
       "      <td>0.715673</td>\n",
       "      <td>-0.243507</td>\n",
       "      <td>-0.004827</td>\n",
       "      <td>0.491623</td>\n",
       "      <td>-0.027491</td>\n",
       "      <td>-0.621791</td>\n",
       "      <td>1.520924</td>\n",
       "      <td>-1.725467</td>\n",
       "      <td>-0.762519</td>\n",
       "      <td>0.823468</td>\n",
       "      <td>-0.832005</td>\n",
       "      <td>-1.385254</td>\n",
       "      <td>1.463969</td>\n",
       "      <td>-1.065097</td>\n",
       "      <td>-0.268777</td>\n",
       "      <td>0.191179</td>\n",
       "      <td>-0.256498</td>\n",
       "      <td>1.285461</td>\n",
       "      <td>1.410629</td>\n",
       "      <td>-0.480923</td>\n",
       "      <td>0.052641</td>\n",
       "      <td>-1.333732</td>\n",
       "      <td>-0.808122</td>\n",
       "      <td>-0.641196</td>\n",
       "      <td>0.846606</td>\n",
       "      <td>0.054078</td>\n",
       "      <td>-2.322969</td>\n",
       "      <td>-1.790822</td>\n",
       "      <td>0.040727</td>\n",
       "      <td>0.311768</td>\n",
       "      <td>2.213273</td>\n",
       "      <td>1.011133</td>\n",
       "      <td>1.735011</td>\n",
       "      <td>2.706975</td>\n",
       "      <td>2.267068</td>\n",
       "      <td>1.176308</td>\n",
       "      <td>-1.155246</td>\n",
       "      <td>0.358741</td>\n",
       "      <td>-0.040418</td>\n",
       "      <td>0.549987</td>\n",
       "      <td>1.847084</td>\n",
       "      <td>-0.215823</td>\n",
       "      <td>-0.858711</td>\n",
       "      <td>-1.608082</td>\n",
       "      <td>-1.274341</td>\n",
       "      <td>-0.575624</td>\n",
       "      <td>0.875942</td>\n",
       "      <td>0.312031</td>\n",
       "      <td>-0.394171</td>\n",
       "      <td>-1.378174</td>\n",
       "      <td>-1.457255</td>\n",
       "      <td>0.548059</td>\n",
       "      <td>-0.840706</td>\n",
       "      <td>2.288101</td>\n",
       "      <td>0.837432</td>\n",
       "      <td>0.640592</td>\n",
       "      <td>1.597600</td>\n",
       "      <td>2.678149</td>\n",
       "      <td>2.460815</td>\n",
       "      <td>-3.140487</td>\n",
       "      <td>2.607033</td>\n",
       "      <td>0.160671</td>\n",
       "      <td>1.281283</td>\n",
       "      <td>0.391561</td>\n",
       "      <td>0.285112</td>\n",
       "      <td>0.313980</td>\n",
       "      <td>1.758898</td>\n",
       "      <td>-0.577085</td>\n",
       "      <td>-1.576527</td>\n",
       "      <td>-0.883964</td>\n",
       "      <td>-0.221910</td>\n",
       "      <td>-0.472430</td>\n",
       "      <td>0.627727</td>\n",
       "      <td>1.816311</td>\n",
       "      <td>-1.552552</td>\n",
       "      <td>-2.032012</td>\n",
       "      <td>1.988275</td>\n",
       "      <td>1.918780</td>\n",
       "      <td>-1.275063</td>\n",
       "      <td>-1.375782</td>\n",
       "      <td>0.033672</td>\n",
       "      <td>-1.072162</td>\n",
       "      <td>-0.100851</td>\n",
       "      <td>-0.315402</td>\n",
       "      <td>-0.936294</td>\n",
       "      <td>-0.149866</td>\n",
       "      <td>1.335872</td>\n",
       "      <td>1.107367</td>\n",
       "      <td>0.755761</td>\n",
       "      <td>0.614412</td>\n",
       "      <td>-0.339984</td>\n",
       "      <td>1.296790</td>\n",
       "      <td>0.086605</td>\n",
       "      <td>-0.325200</td>\n",
       "      <td>0.626342</td>\n",
       "      <td>0.139817</td>\n",
       "      <td>-1.742222</td>\n",
       "      <td>0.486119</td>\n",
       "      <td>0.835788</td>\n",
       "      <td>1.808517</td>\n",
       "      <td>-0.691114</td>\n",
       "      <td>1.019787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.082794</td>\n",
       "      <td>-1.315036</td>\n",
       "      <td>0.377679</td>\n",
       "      <td>-1.303769</td>\n",
       "      <td>1.028240</td>\n",
       "      <td>2.277051</td>\n",
       "      <td>-1.140310</td>\n",
       "      <td>0.201248</td>\n",
       "      <td>0.481489</td>\n",
       "      <td>0.407088</td>\n",
       "      <td>0.760801</td>\n",
       "      <td>-1.863295</td>\n",
       "      <td>1.475667</td>\n",
       "      <td>0.822474</td>\n",
       "      <td>-2.524725</td>\n",
       "      <td>0.393658</td>\n",
       "      <td>-0.127901</td>\n",
       "      <td>1.751877</td>\n",
       "      <td>0.297822</td>\n",
       "      <td>0.382287</td>\n",
       "      <td>-0.939208</td>\n",
       "      <td>1.297959</td>\n",
       "      <td>1.379947</td>\n",
       "      <td>-1.500982</td>\n",
       "      <td>-1.073636</td>\n",
       "      <td>-1.720251</td>\n",
       "      <td>0.947168</td>\n",
       "      <td>0.489036</td>\n",
       "      <td>-0.398783</td>\n",
       "      <td>1.048175</td>\n",
       "      <td>-1.578853</td>\n",
       "      <td>0.202615</td>\n",
       "      <td>2.475548</td>\n",
       "      <td>-2.330920</td>\n",
       "      <td>-0.625377</td>\n",
       "      <td>1.575310</td>\n",
       "      <td>2.092734</td>\n",
       "      <td>-2.183469</td>\n",
       "      <td>-0.456739</td>\n",
       "      <td>1.414051</td>\n",
       "      <td>0.921833</td>\n",
       "      <td>0.912542</td>\n",
       "      <td>-0.522701</td>\n",
       "      <td>-1.903706</td>\n",
       "      <td>2.279727</td>\n",
       "      <td>-0.167850</td>\n",
       "      <td>0.331005</td>\n",
       "      <td>0.305503</td>\n",
       "      <td>0.690275</td>\n",
       "      <td>1.713022</td>\n",
       "      <td>-0.114235</td>\n",
       "      <td>0.247336</td>\n",
       "      <td>-1.971480</td>\n",
       "      <td>-1.755766</td>\n",
       "      <td>-1.216914</td>\n",
       "      <td>-1.356737</td>\n",
       "      <td>0.120693</td>\n",
       "      <td>2.188673</td>\n",
       "      <td>1.620649</td>\n",
       "      <td>-1.219324</td>\n",
       "      <td>-1.883983</td>\n",
       "      <td>-2.129232</td>\n",
       "      <td>-0.349166</td>\n",
       "      <td>-1.165124</td>\n",
       "      <td>1.202160</td>\n",
       "      <td>-0.446142</td>\n",
       "      <td>-1.265840</td>\n",
       "      <td>-1.397379</td>\n",
       "      <td>0.963779</td>\n",
       "      <td>-0.340953</td>\n",
       "      <td>-2.203007</td>\n",
       "      <td>1.697120</td>\n",
       "      <td>2.142285</td>\n",
       "      <td>-0.070298</td>\n",
       "      <td>-0.790852</td>\n",
       "      <td>-0.047399</td>\n",
       "      <td>-0.163245</td>\n",
       "      <td>-1.561266</td>\n",
       "      <td>0.780174</td>\n",
       "      <td>-1.106945</td>\n",
       "      <td>0.288282</td>\n",
       "      <td>-1.763729</td>\n",
       "      <td>0.198121</td>\n",
       "      <td>-1.961371</td>\n",
       "      <td>-2.751048</td>\n",
       "      <td>-2.657668</td>\n",
       "      <td>-0.304017</td>\n",
       "      <td>1.781997</td>\n",
       "      <td>-0.442557</td>\n",
       "      <td>-1.629253</td>\n",
       "      <td>1.846559</td>\n",
       "      <td>0.596902</td>\n",
       "      <td>-2.496807</td>\n",
       "      <td>-1.063768</td>\n",
       "      <td>1.810785</td>\n",
       "      <td>-1.048127</td>\n",
       "      <td>-0.712356</td>\n",
       "      <td>-0.200570</td>\n",
       "      <td>-0.918905</td>\n",
       "      <td>-1.857498</td>\n",
       "      <td>0.661085</td>\n",
       "      <td>-0.874454</td>\n",
       "      <td>-1.888536</td>\n",
       "      <td>1.837306</td>\n",
       "      <td>-0.410973</td>\n",
       "      <td>-1.405960</td>\n",
       "      <td>-0.243723</td>\n",
       "      <td>-0.118655</td>\n",
       "      <td>0.758936</td>\n",
       "      <td>-0.238423</td>\n",
       "      <td>0.035166</td>\n",
       "      <td>0.525149</td>\n",
       "      <td>0.007085</td>\n",
       "      <td>-0.653448</td>\n",
       "      <td>1.576913</td>\n",
       "      <td>-1.723523</td>\n",
       "      <td>-0.764785</td>\n",
       "      <td>0.659238</td>\n",
       "      <td>-0.681803</td>\n",
       "      <td>-1.357008</td>\n",
       "      <td>1.411377</td>\n",
       "      <td>-1.068875</td>\n",
       "      <td>-0.276262</td>\n",
       "      <td>0.145424</td>\n",
       "      <td>-0.154642</td>\n",
       "      <td>1.320319</td>\n",
       "      <td>1.450584</td>\n",
       "      <td>-0.400749</td>\n",
       "      <td>0.072105</td>\n",
       "      <td>-1.270154</td>\n",
       "      <td>-0.710633</td>\n",
       "      <td>-0.639188</td>\n",
       "      <td>0.825336</td>\n",
       "      <td>0.027273</td>\n",
       "      <td>-2.346619</td>\n",
       "      <td>-1.877316</td>\n",
       "      <td>-0.009294</td>\n",
       "      <td>0.250529</td>\n",
       "      <td>2.121087</td>\n",
       "      <td>0.856824</td>\n",
       "      <td>1.638516</td>\n",
       "      <td>2.668117</td>\n",
       "      <td>2.182310</td>\n",
       "      <td>1.150948</td>\n",
       "      <td>-1.064407</td>\n",
       "      <td>0.255830</td>\n",
       "      <td>-0.072337</td>\n",
       "      <td>0.510535</td>\n",
       "      <td>1.832212</td>\n",
       "      <td>-0.094357</td>\n",
       "      <td>-0.771008</td>\n",
       "      <td>-1.546097</td>\n",
       "      <td>-1.273632</td>\n",
       "      <td>-0.578555</td>\n",
       "      <td>0.783154</td>\n",
       "      <td>0.274340</td>\n",
       "      <td>-0.444182</td>\n",
       "      <td>-1.413612</td>\n",
       "      <td>-1.433344</td>\n",
       "      <td>0.548449</td>\n",
       "      <td>-0.811656</td>\n",
       "      <td>2.192537</td>\n",
       "      <td>0.838117</td>\n",
       "      <td>0.546955</td>\n",
       "      <td>1.536627</td>\n",
       "      <td>2.484802</td>\n",
       "      <td>2.363493</td>\n",
       "      <td>-3.035698</td>\n",
       "      <td>2.486802</td>\n",
       "      <td>0.129263</td>\n",
       "      <td>1.323699</td>\n",
       "      <td>0.465493</td>\n",
       "      <td>0.312922</td>\n",
       "      <td>0.323904</td>\n",
       "      <td>1.663263</td>\n",
       "      <td>-0.602994</td>\n",
       "      <td>-1.513537</td>\n",
       "      <td>-0.794236</td>\n",
       "      <td>-0.261255</td>\n",
       "      <td>-0.463526</td>\n",
       "      <td>0.499759</td>\n",
       "      <td>1.750798</td>\n",
       "      <td>-1.547009</td>\n",
       "      <td>-1.931046</td>\n",
       "      <td>1.905437</td>\n",
       "      <td>1.763401</td>\n",
       "      <td>-1.184554</td>\n",
       "      <td>-1.306713</td>\n",
       "      <td>0.010132</td>\n",
       "      <td>-0.859298</td>\n",
       "      <td>-0.293271</td>\n",
       "      <td>-0.446997</td>\n",
       "      <td>-0.878223</td>\n",
       "      <td>-0.176436</td>\n",
       "      <td>1.229133</td>\n",
       "      <td>1.027897</td>\n",
       "      <td>0.718036</td>\n",
       "      <td>0.600043</td>\n",
       "      <td>-0.344983</td>\n",
       "      <td>1.210207</td>\n",
       "      <td>0.189907</td>\n",
       "      <td>-0.176607</td>\n",
       "      <td>0.637552</td>\n",
       "      <td>0.016095</td>\n",
       "      <td>-1.645154</td>\n",
       "      <td>0.400391</td>\n",
       "      <td>0.861786</td>\n",
       "      <td>1.831862</td>\n",
       "      <td>-0.617930</td>\n",
       "      <td>0.849381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.123702</td>\n",
       "      <td>-1.333739</td>\n",
       "      <td>0.366915</td>\n",
       "      <td>-1.481333</td>\n",
       "      <td>1.064120</td>\n",
       "      <td>2.436097</td>\n",
       "      <td>-1.182564</td>\n",
       "      <td>0.240058</td>\n",
       "      <td>0.532565</td>\n",
       "      <td>0.530294</td>\n",
       "      <td>0.786545</td>\n",
       "      <td>-1.937065</td>\n",
       "      <td>1.474733</td>\n",
       "      <td>0.902049</td>\n",
       "      <td>-2.661757</td>\n",
       "      <td>0.461727</td>\n",
       "      <td>-0.000827</td>\n",
       "      <td>1.899083</td>\n",
       "      <td>0.339538</td>\n",
       "      <td>0.481326</td>\n",
       "      <td>-0.989064</td>\n",
       "      <td>1.271149</td>\n",
       "      <td>1.472290</td>\n",
       "      <td>-1.670950</td>\n",
       "      <td>-1.078952</td>\n",
       "      <td>-1.766250</td>\n",
       "      <td>0.818193</td>\n",
       "      <td>0.571061</td>\n",
       "      <td>-0.305479</td>\n",
       "      <td>1.043114</td>\n",
       "      <td>-1.586039</td>\n",
       "      <td>0.186335</td>\n",
       "      <td>2.619740</td>\n",
       "      <td>-2.360800</td>\n",
       "      <td>-0.646138</td>\n",
       "      <td>1.595065</td>\n",
       "      <td>2.218405</td>\n",
       "      <td>-2.316889</td>\n",
       "      <td>-0.417599</td>\n",
       "      <td>1.419278</td>\n",
       "      <td>0.933096</td>\n",
       "      <td>1.008337</td>\n",
       "      <td>-0.425961</td>\n",
       "      <td>-2.040859</td>\n",
       "      <td>2.348625</td>\n",
       "      <td>-0.230205</td>\n",
       "      <td>0.342206</td>\n",
       "      <td>0.438278</td>\n",
       "      <td>0.790901</td>\n",
       "      <td>1.707645</td>\n",
       "      <td>-0.197786</td>\n",
       "      <td>0.317424</td>\n",
       "      <td>-2.102586</td>\n",
       "      <td>-1.823015</td>\n",
       "      <td>-1.326136</td>\n",
       "      <td>-1.441910</td>\n",
       "      <td>0.130691</td>\n",
       "      <td>2.245629</td>\n",
       "      <td>1.784710</td>\n",
       "      <td>-1.232561</td>\n",
       "      <td>-1.864752</td>\n",
       "      <td>-2.223518</td>\n",
       "      <td>-0.395831</td>\n",
       "      <td>-1.202127</td>\n",
       "      <td>1.219322</td>\n",
       "      <td>-0.429746</td>\n",
       "      <td>-1.399199</td>\n",
       "      <td>-1.524138</td>\n",
       "      <td>0.935269</td>\n",
       "      <td>-0.459730</td>\n",
       "      <td>-2.240560</td>\n",
       "      <td>1.856851</td>\n",
       "      <td>2.189566</td>\n",
       "      <td>-0.092959</td>\n",
       "      <td>-0.995058</td>\n",
       "      <td>-0.048542</td>\n",
       "      <td>-0.032541</td>\n",
       "      <td>-1.609916</td>\n",
       "      <td>0.883162</td>\n",
       "      <td>-1.187196</td>\n",
       "      <td>0.243556</td>\n",
       "      <td>-1.807652</td>\n",
       "      <td>0.244450</td>\n",
       "      <td>-2.132658</td>\n",
       "      <td>-2.778566</td>\n",
       "      <td>-2.639370</td>\n",
       "      <td>-0.264530</td>\n",
       "      <td>1.877488</td>\n",
       "      <td>-0.369802</td>\n",
       "      <td>-1.802853</td>\n",
       "      <td>1.705964</td>\n",
       "      <td>0.503266</td>\n",
       "      <td>-2.575821</td>\n",
       "      <td>-1.141595</td>\n",
       "      <td>1.846794</td>\n",
       "      <td>-1.179568</td>\n",
       "      <td>-0.783157</td>\n",
       "      <td>-0.332587</td>\n",
       "      <td>-1.004201</td>\n",
       "      <td>-1.810084</td>\n",
       "      <td>0.700647</td>\n",
       "      <td>-0.936017</td>\n",
       "      <td>-1.972526</td>\n",
       "      <td>1.827652</td>\n",
       "      <td>-0.550069</td>\n",
       "      <td>-1.534301</td>\n",
       "      <td>-0.285710</td>\n",
       "      <td>-0.250438</td>\n",
       "      <td>0.710358</td>\n",
       "      <td>-0.247381</td>\n",
       "      <td>-0.005479</td>\n",
       "      <td>0.497706</td>\n",
       "      <td>-0.025079</td>\n",
       "      <td>-0.622655</td>\n",
       "      <td>1.515863</td>\n",
       "      <td>-1.725029</td>\n",
       "      <td>-0.768893</td>\n",
       "      <td>0.822101</td>\n",
       "      <td>-0.829749</td>\n",
       "      <td>-1.389199</td>\n",
       "      <td>1.458424</td>\n",
       "      <td>-1.061635</td>\n",
       "      <td>-0.267768</td>\n",
       "      <td>0.189667</td>\n",
       "      <td>-0.251762</td>\n",
       "      <td>1.280175</td>\n",
       "      <td>1.409162</td>\n",
       "      <td>-0.482356</td>\n",
       "      <td>0.053705</td>\n",
       "      <td>-1.336888</td>\n",
       "      <td>-0.803956</td>\n",
       "      <td>-0.637138</td>\n",
       "      <td>0.844385</td>\n",
       "      <td>0.056590</td>\n",
       "      <td>-2.324191</td>\n",
       "      <td>-1.794218</td>\n",
       "      <td>0.045212</td>\n",
       "      <td>0.311169</td>\n",
       "      <td>2.211957</td>\n",
       "      <td>1.013297</td>\n",
       "      <td>1.733366</td>\n",
       "      <td>2.711608</td>\n",
       "      <td>2.267383</td>\n",
       "      <td>1.180971</td>\n",
       "      <td>-1.155435</td>\n",
       "      <td>0.362334</td>\n",
       "      <td>-0.041591</td>\n",
       "      <td>0.555404</td>\n",
       "      <td>1.850645</td>\n",
       "      <td>-0.215913</td>\n",
       "      <td>-0.854904</td>\n",
       "      <td>-1.611007</td>\n",
       "      <td>-1.278233</td>\n",
       "      <td>-0.572426</td>\n",
       "      <td>0.872702</td>\n",
       "      <td>0.310544</td>\n",
       "      <td>-0.392968</td>\n",
       "      <td>-1.380815</td>\n",
       "      <td>-1.458252</td>\n",
       "      <td>0.546254</td>\n",
       "      <td>-0.840335</td>\n",
       "      <td>2.291621</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.639362</td>\n",
       "      <td>1.597017</td>\n",
       "      <td>2.676531</td>\n",
       "      <td>2.464077</td>\n",
       "      <td>-3.141581</td>\n",
       "      <td>2.609081</td>\n",
       "      <td>0.159573</td>\n",
       "      <td>1.282543</td>\n",
       "      <td>0.395703</td>\n",
       "      <td>0.290928</td>\n",
       "      <td>0.314056</td>\n",
       "      <td>1.763387</td>\n",
       "      <td>-0.576015</td>\n",
       "      <td>-1.577078</td>\n",
       "      <td>-0.882917</td>\n",
       "      <td>-0.221786</td>\n",
       "      <td>-0.469884</td>\n",
       "      <td>0.630226</td>\n",
       "      <td>1.810092</td>\n",
       "      <td>-1.554211</td>\n",
       "      <td>-2.031102</td>\n",
       "      <td>1.985721</td>\n",
       "      <td>1.916608</td>\n",
       "      <td>-1.274601</td>\n",
       "      <td>-1.378692</td>\n",
       "      <td>0.030345</td>\n",
       "      <td>-1.073612</td>\n",
       "      <td>-0.098157</td>\n",
       "      <td>-0.311278</td>\n",
       "      <td>-0.941815</td>\n",
       "      <td>-0.152202</td>\n",
       "      <td>1.338775</td>\n",
       "      <td>1.102758</td>\n",
       "      <td>0.755549</td>\n",
       "      <td>0.615726</td>\n",
       "      <td>-0.340387</td>\n",
       "      <td>1.296974</td>\n",
       "      <td>0.080942</td>\n",
       "      <td>-0.324911</td>\n",
       "      <td>0.624071</td>\n",
       "      <td>0.138197</td>\n",
       "      <td>-1.744510</td>\n",
       "      <td>0.489025</td>\n",
       "      <td>0.833121</td>\n",
       "      <td>1.808313</td>\n",
       "      <td>-0.695774</td>\n",
       "      <td>1.022946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         -5P       -5G       -5A       -5C       -5S       -5T       -5V  \\\n",
       "3  -1.124083 -1.333018  0.370999 -1.482608  1.065671  2.435333 -1.181065   \n",
       "8  -1.082794 -1.315036  0.377679 -1.303769  1.028240  2.277051 -1.140310   \n",
       "10 -1.123702 -1.333739  0.366915 -1.481333  1.064120  2.436097 -1.182564   \n",
       "\n",
       "         -5I       -5L       -5M       -5F       -5Y       -5W       -5H  \\\n",
       "3   0.243360  0.541074  0.526481  0.789112 -1.935882  1.474911  0.908850   \n",
       "8   0.201248  0.481489  0.407088  0.760801 -1.863295  1.475667  0.822474   \n",
       "10  0.240058  0.532565  0.530294  0.786545 -1.937065  1.474733  0.902049   \n",
       "\n",
       "         -5K       -5R       -5Q       -5N       -5D       -5E       -5s  \\\n",
       "3  -2.656013  0.462956 -0.006826  1.895988  0.339413  0.476922 -0.983320   \n",
       "8  -2.524725  0.393658 -0.127901  1.751877  0.297822  0.382287 -0.939208   \n",
       "10 -2.661757  0.461727 -0.000827  1.899083  0.339538  0.481326 -0.989064   \n",
       "\n",
       "         -5t       -5y       -4P       -4G       -4A       -4C       -4S  \\\n",
       "3   1.270476  1.475200 -1.669044 -1.078790 -1.765836  0.821250  0.570236   \n",
       "8   1.297959  1.379947 -1.500982 -1.073636 -1.720251  0.947168  0.489036   \n",
       "10  1.271149  1.472290 -1.670950 -1.078952 -1.766250  0.818193  0.571061   \n",
       "\n",
       "         -4T       -4V       -4I       -4L       -4M       -4F       -4Y  \\\n",
       "3  -0.302717  1.042980 -1.587289  0.182884  2.623421 -2.359888 -0.646681   \n",
       "8  -0.398783  1.048175 -1.578853  0.202615  2.475548 -2.330920 -0.625377   \n",
       "10 -0.305479  1.043114 -1.586039  0.186335  2.619740 -2.360800 -0.646138   \n",
       "\n",
       "         -4W       -4H       -4K       -4R       -4Q       -4N       -4D  \\\n",
       "3   1.590253  2.212846 -2.315615 -0.418003  1.428890  0.929068  1.008114   \n",
       "8   1.575310  2.092734 -2.183469 -0.456739  1.414051  0.921833  0.912542   \n",
       "10  1.595065  2.218405 -2.316889 -0.417599  1.419278  0.933096  1.008337   \n",
       "\n",
       "         -4E       -4s       -4t       -4y       -3P       -3G       -3A  \\\n",
       "3  -0.425482 -2.041824  2.350778 -0.230695  0.340297  0.432018  0.793356   \n",
       "8  -0.522701 -1.903706  2.279727 -0.167850  0.331005  0.305503  0.690275   \n",
       "10 -0.425961 -2.040859  2.348625 -0.230205  0.342206  0.438278  0.790901   \n",
       "\n",
       "         -3C       -3S       -3T       -3V       -3I       -3L       -3M  \\\n",
       "3   1.710751 -0.199124  0.316310 -2.101466 -1.823912 -1.321935 -1.437125   \n",
       "8   1.713022 -0.114235  0.247336 -1.971480 -1.755766 -1.216914 -1.356737   \n",
       "10  1.707645 -0.197786  0.317424 -2.102586 -1.823015 -1.326136 -1.441910   \n",
       "\n",
       "         -3F       -3Y       -3W       -3H       -3K       -3R       -3Q  \\\n",
       "3   0.130883  2.243388  1.785522 -1.228163 -1.860953 -2.221823 -0.398452   \n",
       "8   0.120693  2.188673  1.620649 -1.219324 -1.883983 -2.129232 -0.349166   \n",
       "10  0.130691  2.245629  1.784710 -1.232561 -1.864752 -2.223518 -0.395831   \n",
       "\n",
       "         -3N       -3D       -3E       -3s       -3t       -3y       -2P  \\\n",
       "3  -1.200318  1.220733 -0.437580 -1.395456 -1.520077  0.929825 -0.452956   \n",
       "8  -1.165124  1.202160 -0.446142 -1.265840 -1.397379  0.963779 -0.340953   \n",
       "10 -1.202127  1.219322 -0.429746 -1.399199 -1.524138  0.935269 -0.459730   \n",
       "\n",
       "         -2G       -2A       -2C       -2S       -2T       -2V       -2I  \\\n",
       "3  -2.240156  1.864775  2.193667 -0.092991 -0.991873 -0.043211 -0.026852   \n",
       "8  -2.203007  1.697120  2.142285 -0.070298 -0.790852 -0.047399 -0.163245   \n",
       "10 -2.240560  1.856851  2.189566 -0.092959 -0.995058 -0.048542 -0.032541   \n",
       "\n",
       "         -2L       -2M       -2F       -2Y       -2W       -2H       -2K  \\\n",
       "3  -1.612026  0.882562 -1.191970  0.247461 -1.808025  0.248498 -2.131180   \n",
       "8  -1.561266  0.780174 -1.106945  0.288282 -1.763729  0.198121 -1.961371   \n",
       "10 -1.609916  0.883162 -1.187196  0.243556 -1.807652  0.244450 -2.132658   \n",
       "\n",
       "         -2R       -2Q       -2N       -2D       -2E       -2s       -2t  \\\n",
       "3  -2.775856 -2.635972 -0.260469  1.875052 -0.374094 -1.802452  1.710730   \n",
       "8  -2.751048 -2.657668 -0.304017  1.781997 -0.442557 -1.629253  1.846559   \n",
       "10 -2.778566 -2.639370 -0.264530  1.877488 -0.369802 -1.802853  1.705964   \n",
       "\n",
       "         -2y       -1P       -1G       -1A       -1C       -1S       -1T  \\\n",
       "3   0.500554 -2.568784 -1.137813  1.847648 -1.176928 -0.784231 -0.335855   \n",
       "8   0.596902 -2.496807 -1.063768  1.810785 -1.048127 -0.712356 -0.200570   \n",
       "10  0.503266 -2.575821 -1.141595  1.846794 -1.179568 -0.783157 -0.332587   \n",
       "\n",
       "         -1V       -1I       -1L       -1M       -1F       -1Y       -1W  \\\n",
       "3  -0.998383 -1.808661  0.702443 -0.933349 -1.970836  1.824810 -0.550751   \n",
       "8  -0.918905 -1.857498  0.661085 -0.874454 -1.888536  1.837306 -0.410973   \n",
       "10 -1.004201 -1.810084  0.700647 -0.936017 -1.972526  1.827652 -0.550069   \n",
       "\n",
       "         -1H       -1K       -1R       -1Q       -1N       -1D       -1E  \\\n",
       "3  -1.536322 -0.277769 -0.246322  0.715673 -0.243507 -0.004827  0.491623   \n",
       "8  -1.405960 -0.243723 -0.118655  0.758936 -0.238423  0.035166  0.525149   \n",
       "10 -1.534301 -0.285710 -0.250438  0.710358 -0.247381 -0.005479  0.497706   \n",
       "\n",
       "         -1s       -1t       -1y        1P        1G        1A        1C  \\\n",
       "3  -0.027491 -0.621791  1.520924 -1.725467 -0.762519  0.823468 -0.832005   \n",
       "8   0.007085 -0.653448  1.576913 -1.723523 -0.764785  0.659238 -0.681803   \n",
       "10 -0.025079 -0.622655  1.515863 -1.725029 -0.768893  0.822101 -0.829749   \n",
       "\n",
       "          1S        1T        1V        1I        1L        1M        1F  \\\n",
       "3  -1.385254  1.463969 -1.065097 -0.268777  0.191179 -0.256498  1.285461   \n",
       "8  -1.357008  1.411377 -1.068875 -0.276262  0.145424 -0.154642  1.320319   \n",
       "10 -1.389199  1.458424 -1.061635 -0.267768  0.189667 -0.251762  1.280175   \n",
       "\n",
       "          1Y        1W        1H        1K        1R        1Q        1N  \\\n",
       "3   1.410629 -0.480923  0.052641 -1.333732 -0.808122 -0.641196  0.846606   \n",
       "8   1.450584 -0.400749  0.072105 -1.270154 -0.710633 -0.639188  0.825336   \n",
       "10  1.409162 -0.482356  0.053705 -1.336888 -0.803956 -0.637138  0.844385   \n",
       "\n",
       "          1D        1E        1s        1t        1y        2P        2G  \\\n",
       "3   0.054078 -2.322969 -1.790822  0.040727  0.311768  2.213273  1.011133   \n",
       "8   0.027273 -2.346619 -1.877316 -0.009294  0.250529  2.121087  0.856824   \n",
       "10  0.056590 -2.324191 -1.794218  0.045212  0.311169  2.211957  1.013297   \n",
       "\n",
       "          2A        2C        2S        2T        2V        2I        2L  \\\n",
       "3   1.735011  2.706975  2.267068  1.176308 -1.155246  0.358741 -0.040418   \n",
       "8   1.638516  2.668117  2.182310  1.150948 -1.064407  0.255830 -0.072337   \n",
       "10  1.733366  2.711608  2.267383  1.180971 -1.155435  0.362334 -0.041591   \n",
       "\n",
       "          2M        2F        2Y        2W        2H        2K        2R  \\\n",
       "3   0.549987  1.847084 -0.215823 -0.858711 -1.608082 -1.274341 -0.575624   \n",
       "8   0.510535  1.832212 -0.094357 -0.771008 -1.546097 -1.273632 -0.578555   \n",
       "10  0.555404  1.850645 -0.215913 -0.854904 -1.611007 -1.278233 -0.572426   \n",
       "\n",
       "          2Q        2N        2D        2E        2s        2t        2y  \\\n",
       "3   0.875942  0.312031 -0.394171 -1.378174 -1.457255  0.548059 -0.840706   \n",
       "8   0.783154  0.274340 -0.444182 -1.413612 -1.433344  0.548449 -0.811656   \n",
       "10  0.872702  0.310544 -0.392968 -1.380815 -1.458252  0.546254 -0.840335   \n",
       "\n",
       "          3P        3G        3A        3C        3S        3T        3V  \\\n",
       "3   2.288101  0.837432  0.640592  1.597600  2.678149  2.460815 -3.140487   \n",
       "8   2.192537  0.838117  0.546955  1.536627  2.484802  2.363493 -3.035698   \n",
       "10  2.291621  0.837838  0.639362  1.597017  2.676531  2.464077 -3.141581   \n",
       "\n",
       "          3I        3L        3M        3F        3Y        3W        3H  \\\n",
       "3   2.607033  0.160671  1.281283  0.391561  0.285112  0.313980  1.758898   \n",
       "8   2.486802  0.129263  1.323699  0.465493  0.312922  0.323904  1.663263   \n",
       "10  2.609081  0.159573  1.282543  0.395703  0.290928  0.314056  1.763387   \n",
       "\n",
       "          3K        3R        3Q        3N        3D        3E        3s  \\\n",
       "3  -0.577085 -1.576527 -0.883964 -0.221910 -0.472430  0.627727  1.816311   \n",
       "8  -0.602994 -1.513537 -0.794236 -0.261255 -0.463526  0.499759  1.750798   \n",
       "10 -0.576015 -1.577078 -0.882917 -0.221786 -0.469884  0.630226  1.810092   \n",
       "\n",
       "          3t        3y        4P        4G        4A        4C        4S  \\\n",
       "3  -1.552552 -2.032012  1.988275  1.918780 -1.275063 -1.375782  0.033672   \n",
       "8  -1.547009 -1.931046  1.905437  1.763401 -1.184554 -1.306713  0.010132   \n",
       "10 -1.554211 -2.031102  1.985721  1.916608 -1.274601 -1.378692  0.030345   \n",
       "\n",
       "          4T        4V        4I        4L        4M        4F        4Y  \\\n",
       "3  -1.072162 -0.100851 -0.315402 -0.936294 -0.149866  1.335872  1.107367   \n",
       "8  -0.859298 -0.293271 -0.446997 -0.878223 -0.176436  1.229133  1.027897   \n",
       "10 -1.073612 -0.098157 -0.311278 -0.941815 -0.152202  1.338775  1.102758   \n",
       "\n",
       "          4W        4H        4K        4R        4Q        4N        4D  \\\n",
       "3   0.755761  0.614412 -0.339984  1.296790  0.086605 -0.325200  0.626342   \n",
       "8   0.718036  0.600043 -0.344983  1.210207  0.189907 -0.176607  0.637552   \n",
       "10  0.755549  0.615726 -0.340387  1.296974  0.080942 -0.324911  0.624071   \n",
       "\n",
       "          4E        4s        4t        4y        0s        0t        0y  \n",
       "3   0.139817 -1.742222  0.486119  0.835788  1.808517 -0.691114  1.019787  \n",
       "8   0.016095 -1.645154  0.400391  0.861786  1.831862 -0.617930  0.849381  \n",
       "10  0.138197 -1.744510  0.489025  0.833121  1.808313 -0.695774  1.022946  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = predict_dl(test.head(3),\n",
    "                  feat_col,\n",
    "                  target_col, \n",
    "                  model,'test')\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall MSE: 3.1266\n",
      "Average Pearson: 0.1593 \n"
     ]
    }
   ],
   "source": [
    "_,_,corr = score_each(test[target_col].head(3),pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
