{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DL\n",
    "\n",
    "> A collection of deep learning tools via Fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import sys\n",
    "sys.path.append(\"/notebooks/katlas\")\n",
    "from nbdev.showdoc import *\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| output: False\n",
    "from fastbook import *\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import spearmanr,pearsonr\n",
    "from katlas.core import *\n",
    "from katlas.feature import *\n",
    "from sklearn.model_selection import *\n",
    "import torch.nn.init as init\n",
    "from katlas.train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def seed_everything(seed=123):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GeneralDataset:\n",
    "    def __init__(self, \n",
    "                 df, # a dataframe of values\n",
    "                 feat_col, # feature columns\n",
    "                 target_col=None # Will return test set for prediction if target col is None\n",
    "                ):\n",
    "        \"A general dataset that can be applied to any dataframe\"\n",
    "        \n",
    "        self.test = False if target_col is not None else True\n",
    "        \n",
    "        self.X = df[feat_col].values \n",
    "        self.y = df[target_col].values if not self.test else None\n",
    "        \n",
    "        self.len = df.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = torch.Tensor(self.X[index])\n",
    "        if self.test:\n",
    "            return X\n",
    "        else:\n",
    "            y = torch.Tensor(self.y[index])\n",
    "            return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_parquet('train/scaled_t5.parquet')\n",
    "\n",
    "# feat_col = df.columns[199:]\n",
    "# target_col = df.columns[1:199]\n",
    "\n",
    "# info = Data.get_kinase_info_full()\n",
    "# info = info.query('in_paper ==1')\n",
    "# info = df[['kinase']].merge(info,'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| output: False\n",
    "# splits = get_splits(info, stratified = 'category')\n",
    "# split = splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = df.loc[split[0]]\n",
    "# valid = df.loc[split[1]]\n",
    "\n",
    "# train.shape, valid.shape\n",
    "\n",
    "# train_ds = GeneralDataset(train, feat_col, target_col)\n",
    "# valid_ds = GeneralDataset(valid, feat_col, target_col)\n",
    "\n",
    "# len(train_ds),len(valid_ds)\n",
    "\n",
    "# dls = DataLoaders.from_dsets(train_ds, valid_ds, num_workers=4,bs=32)\n",
    "\n",
    "# for batch in dls.train: # or dls.valid\n",
    "#     break\n",
    "\n",
    "# batch[0].shape, batch[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def MLP_1(num_features, \n",
    "          num_targets,\n",
    "          hidden_units = [512, 218],\n",
    "          dp = 0.2):\n",
    "    \n",
    "    # Start with the first layer from num_features to the first hidden layer\n",
    "    layers = [\n",
    "        nn.Linear(num_features, hidden_units[0]),\n",
    "        nn.BatchNorm1d(hidden_units[0]),\n",
    "        nn.Dropout(dp),\n",
    "        nn.PReLU()\n",
    "    ]\n",
    "    \n",
    "    # Loop over hidden units to create intermediate layers\n",
    "    for i in range(len(hidden_units) - 1):\n",
    "        layers.extend([\n",
    "            nn.Linear(hidden_units[i], hidden_units[i+1]),\n",
    "            nn.BatchNorm1d(hidden_units[i+1]),\n",
    "            nn.Dropout(dp),\n",
    "            nn.PReLU()\n",
    "        ])\n",
    "    \n",
    "    # Add the output layer\n",
    "    layers.append(nn.Linear(hidden_units[-1], num_targets))\n",
    "    \n",
    "    model = nn.Sequential(*layers)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_units = [512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_feat = len(feat_col)\n",
    "# num_target = len(target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MLP_1(num_feat, num_target, hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train\n",
    "# learn = Learner(dls.cuda(), model.cuda(), mse)\n",
    "\n",
    "# learn.fit_one_cycle(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CNN1D_1(Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_features, # this does not matter, just for format\n",
    "                 num_targets):\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=3, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=3, out_channels=8, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = nn.Linear(in_features = int(8 * num_features/4), out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=num_targets)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1) # need shape (bs, 1, num_features) for CNN\n",
    "        x = self.pool1(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool2(nn.functional.relu(self.conv2(x)))\n",
    "        # x = torch.flatten(x, 1)\n",
    "        x = self.flatten(x)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CNN1D_1(num_feat, num_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "def ConvBlock(in_channels, \n",
    "              out_channels, \n",
    "              kernel_size, \n",
    "              stride, \n",
    "              padding, \n",
    "              dropout_rate):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm1d(in_channels),\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.utils.weight_norm(nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)),\n",
    "        nn.ReLU()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CNN1D_2(Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_features, \n",
    "                 num_targets, \n",
    "                 hidden_size=4096):\n",
    "\n",
    "        cha_1 = 256\n",
    "        cha_2 = 512\n",
    "        cha_3 = 512\n",
    "\n",
    "        cha_1_reshape = int(hidden_size/cha_1)\n",
    "        cha_po_1 = int(hidden_size/cha_1/2)\n",
    "        cha_po_2 = int(hidden_size/cha_1/2/2) * cha_3\n",
    "\n",
    "        self.cha_1 = cha_1\n",
    "        self.cha_2 = cha_2\n",
    "        self.cha_3 = cha_3\n",
    "        self.cha_1_reshape = cha_1_reshape\n",
    "        self.cha_po_1 = cha_po_1\n",
    "        self.cha_po_2 = cha_po_2\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "\n",
    "        self.convBlock1 = ConvBlock(cha_1, cha_2, kernel_size=5, stride=1, padding=2, dropout_rate=0.1)\n",
    "\n",
    "        self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size = cha_po_1)\n",
    "\n",
    "        self.convBlock2 = ConvBlock(cha_2, cha_2, kernel_size=3, stride=1, padding=1, dropout_rate=0.1)\n",
    "        self.convBlock3 = ConvBlock(cha_2, cha_2, kernel_size=3, stride=1, padding=1, dropout_rate=0.3)\n",
    "        self.convBlock4 = ConvBlock(cha_2, cha_3, kernel_size=5, stride=1, padding=2, dropout_rate=0.2)\n",
    "\n",
    "        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.flt = nn.Flatten()\n",
    "\n",
    "        self.batch_norm3 = nn.BatchNorm1d(cha_po_2)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(cha_po_2, num_targets))\n",
    "\n",
    "        # Manual weights initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm1d): # initialize for clarification\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.celu(self.dense1(x), alpha=0.06)\n",
    "\n",
    "        x = x.reshape(x.shape[0],self.cha_1, self.cha_1_reshape)\n",
    "\n",
    "        x = self.convBlock1(x)\n",
    "\n",
    "        x = self.ave_po_c1(x)\n",
    "\n",
    "        x = self.convBlock2(x)\n",
    "        x_s = x  # Saving for later use\n",
    "\n",
    "        x = self.convBlock3(x)\n",
    "\n",
    "        x = self.convBlock4(x)\n",
    "        x = x * x_s  # Using saved value\n",
    "\n",
    "        # Max pooling\n",
    "        x = self.max_po_c2(x)\n",
    "\n",
    "        # Flatten before final block\n",
    "        x = self.flt(x)\n",
    "\n",
    "        # Final block\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CNN1D_2(num_feat, num_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_dl(df, \n",
    "            feat_col, \n",
    "            target_col,\n",
    "            split, \n",
    "            model, \n",
    "             bs = 32,\n",
    "            loss = mse, \n",
    "            save = None, # models/{save}.pth\n",
    "             params = {'n_epoch': 4} # params in fastai Learner, e.g., lr_max\n",
    "              ):\n",
    "    \n",
    "    train = df.loc[split[0]]\n",
    "    valid = df.loc[split[1]]\n",
    "    \n",
    "    train_ds = GeneralDataset(train, feat_col, target_col)\n",
    "    valid_ds = GeneralDataset(valid, feat_col, target_col)\n",
    "\n",
    "    dls = DataLoaders.from_dsets(train_ds, valid_ds, bs=bs, num_workers=4)\n",
    "    \n",
    "    learn = Learner(dls.cuda(), model.cuda(), loss, metrics= [PearsonCorrCoef(),SpearmanCorrCoef()] )\n",
    "    \n",
    "    lr_max = learn.lr_find()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(lr_max)\n",
    "    \n",
    "    if 'lr_max' not in params:\n",
    "        params['lr_max'] = lr_max\n",
    "    \n",
    "    print(f\"lr_find is :{lr_max}, params lr is {params['lr_max']}\")\n",
    "    learn.fit_one_cycle(**params)\n",
    "    \n",
    "    if save is not None:\n",
    "        learn.save(save)\n",
    "    \n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CNN1D_2(num_feat, num_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'n_epoch': 14, \n",
    "#           'lr_max': 2e-2,\n",
    "#           'cbs': [SaveModelCallback(fname = 'best')] # save best model\n",
    "#          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn = train_dl(df,feat_col,target_col, split, model, save = 'final', params = params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def predict_dl(df, \n",
    "               feat_col, \n",
    "               model, # model architecture\n",
    "               model_pth, # only name, not with .pth\n",
    "              ):\n",
    "    test_dset = GeneralDataset(df,feat_col)\n",
    "    test_dl = torch.utils.data.DataLoader(test_dset)\n",
    "    \n",
    "    \n",
    "    learn = Learner(None, model.cuda(), loss_func=1)\n",
    "    learn.load(model_pth)\n",
    "    \n",
    "    learn.model.eval()\n",
    "    \n",
    "    preds = []\n",
    "    for data in test_dl:\n",
    "        inputs = data.cuda()\n",
    "        outputs = learn.model(inputs) #learn.model(x).sigmoid().detach().cpu().numpy()\n",
    "\n",
    "        preds.append(outputs.detach().cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "    preds = pd.DataFrame(preds)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = predict_dl(df,feat_col,model,'best')\n",
    "\n",
    "# target = df[target_col]\n",
    "\n",
    "# target = pd.DataFrame(target.values)\n",
    "\n",
    "# score_all(target,pred)\n",
    "\n",
    "# score_each(target,pred).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_dl_cv(df, \n",
    "                feat_col, \n",
    "                target_col, \n",
    "                splits, \n",
    "                model_fn, # functions like lambda x: return MLP_1(num_feat, num_target)\n",
    "                save = 'model', # model name\n",
    "                bs=32, \n",
    "                params={}):\n",
    "    \n",
    "    y = df[target_col]\n",
    "    OOF = np.zeros(y.shape)  # Initialize an array with zeros, which will store the OOF predictions.\n",
    "    \n",
    "    for fold,split in enumerate(splits):\n",
    "\n",
    "        print(f'------fold{fold}------')\n",
    "        \n",
    "        model = model_fn()\n",
    "        fname = f'{save}_fold{fold}_best'\n",
    "        \n",
    "        params['cbs'] = [SaveModelCallback(fname=fname)] \n",
    "\n",
    "        learn = train_dl(df,feat_col,target_col, split, model ,bs = bs, params = params)\n",
    "\n",
    "        # predict on valid split\n",
    "        valid_df = df.loc[split[1]]\n",
    "        pred = predict_dl(valid_df,feat_col,model,fname)\n",
    "\n",
    "        # prepare target and pred\n",
    "        target = valid_df[target_col]\n",
    "        target.index, pred.index=valid_df.kinase,valid_df.kinase\n",
    "        pred.columns = target.columns\n",
    "\n",
    "        # get overall score and average score of each kinase\n",
    "        score_all(target,pred)\n",
    "        cor = score_each(target,pred)\n",
    "\n",
    "        # get OOF\n",
    "        OOF[split[1]] = pred.values\n",
    "\n",
    "    OOF = pd.DataFrame(OOF,columns=target_col)\n",
    "    \n",
    "    print('---------- score of OOF ----------------')\n",
    "    cor = score_each(y,OOF)\n",
    "    return OOF, cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_mlp():\n",
    "#     return MLP_1(num_feat, num_target)\n",
    "\n",
    "# mlp_params = {'n_epoch': 14, \n",
    "#           'lr_max': 3e-3,\n",
    "#           'cbs': [SaveModelCallback(fname = 'best')] # save best model\n",
    "#          }\n",
    "\n",
    "# OOF, cor = train_dl_cv(df,feat_col,target_col, splits, create_mlp, params = mlp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
