{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DL\n",
    "\n",
    "> A collection of deep learning tools via Fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "import sys\n",
    "sys.path.append(\"/notebooks/katlas\")\n",
    "from nbdev.showdoc import *\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastbook import *\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import spearmanr,pearsonr\n",
    "from katlas.core import *\n",
    "from katlas.feature import *\n",
    "from sklearn.model_selection import *\n",
    "from torchsummary import summary\n",
    "import torch.nn.init as init\n",
    "from katlas.train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def seed_everything(seed=123):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GeneralDataset:\n",
    "    def __init__(self, \n",
    "                 df, # a dataframe of values\n",
    "                 feat_col, # feature columns\n",
    "                 target_col=None # Will return test set for prediction if target col is None\n",
    "                ):\n",
    "        \"A general dataset that can be applied to any dataframe\"\n",
    "        \n",
    "        self.test = False if target_col is not None else True\n",
    "        \n",
    "        self.X = df[feat_col].values \n",
    "        self.y = df[target_col].values if not self.test else None\n",
    "        \n",
    "        self.len = df.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = torch.Tensor(self.X[index])\n",
    "        if self.test:\n",
    "            return X\n",
    "        else:\n",
    "            y = torch.Tensor(self.y[index])\n",
    "            return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('train/scaled_t5.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_col = df.columns[199:]\n",
    "target_col = df.columns[1:199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = Data.get_kinase_info_full()\n",
    "info = info.query('in_paper ==1')\n",
    "info = df[['kinase']].merge(info,'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
      "# kinase category in train set: 39\n",
      "# kinase category in test set: 31\n",
      "---------------------------\n",
      "# kinase in train set: 243\n",
      "---------------------------\n",
      "# kinase in test set: 60\n",
      "---------------------------\n",
      "test set: ['BMPR1B' 'CDK5' 'CDK7' 'CDK9' 'CK1G2' 'CK1G3' 'GRK7' 'MAK' 'MOK' 'NEK5' 'NEK9' 'P38B' 'P38D' 'P38G' 'PAK2' 'PAK3' 'PDHK4' 'PIM1' 'PIM3' 'PINK1' 'PKCT' 'PKCZ' 'PKN1' 'PKN3' 'PKR' 'PLK3' 'PRKD2'\n",
      " 'PRKD3' 'PRKX' 'PRP4' 'PRPK' 'RSK3' 'RSK4' 'SBK' 'SGK1' 'SGK3' 'SIK' 'SKMLCK' 'SMG1' 'SMMLCK' 'SRPK3' 'STLK3' 'TAK1' 'TAO2' 'TAO3' 'TBK1' 'TGFBR1' 'TLK2' 'TNIK' 'TSSK2' 'TTK' 'ULK2' 'VRK1' 'VRK2'\n",
      " 'WNK1' 'WNK3' 'WNK4' 'YSK1' 'YSK4' 'ZAK']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#| output: False\n",
    "splits = get_splits(info, stratified = 'category')\n",
    "split = splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.loc[split[0]]\n",
    "valid = df.loc[split[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((242, 1223), (61, 1223))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = GeneralDataset(train, feat_col, target_col)\n",
    "valid_ds = GeneralDataset(valid, feat_col, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242, 61)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds),len(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders.from_dsets(train_ds, valid_ds, num_workers=4,bs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dls.train: # or dls.valid\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1024]), torch.Size([32, 198]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape, batch[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def MLP_1(num_features, \n",
    "          num_targets,\n",
    "          dp = 0.2):\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(num_features, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.Dropout(dp),\n",
    "        nn.PReLU(),\n",
    "        nn.Linear(512, 218),\n",
    "        nn.BatchNorm1d(218),\n",
    "        nn.Dropout(dp),\n",
    "        nn.PReLU(),\n",
    "        nn.Linear(218, num_targets)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feat = len(feat_col)\n",
    "num_target = len(target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_1(num_feat, num_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 198])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(batch[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train\n",
    "# learn = Learner(dls.cuda(), model.cuda(), mse)\n",
    "\n",
    "# learn.fit_one_cycle(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CNN1D_1(Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_features, # this does not matter, just for format\n",
    "                 num_targets):\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=3, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=3, out_channels=8, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = nn.Linear(in_features = int(8 * num_features/4), out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=num_targets)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1) # need shape (bs, 1, num_features) for CNN\n",
    "        x = self.pool1(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool2(nn.functional.relu(self.conv2(x)))\n",
    "        # x = torch.flatten(x, 1)\n",
    "        x = self.flatten(x)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN1D_1(num_feat, num_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 198])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(batch[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "def ConvBlock(in_channels, \n",
    "              out_channels, \n",
    "              kernel_size, \n",
    "              stride, \n",
    "              padding, \n",
    "              dropout_rate):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm1d(in_channels),\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.utils.weight_norm(nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)),\n",
    "        nn.ReLU()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CNN1D_2(Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_features, \n",
    "                 num_targets, \n",
    "                 hidden_size=4096):\n",
    "\n",
    "        cha_1 = 256\n",
    "        cha_2 = 512\n",
    "        cha_3 = 512\n",
    "\n",
    "        cha_1_reshape = int(hidden_size/cha_1)\n",
    "        cha_po_1 = int(hidden_size/cha_1/2)\n",
    "        cha_po_2 = int(hidden_size/cha_1/2/2) * cha_3\n",
    "\n",
    "        self.cha_1 = cha_1\n",
    "        self.cha_2 = cha_2\n",
    "        self.cha_3 = cha_3\n",
    "        self.cha_1_reshape = cha_1_reshape\n",
    "        self.cha_po_1 = cha_po_1\n",
    "        self.cha_po_2 = cha_po_2\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "\n",
    "        self.convBlock1 = ConvBlock(cha_1, cha_2, kernel_size=5, stride=1, padding=2, dropout_rate=0.1)\n",
    "\n",
    "        self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size = cha_po_1)\n",
    "\n",
    "        self.convBlock2 = ConvBlock(cha_2, cha_2, kernel_size=3, stride=1, padding=1, dropout_rate=0.1)\n",
    "        self.convBlock3 = ConvBlock(cha_2, cha_2, kernel_size=3, stride=1, padding=1, dropout_rate=0.3)\n",
    "        self.convBlock4 = ConvBlock(cha_2, cha_3, kernel_size=5, stride=1, padding=2, dropout_rate=0.2)\n",
    "\n",
    "        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.flt = nn.Flatten()\n",
    "\n",
    "        self.batch_norm3 = nn.BatchNorm1d(cha_po_2)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(cha_po_2, num_targets))\n",
    "\n",
    "        # Manual weights initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm1d): # initialize for clarification\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.celu(self.dense1(x), alpha=0.06)\n",
    "\n",
    "        x = x.reshape(x.shape[0],self.cha_1, self.cha_1_reshape)\n",
    "\n",
    "        x = self.convBlock1(x)\n",
    "\n",
    "        x = self.ave_po_c1(x)\n",
    "\n",
    "        x = self.convBlock2(x)\n",
    "        x_s = x  # Saving for later use\n",
    "\n",
    "        x = self.convBlock3(x)\n",
    "\n",
    "        x = self.convBlock4(x)\n",
    "        x = x * x_s  # Using saved value\n",
    "\n",
    "        # Max pooling\n",
    "        x = self.max_po_c2(x)\n",
    "\n",
    "        # Flatten before final block\n",
    "        x = self.flt(x)\n",
    "\n",
    "        # Final block\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN1D_2(num_feat, num_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 198])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(batch[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_dl(df, \n",
    "            feat_col, \n",
    "            target_col,\n",
    "            split, \n",
    "            model, \n",
    "             bs = 32,\n",
    "            loss = mse, \n",
    "            save = None, # models/name.pth\n",
    "             params = {'n_epoch': 4} # params in fastai Learner, e.g., lr_max\n",
    "              ):\n",
    "    \n",
    "    train = df.loc[split[0]]\n",
    "    valid = df.loc[split[1]]\n",
    "    \n",
    "    train_ds = GeneralDataset(train, feat_col, target_col)\n",
    "    valid_ds = GeneralDataset(valid, feat_col, target_col)\n",
    "\n",
    "    dls = DataLoaders.from_dsets(train_ds, valid_ds, bs=bs, num_workers=4)\n",
    "    \n",
    "    learn = Learner(dls.cuda(), model.cuda(), loss, metrics= [PearsonCorrCoef(),SpearmanCorrCoef()] )\n",
    "    \n",
    "    lr_max = learn.lr_find()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(lr_max)\n",
    "    \n",
    "    if 'lr_max' not in params:\n",
    "        params['lr_max'] = lr_max\n",
    "    \n",
    "    print(f\"lr_find is :{lr_max}, params lr is {params['lr_max']}\")\n",
    "    learn.fit_one_cycle(**params)\n",
    "    \n",
    "    if save is not None:\n",
    "        learn.save(save)\n",
    "    \n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN1D_2(num_feat, num_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_epoch': 14, \n",
    "          'lr_max': 2e-2,\n",
    "          'cbs': [SaveModelCallback(fname = 'best')] # save best model\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn = train_dl(df,feat_col,target_col, split, model, save = 'final', params = params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def predict_dl(df, \n",
    "               feat_col, \n",
    "               model, # model architecture\n",
    "               model_pth, # only name, not with .pth\n",
    "              ):\n",
    "    test_dset = GeneralDataset(df,feat_col)\n",
    "    test_dl = torch.utils.data.DataLoader(test_dset)\n",
    "    \n",
    "    \n",
    "    learn = Learner(None, model.cuda(), loss_func=1)\n",
    "    learn.load(model_pth)\n",
    "    \n",
    "    learn.model.eval()\n",
    "    \n",
    "    preds = []\n",
    "    for data in test_dl:\n",
    "        inputs = data.cuda()\n",
    "        outputs = learn.model(inputs) #learn.model(x).sigmoid().detach().cpu().numpy()\n",
    "\n",
    "        preds.append(outputs.detach().cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "    preds = pd.DataFrame(preds)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = predict_dl(df,feat_col,model,'best')\n",
    "\n",
    "# target = df[target_col]\n",
    "\n",
    "# target = pd.DataFrame(target.values)\n",
    "\n",
    "# score_all(target,pred)\n",
    "\n",
    "# score_each(target,pred).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
