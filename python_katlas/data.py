"""Load various kinase-relatd datasets"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_data.ipynb.

# %% auto #0
__all__ = ['Data', 'CPTAC']

# %% ../nbs/00_data.ipynb #34ccf9ea
import pandas as pd
from functools import lru_cache
from fastcore.all import patch,patch_to

import gdown,zipfile,shutil,tempfile
from pathlib import Path

# %% ../nbs/00_data.ipynb #a45a993f
class Data:
    "A class for fetching various datasets."
    DATASET_DIR = Path(tempfile.gettempdir()) / 'katlas_dataset'

# %% ../nbs/00_data.ipynb #a5f9e650
@patch_to(Data)
def set_dir(path):
    Data.DATASET_DIR = Path(path)/'katlas_dataset'
    print(f"âœ… Dataset path set to: {Data.DATASET_DIR}")

# %% ../nbs/00_data.ipynb #8e7b93db
@patch_to(Data)
def download(
    download_dir=None,   # Parent directory for katlas_dataset folder
    force=False,          # If True, re-download even if exists
    verbose=True,         # Print status messages
):
    """Download dataset zip and extract to folder."""
    
    url = "https://drive.google.com/uc?id=17wIl0DbdoHV036Z3xgaT_0H3LlM_W47l"

    # Resolve dataset directory
    if download_dir is not None:
        Data.DATASET_DIR = Path(download_dir).expanduser().resolve() / "katlas_dataset"

    dataset_dir = Data.DATASET_DIR
    zip_path = dataset_dir.parent / "katlas_dataset.zip"

    # Existing dataset handling
    if dataset_dir.exists():
        if force:
            if verbose:
                print(f"â™»ï¸ Removing existing folder: {dataset_dir}")
            shutil.rmtree(dataset_dir)
        else:
            if verbose:
                print(f"âœ… Dataset exists at: {dataset_dir}")
            return

    dataset_dir.mkdir(parents=True, exist_ok=True)

    # Download
    if verbose:
        print(f"â¬‡ï¸ Downloading katlas_dataset.zip ...")

    downloaded_file = gdown.download(url, output=str(zip_path), quiet=not verbose)

    if downloaded_file is None or not Path(downloaded_file).exists():
        raise RuntimeError(
            "Dataset download failed. "
            "Please check your internet connection or Google Drive permissions."
        )

    # Safe extraction (zip-slip protection)
    if verbose:
        print(f"ðŸ“‚ Extracting to {dataset_dir} ...")

    with zipfile.ZipFile(downloaded_file, "r") as zip_ref:
        for member in zip_ref.namelist():
            member_path = dataset_dir / member
            if not member_path.resolve().is_relative_to(dataset_dir.resolve()):
                raise RuntimeError(f"Unsafe zip file detected (zip-slip): {member}")
        zip_ref.extractall(dataset_dir)

    # Cleanup
    try:
        if verbose:
            print(f"ðŸ§¹ Removing zip file: {downloaded_file}")
        Path(downloaded_file).unlink()
    except Exception as e:
        if verbose:
            print(f"âš ï¸ Could not remove {downloaded_file}: {e}")

    if verbose:
        print(f"âœ… Done! Extracted dataset is at: {dataset_dir}")

# %% ../nbs/00_data.ipynb #75076a00
@patch_to(Data)
def read_file(rel_path):
    """
    Load a CSV or Parquet file from the local dataset folder.

    Automatically infers file type from the filename extension.
    Renames 'Unnamed: 0' column to 'kinase' if present.
    """
    Data.download(verbose=False)
    path = Data.DATASET_DIR / rel_path
    
    if not path.exists(): raise FileNotFoundError(f"Dataset file not found: {path}")
    
    ext = path.suffix.lower()
    
    if ext == '.csv': df = pd.read_csv(path)
    elif ext == '.parquet': df = pd.read_parquet(path)  # Let pandas choose engine automatically
    else: raise ValueError(f"Unsupported file type: {ext}. Supported types: .csv, .parquet")
    
    return df.rename(columns={"Unnamed: 0": "kinase"}) if "Unnamed: 0" in df.columns else df

# %% ../nbs/00_data.ipynb #36b703e4
@patch_to(Data)
def get_kinase_info():
    """
    Get information of 523 human kinases on kinome tree. 
    Group, family, and subfamily classifications are sourced from Coral; 
    full protein sequences are retrieved using UniProt IDs; 
    kinase domain sequences are obtained from KinaseDomain.com; 
    and cellular localization data is extracted from published literature.
    """
    return Data.read_file("kinase_info.csv")

# %% ../nbs/00_data.ipynb #03b7efd0
@patch_to(Data)
def get_kinase_uniprot() -> pd.DataFrame:
    """
    Get information of 672 uniprot human kinases, which were retrieved from UniProt by filtering all human protein entries using the keyword 'kinase'. 
    It includes additional pseudokinases and lipid kinases.
    """
    path = "uniprot_human_keyword_kinase.parquet"
    return Data.read_file(path)

# %% ../nbs/00_data.ipynb #d20cc783
@patch_to(Data)
def get_kd_uniprot():
    "Kinase domains extracted from UniProt database. "
    path = "uniprot_kd_labeled.parquet"
    return Data.read_file(path)

# %% ../nbs/00_data.ipynb #aef6622b
@patch_to(Data)
@lru_cache
def get_pspa_tyr():
    """Get PSPA normalized data of tyrosine kinase."""
    path = "PSPA/pspa_tyr_norm.parquet"
    return Data.read_file(path)

# %% ../nbs/00_data.ipynb #cec0e3f4
@patch_to(Data)
@lru_cache
def get_pspa_st():
    """Get PSPA normalized data of serine/threonine kinase."""
    path = "PSPA/pspa_st_norm.parquet"
    return Data.read_file(path)

# %% ../nbs/00_data.ipynb #fa155596
@patch_to(Data)
@lru_cache
def get_pspa() -> pd.DataFrame:
    """Get PSPA normalized data of serine/threonine and tyrosine kinases.
    pS is duplicate of pT; 
    For all kinases:
        -pS is duplicate of pT 
    S/T kinases:
        - Column normalized to 17 (20 standard-S,T,C) or 16 (20-S,T,C,Y;PDHK1 & PDHK4) amino acids.
        - C is median of 17 or 16 (PDHK1 & PDHK4) amino acids
    TK (including _TYR which is non-canonical):
        - Column normalized to 18 (20-Y,C; canonical +TNNI3K,WEE1) or 16 (20-S,T,Y,C; IRR, JAK3, MST1R + non-canonical _TYR) aa.
        - Y is duplicate of F;
        - C is median of 18 or 16 amino acids.
"""
    path = "PSPA/pspa_all_norm.parquet"
    return Data.read_file(path)

# %% ../nbs/00_data.ipynb #c9c0011e
@patch_to(Data)
@lru_cache
def get_pspa_raw(include_s=False, # pS is duplicate of pT
                ) -> pd.DataFrame:
    """Get PSPA raw data of serine/threonine and tyrosine kinases.
    pS is duplicate of pT; 
    """
    path = "PSPA/pspa_all_raw.parquet"
    df = Data.read_file(path)
    if not include_s:
        df = df.loc[:,~df.columns.str.endswith("s")]
    return df.copy()

# %% ../nbs/00_data.ipynb #390f8e41
@patch_to(Data)
@lru_cache
def get_pspa_scale():
    """
    Get PSPA (-5 to +4) scaled data from PSPA normalized data. 
    Each position (including both pS/pT and pS=pT) are normalized to 1.
    """
    path = "PSPA/pspa_all_scale.parquet"
    return Data.read_file(path)

# %% ../nbs/00_data.ipynb #2bf9fd7f
@patch_to(Data)
@lru_cache
def get_pspa_st_pct():
    """Get PSPA reference score to calculate percentile for serine/threonine kinases."""
    path = "PSPA/pspa_pct_st.parquet"
    return Data.read_file(path)

# %% ../nbs/00_data.ipynb #fe09c492
@patch_to(Data)
@lru_cache
def get_pspa_tyr_pct():
    """Get PSPA reference score to calculate percentile for tyrosine kinases."""
    path = "PSPA/pspa_pct_tyr.parquet"
    return Data.read_file(path)

# %% ../nbs/00_data.ipynb #f12962c3
@patch_to(Data)
@lru_cache
def get_num_dict():
    """Get a dictionary mapping kinase to number of random amino acids in PSPA."""
    path = "PSPA/pspa_divide_num.csv"
    return Data.read_file(path).set_index("kinase")["num_random_aa"].to_dict()

# %% ../nbs/00_data.ipynb #95e100e5
@patch_to(Data)
def get_ks_unique():
    """Get kinase substrate dataset with unique sub site ID."""
    path = "CDDM/unique_ks_sites.parquet"
    return Data.read_file(path)

# %% ../nbs/00_data.ipynb #3966fade
@patch_to(Data)
def get_ks_dataset(add_kinase_info=True):
    """
    Get kinaseâ€“substrate dataset collected from public resources,
    with the option of enriching with kinase info.
    """
    # --- 1ï¸âƒ£ Load and preprocess dataset ---
    path = "CDDM/ks_datasets_20250407.parquet"
    df = Data.read_file(path)

    # Convert numeric-looking column names once
    df.columns = [
        int(c) if isinstance(c, str) and c.lstrip("-").isdigit() else c
        for c in df.columns
    ]

    if "substrate_phosphoseq" in df.columns:
        df["substrate_sequence"] = df["substrate_phosphoseq"].str.upper()

    if not add_kinase_info:
        return df

    # --- 2ï¸âƒ£ Prepare kinase info (dedup, indexed maps) ---
    info = (
        Data.get_kinase_info()
        .sort_values("kinase")
        .drop_duplicates("uniprot")
        .set_index("uniprot")
    )

    # Extract clean UniProt IDs (remove isoforms)
    df["uniprot_clean"] = df["kinase_uniprot"].str.split("-", n=1).str[0]

    # Create mapping dicts once
    maps = {
        "kinase_group": info["modi_group"].to_dict(),
        "kinase_family": info["family"].to_dict(),
        "kinase_subfamily": info["subfamily"].to_dict(),
        "kinase_pspa_big": info["pspa_category_big"].to_dict(),
        "kinase_pspa_small": info["pspa_category_small"].to_dict(),
        "kinase_coral_ID": info["ID_coral"].to_dict(),
        "kinase_protein": info["kinase"].to_dict(),
    }

    # Preload gene name map once
    gene_map = Data.get_kinase_uniprot().set_index("Entry")["Gene Names"].to_dict()

    # --- 3ï¸âƒ£ Vectorized assignment ---
    df["kinase_on_tree"] = df["uniprot_clean"].isin(info.index).astype(int)
    df["kinase_genes"] = df["uniprot_clean"].map(gene_map)

    # Assign all kinase attributes in a loop instead of repeated `.map()`
    for col, mapping in maps.items():
        df[col] = df["uniprot_clean"].map(mapping)

    # --- 4ï¸âƒ£ Add num_kin from ks_unique ---
    site_info = Data.get_ks_unique()[["sub_site", "num_kin"]].set_index("sub_site")
    df["num_kin"] = df["sub_site"].map(site_info["num_kin"])

    # Clean up
    df.drop(columns="uniprot_clean", inplace=True)
    return df

# %% ../nbs/00_data.ipynb #32d187d0
@patch_to(Data)
@lru_cache
def get_ks_background():
    """Get kinase substrate dataset with unique sub site ID."""
    path = "CDDM/ks_background.parquet"
    return Data.read_file(path)

# %% ../nbs/00_data.ipynb #ab60578b
@patch_to(Data)
@lru_cache
def get_cddm():
    """Get the CDDM dataset."""
    path = "CDDM/pssms.parquet"
    return Data.read_file(path)

# %% ../nbs/00_data.ipynb #43b8a96e
@patch_to(Data)
@lru_cache
def get_cddm_upper():
    """Get the CDDM dataset of all uppercase sequence."""
    path = "CDDM/pssms_upper.parquet"
    return Data.read_file(path)

# %% ../nbs/00_data.ipynb #12495078
@patch_to(Data)
@lru_cache
def get_cddm_LO():
    """Get CDDM Log-odds data with 'STY' background."""
    path = "CDDM/pssms_LO.parquet"
    return Data.read_file(path)

# %% ../nbs/00_data.ipynb #1156d798
@patch_to(Data)
@lru_cache
def get_cddm_LO_upper():
    """Get CDDM Log-odds data of all-uppercase sequence with 'STY' background."""
    path = "CDDM/pssms_LO_upper.parquet"
    return Data.read_file(path)

# %% ../nbs/00_data.ipynb #53011b71
@patch_to(Data)
def get_aa_info():
    """Get amino acid information."""
    path = f"amino_acids/aa_info.parquet"
    return Data.read_file(path)

# %% ../nbs/00_data.ipynb #9af474dd
@patch_to(Data)
def get_aa_rdkit():
    """Get RDKit representations of amino acids."""
    path = "amino_acids/aa_rdkit.parquet"
    return Data.read_file(path)

# %% ../nbs/00_data.ipynb #0193809d
@patch_to(Data)
def get_aa_morgan():
    """Get Morgan fingerprint representations of amino acids."""
    path = "amino_acids/aa_morgan.parquet"
    return Data.read_file(path)

# %% ../nbs/00_data.ipynb #d3af40c7
@patch_to(Data)
def get_cptac_ensembl_site():
    """Get CPTAC dataset with unique EnsemblProteinID+site."""
    path = "phosphosites/linkedOmicsKB_ref_pan.parquet"
    return Data.read_file(path)

# %% ../nbs/00_data.ipynb #ef3f1ccf
@patch_to(Data)
def get_cptac_unique_site():
    """Get CPTAC dataset with unique site sequences."""
    path = "phosphosites/cptac_unique_site.parquet"
    return Data.read_file(path)

# %% ../nbs/00_data.ipynb #9dfb987b
@patch_to(Data)
def get_cptac_gene_site():
    """Get CPTAC dataset with unique Gene+site."""
    path = "phosphosites/linkedOmics_ref_pan.parquet"
    return Data.read_file(path)

# %% ../nbs/00_data.ipynb #6bf2435b
@patch_to(Data)
def get_psp_human_site():
    """Get PhosphoSitePlus human dataset (Gene+site)."""
    path = "phosphosites/psp_human.parquet"
    return Data.read_file(path)

# %% ../nbs/00_data.ipynb #39e15af0
@patch_to(Data)
def get_ochoa_site():
    """Get phosphoproteomics dataset from Ochoa et al."""
    path = "phosphosites/ochoa_site.parquet"
    return Data.read_file(path)

# %% ../nbs/00_data.ipynb #9cabd935
@patch_to(Data)
def get_combine_site_psp_ochoa() -> pd.DataFrame:
    """
    Get the combined dataset from Ochoa and PhosphoSitePlus.
    """
    path = "phosphosites/combine_site_psp_ochoa.parquet"
    return Data.read_file(path)

# %% ../nbs/00_data.ipynb #c3b636a4
@patch_to(Data)
def get_combine_site_phosphorylated():
    """
    Get the combined phosphorylated dataset from Ochoa and PhosphoSitePlus.
    """
    path = "phosphosites/phosphorylated_combine_site.parquet"
    return Data.read_file(path)

# %% ../nbs/00_data.ipynb #ba1afbae
@patch_to(Data)
@lru_cache
def get_human_site():
    """
    Get the combined phosphorylated dataset from Ochoa and PhosphoSitePlus (20-length version).
    """
    path = "phosphosites/phosphorylated_combine_site20.parquet"
    return Data.read_file(path)

# %% ../nbs/00_data.ipynb #c5f1296e
@patch_to(Data)
@lru_cache
def get_reactome_pathway_lo() -> pd.DataFrame:
    """
    Get lowest reactome pathways with Uniprot ID as identifier.
    """
    path = "reactome_lowest_level.parquet"
    return Data.read_file(path)

# %% ../nbs/00_data.ipynb #e08802eb
@patch_to(Data)
@lru_cache
def get_reactome_pathway() -> pd.DataFrame:
    """
    Get all level reactome pathways with Uniprot ID as identifier.
    """
    path = "reactome_all_levels.parquet"
    path_all = Data.read_file(path)
    # path_lo = Data.get_reactome_pathway_lo()
    # path_all['lowest'] = path_all.reactome_id.isin(path_lo.reactome_id).astype(int)
    return path_all

# %% ../nbs/00_data.ipynb #5e04dfc3
class CPTAC:
    
    "A class for fetching CPTAC phosphoproteomics data."
    @staticmethod
    def _read_file(cancer: str, # cancer type CPTAC
                    is_Tumor: bool=True, # tumor tissue or normal
                    is_KB: bool=False, # whether it is for LinkedOmicsKB or LinkedOmics
                   ):
        "Fetches the data from the given path and returns a DataFrame"
        
        # path of ID and data
        sample_type = "Tumor" if is_Tumor else "Normal"
        ID_URL = f"https://zenodo.org/records/8196130/files/bcm-{cancer.lower()}-mapping-gencode.v34.basic.annotation-mapping.txt.gz"
        DATA_URL = f"https://cptac-pancancer-data.s3.us-west-2.amazonaws.com/data_freeze_v1.2_reorganized/{cancer.upper()}/{cancer.upper()}_phospho_site_abundance_log2_reference_intensity_normalized_{sample_type}.txt"

        # Load ID data
        ref = pd.read_csv(ID_URL, compression='gzip', sep='\t')[['protein','gene','gene_name']].drop_duplicates().reset_index(drop=True)
        
        # Load CPTAC phosphoproteomics data
        try:
            raw = pd.read_csv(DATA_URL, sep='\t')
        except Exception as e:
            print(f'{cancer} has {e}')
        else:
            info = pd.DataFrame({'gene':raw.idx.str.split('|').str[0],
                                 'site':raw.idx.str.split('|').str[2],
                                 'site_seq':raw.idx.str.split('|').str[3]})

            print(f'the {cancer} dataset length is: {info.shape[0]}')

            # Merge ensembl ID with gene name
            info = info.merge(ref,'left')
            print(f'after id mapping, the length is {info.shape[0]}')

            print(f'{info.gene_name.isna().sum()} sites does not have a mapped gene name')

            info['gene_site'] = info['gene_name'] + '_' + info['site']
            info['protein_site'] = info['protein'].str.split('.').str[0] + '_' + info['site']
            
            info = info.drop_duplicates(subset="protein_site" if is_KB else "gene_site").reset_index(drop=True)
            print(f'after removing duplicates of protein_site, the length is {info.shape[0]}')

            return info
    

# %% ../nbs/00_data.ipynb #53e305a9
@patch_to(CPTAC)
def list_cancer():
    "List available CPTAC cancer type"
    return ['HNSCC','GBM','COAD','CCRCC','LSCC','BRCA','UCEC','LUAD','PDAC','OV']

# %% ../nbs/00_data.ipynb #ce91102e
@patch_to(CPTAC)
def get_id(cancer_type: str,
           is_Tumor: bool=True, # tumor tissue or normal
           is_KB: bool=False, # whether it is for LinkedOmicsKB or LinkedOmics
          ):
    "Get CPTAC phosphorylation sites information given a cancer type"
    if cancer_type not in CPTAC.list_cancer(): raise ValueError("cancer type is not included, check available cancer types from CPTAC.list_cancer()")
    return CPTAC._read_file(cancer_type,is_Tumor, is_KB)
