"""Core functions in Katlas library"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_core.ipynb.

# %% auto 0
__all__ = ['Data', 'CPTAC', 'check_seq', 'check_seq_df', 'validate_site', 'validate_site_df', 'onehot_encode', 'multiply_func',
           'multiply', 'sumup', 'STY2sty', 'get_dict', 'predict_kinase', 'Params', 'cut_seq', 'predict_kinase_df',
           'get_pct', 'get_pct_df', 'phosphorylate_seq', 'phosphorylate_seq_df', 'extract_site_seq', 'get_prob',
           'pssm_to_seq', 'recover_pssm', 'process_pssm', 'pssm2dict', 'js_divergence', 'js_divergence_flat', 'entropy',
           'entropy_flat', 'get_IC_standard', 'get_IC', 'get_IC_flat', 'get_scaled_IC', 'get_pvalue', 'get_metaP',
           'raw2norm', 'get_one_kinase']

# %% ../nbs/00_core.ipynb 3
import math, pandas as pd, numpy as np
from tqdm import tqdm
from scipy.stats import chi2
from typing import Callable
from functools import partial
from scipy.stats import ttest_ind, mannwhitneyu, wilcoxon,chi2
from statsmodels.stats.multitest import multipletests
from functools import lru_cache
from sklearn.cluster import KMeans
from sklearn.preprocessing import OneHotEncoder
from fastcore.meta import delegates

# %% ../nbs/00_core.ipynb 8
class Data:
    """A class for fetching various datasets."""
    
    @staticmethod
    @lru_cache(maxsize=None)
    def fetch_data(url: str) -> pd.DataFrame:
        """
        Fetch data from the given URL and return a DataFrame.
        Renames 'Unnamed: 0' column to 'kinase' if present.
        """
        df = pd.read_parquet(url)
        if "Unnamed: 0" in df.columns:
            df = df.rename(columns={"Unnamed: 0": "kinase"})
        return df

    @staticmethod
    @lru_cache(maxsize=None)
    def fetch_csv(url: str) -> pd.DataFrame:
        """
        Fetch data from the given URL and return a DataFrame.
        Renames 'Unnamed: 0' column to 'kinase' if present.
        """
        df = pd.read_csv(url)
        if "Unnamed: 0" in df.columns:
            df = df.rename(columns={"Unnamed: 0": "kinase"})
        return df
    

    @staticmethod
    def _convert_numeric_columns(df: pd.DataFrame) -> pd.DataFrame:
        """
        Convert column names that are numeric strings into integers,
        but only if they are still strings.
        """
        df.columns = [int(col) if isinstance(col, str) and col.lstrip('-').isdigit() else col for col in df.columns]
        return df

    BASE_URL = "https://github.com/sky1ove/katlas/raw/main/"
    
    #--------------------------- Kinase and PSPA ---------------------------
    @staticmethod
    def get_kinase_info() -> pd.DataFrame:
        """
        Get information of 523 human kinases on kinome tree. 
        Group, family, and subfamily classifications are sourced from Coral; 
        full protein sequences are retrieved using UniProt IDs; 
        kinase domain sequences are obtained from KinaseDomain.com; 
        and cellular localization data is extracted from published literature.
        """
        URL = f"{Data.BASE_URL}dataset/kinase_info.csv"
        return Data.fetch_csv(URL)

    @staticmethod
    def get_kinase_uniprot() -> pd.DataFrame:
        """
        Get information of 672 uniprot human kinases, which were retrieved from UniProt by filtering all human protein entries using the keyword 'kinase'. 
        It includes additional pseudokinases and lipid kinases.
        """
        URL = f"{Data.BASE_URL}dataset/uniprot_human_keyword_kinase.parquet"
        return Data.fetch_data(URL)

    @staticmethod
    def get_pspa_tyr_norm() -> pd.DataFrame:
        """Get PSPA tyrosine kinase normalized data."""
        URL = f"{Data.BASE_URL}dataset/PSPA/pspa_tyr_norm.parquet"
        return Data.fetch_data(URL)

    @staticmethod
    def get_pspa_st_norm() -> pd.DataFrame:
        """Get PSPA serine/threonine kinase normalized data."""
        URL = f"{Data.BASE_URL}dataset/PSPA/pspa_st_norm.parquet"
        return Data.fetch_data(URL)

    @staticmethod
    def get_pspa_all_norm() -> pd.DataFrame:
        """Get PSPA combined normalized data for serine/threonine and tyrosine kinases."""
        URL = f"{Data.BASE_URL}dataset/PSPA/pspa_all_norm.parquet"
        return Data.fetch_data(URL)

    @staticmethod
    def get_pspa_st_pct() -> pd.DataFrame:
        """Get PSPA scoring for serine/threonine kinases."""
        URL = f"{Data.BASE_URL}dataset/PSPA/pspa_pct_st.parquet"
        return Data.fetch_data(URL)

    @staticmethod
    def get_pspa_tyr_pct() -> pd.DataFrame:
        """Get PSPA scoring for tyrosine kinases."""
        URL = f"{Data.BASE_URL}dataset/PSPA/pspa_pct_tyr.parquet"
        return Data.fetch_data(URL)

    @staticmethod
    def get_num_dict() -> dict:
        """Get a dictionary mapping kinase to number of random amino acids."""
        URL = f"{Data.BASE_URL}dataset/PSPA/pspa_divide_num.csv"
        num = pd.read_csv(URL)
        return num.set_index("kinase")["num_random_aa"].to_dict()

    #--------------------------- CDDM ---------------------------
    @staticmethod
    def get_ks_dataset(add_kinase_info=True) -> pd.DataFrame:
        """Get kinase substrate dataset with numeric columns converted."""
        URL = f"{Data.BASE_URL}dataset/CDDM/ks_datasets_20250407.parquet"
        df = Data.fetch_data(URL)
        df = Data._convert_numeric_columns(df)
        if 'substrate_phosphoseq' in df.columns:
            df['substrate_sequence'] = df['substrate_phosphoseq'].str.upper()

        if add_kinase_info:
            # Remove pseudokinase duplicates by UniProt ID, keep only one entry per kinase
            info = Data.get_kinase_info().sort_values('kinase').drop_duplicates('uniprot')
            
            # Pre-extract UniProt ID without isoform for matching
            df['uniprot_clean'] = df['kinase_uniprot'].str.split('-').str[0]
            
            info_indexed = info.set_index('uniprot')
            group_map = info_indexed['group']
            family_map = info_indexed['family']
            pspa_small_map = info_indexed['pspa_category_small']
            pspa_big_map = info_indexed['pspa_category_big']
            
            df['kinase_on_tree'] = df['uniprot_clean'].isin(info['uniprot']).astype(int)
            
            kinase_gene_map = Data.get_kinase_uniprot().set_index('Entry')['Gene Names']
            df['kinase_genes'] = df['uniprot_clean'].map(kinase_gene_map)
            
            df['kinase_group'] = df['uniprot_clean'].map(group_map)
            df['kinase_family'] = df['uniprot_clean'].map(family_map)
            df['kinase_pspa_big'] = df['uniprot_clean'].map(pspa_big_map)
            df['kinase_pspa_small'] = df['uniprot_clean'].map(pspa_small_map)
            
            df.drop(columns='uniprot_clean', inplace=True)
        return df

    @staticmethod
    def get_ks_unique() -> pd.DataFrame:
        """Get kinase substrate dataset with unique site sequence (most phosphorylated version)."""
        URL = f"{Data.BASE_URL}dataset/CDDM/ks_datasets_seq_unique_20250407.parquet"
        return Data.fetch_data(URL)

    @staticmethod
    def get_cddm() -> pd.DataFrame:
        """Get the primary CDDM dataset."""
        URL = f"{Data.BASE_URL}dataset/CDDM/ks_main.parquet"
        return Data.fetch_data(URL)

    @staticmethod
    def get_cddm_upper() -> pd.DataFrame:
        """Get the upper CDDM dataset."""
        URL = f"{Data.BASE_URL}dataset/CDDM/ks_main_upper.parquet"
        return Data.fetch_data(URL)

    @staticmethod
    def get_cddm_others() -> pd.DataFrame:
        """Get CDDM data for other kinases with mutations."""
        URL = f"{Data.BASE_URL}dataset/CDDM/ks_others.parquet"
        return Data.fetch_data(URL)

    @staticmethod
    def get_cddm_others_info() -> pd.DataFrame:
        """Get additional information for CDDM 'others' dataset."""
        URL = f"{Data.BASE_URL}dataset/CDDM/ks_others_info.parquet"
        return Data.fetch_data(URL)

    @staticmethod
    def get_combine() -> pd.DataFrame:
        """Get the combined PSPA and CDDM dataset."""
        URL = f"{Data.BASE_URL}dataset/combine_main.parquet"
        return Data.fetch_data(URL)

    #--------------------------- Amino Acid ---------------------------
    @staticmethod
    def get_aa_info() -> pd.DataFrame:
        """Get amino acid information."""
        URL = f"{Data.BASE_URL}dataset/amino_acids/aa_info.parquet"
        return Data.fetch_data(URL)

    @staticmethod
    def get_aa_rdkit() -> pd.DataFrame:
        """Get RDKit representations of amino acids."""
        URL = f"{Data.BASE_URL}dataset/amino_acids/aa_rdkit.parquet"
        return Data.fetch_data(URL)

    @staticmethod
    def get_aa_morgan() -> pd.DataFrame:
        """Get Morgan fingerprint representations of amino acids."""
        URL = f"{Data.BASE_URL}dataset/amino_acids/aa_morgan.parquet"
        return Data.fetch_data(URL)

    #--------------------------- Phosphoproteomics ---------------------------
    @staticmethod
    def get_cptac_ensembl_site() -> pd.DataFrame:
        """Get CPTAC dataset with unique EnsemblProteinID+site."""
        URL = f"{Data.BASE_URL}dataset/phosphosites/linkedOmicsKB_ref_pan.parquet"
        return Data.fetch_data(URL)

    @staticmethod
    def get_cptac_unique_site() -> pd.DataFrame:
        """Get CPTAC dataset with unique site sequences."""
        URL = f"{Data.BASE_URL}dataset/phosphosites/cptac_unique_site.parquet"
        return Data.fetch_data(URL)

    @staticmethod
    def get_cptac_gene_site() -> pd.DataFrame:
        """Get CPTAC dataset with unique Gene+site."""
        URL = f"{Data.BASE_URL}dataset/phosphosites/linkedOmics_ref_pan.parquet"
        return Data.fetch_data(URL)

    @staticmethod
    def get_psp_human_site() -> pd.DataFrame:
        """Get PhosphoSitePlus human dataset (Gene+site)."""
        URL = f"{Data.BASE_URL}dataset/phosphosites/psp_human.parquet"
        return Data.fetch_data(URL)

    @staticmethod
    def get_ochoa_site() -> pd.DataFrame:
        """Get dataset from Ochoa et al."""
        URL = f"{Data.BASE_URL}dataset/phosphosites/ochoa_site.parquet"
        return Data.fetch_data(URL)

    @staticmethod
    def get_combine_site_psp_ochoa() -> pd.DataFrame:
        """
        Get the combined dataset from Ochoa and PhosphoSitePlus,
        converting numeric column names where applicable.
        """
        URL = f"{Data.BASE_URL}dataset/phosphosites/combine_site_psp_ochoa.parquet"
        df = Data.fetch_data(URL)
        return Data._convert_numeric_columns(df)

    @staticmethod
    def get_combine_site_phosphorylated() -> pd.DataFrame:
        """
        Get the combined phosphorylated dataset from Ochoa and PhosphoSitePlus,
        with numeric column names converted.
        """
        URL = f"{Data.BASE_URL}dataset/phosphosites/phosphorylated_combine_site.parquet"
        df = Data.fetch_data(URL)
        return Data._convert_numeric_columns(df)

    @staticmethod
    def get_human_site() -> pd.DataFrame:
        """
        Get the combined phosphorylated dataset from Ochoa and PhosphoSitePlus (20-column version),
        with numeric column names converted.
        """
        URL = f"{Data.BASE_URL}dataset/phosphosites/phosphorylated_combine_site20.parquet"
        df = Data.fetch_data(URL)
        return Data._convert_numeric_columns(df)


# %% ../nbs/00_core.ipynb 39
class CPTAC:
    
    "A class for fetching CPTAC phosphoproteomics data."
    @staticmethod
    def _fetch_data(cancer: str, # cancer type CPTAC
                    is_Tumor: bool=True, # tumor tissue or normal
                    is_KB: bool=False, # whether it is for LinkedOmicsKB or LinkedOmics
                   ):
        "Fetches the data from the given URL and returns a DataFrame"
        
        # URL of ID and data
        sample_type = "Tumor" if is_Tumor else "Normal"
        ID_URL = f"https://zenodo.org/records/8196130/files/bcm-{cancer.lower()}-mapping-gencode.v34.basic.annotation-mapping.txt.gz"
        DATA_URL = f"https://cptac-pancancer-data.s3.us-west-2.amazonaws.com/data_freeze_v1.2_reorganized/{cancer.upper()}/{cancer.upper()}_phospho_site_abundance_log2_reference_intensity_normalized_{sample_type}.txt"

        # Load ID data
        ref = pd.read_csv(ID_URL, compression='gzip', sep='\t')[['protein','gene','gene_name']].drop_duplicates().reset_index(drop=True)
        
        # Load CPTAC phosphoproteomics data
        try:
            raw = pd.read_csv(DATA_URL, sep='\t')
        except Exception as e:
            print(f'{cancer} has {e}')
        else:
            info = pd.DataFrame({'gene':raw.idx.str.split('|').str[0],
                                 'site':raw.idx.str.split('|').str[2],
                                 'site_seq':raw.idx.str.split('|').str[3]})

            print(f'the {cancer} dataset length is: {info.shape[0]}')

            # Merge ensembl ID with gene name
            info = info.merge(ref,'left')
            print(f'after id mapping, the length is {info.shape[0]}')

            print(f'{info.gene_name.isna().sum()} sites does not have a mapped gene name')

            info['gene_site'] = info['gene_name'] + '_' + info['site']
            info['protein_site'] = info['protein'].str.split('.').str[0] + '_' + info['site']
            
            info = info.drop_duplicates(subset="protein_site" if is_KB else "gene_site").reset_index(drop=True)
            print(f'after removing duplicates of protein_site, the length is {info.shape[0]}')

            return info
    
    
    @staticmethod
    def list_cancer():
        "Get available CPTAC cancer type"
        return ['HNSCC','GBM','COAD','CCRCC','LSCC','BRCA','UCEC','LUAD','PDAC','OV']

    @staticmethod
    def get_id(cancer_type: str,
               is_Tumor: bool=True, # tumor tissue or normal
               is_KB: bool=False, # whether it is for LinkedOmicsKB or LinkedOmics
              ):
        "Get CPTAC phosphorylation sites information given a cancer type"
        assert cancer_type in CPTAC.list_cancer(), "cancer type is not included, check available cancer types from CPTAC.list_cancer()"
        return CPTAC._fetch_data(cancer_type,is_Tumor, is_KB)

# %% ../nbs/00_core.ipynb 46
def check_seq(seq):
    """Convert non-s/t/y characters to uppercase and replace disallowed characters with underscores."""
    acceptor = seq[len(seq) // 2]
    assert acceptor.lower() in {'s', 't', 'y'}, f"{seq} has {acceptor} at position {len(seq) // 2}; need to have one of 's', 't', or 'y' in the center"

    allowed_chars = set("PGACSTVILMFYWHKRQNDEsty")
    return "".join(char if char in {'s', 't', 'y'} else (char.upper() if char.upper() in allowed_chars else '_') for char in seq)

# %% ../nbs/00_core.ipynb 49
def check_seq_df(df,col):
    "Convert non-s/t/y to upper case & replace with underscore if the character is not in the allowed set"
    assert len(df[col].str.len().value_counts())==1, 'inconsistent sequence length detected'
    return df[col].apply(check_seq)

# %% ../nbs/00_core.ipynb 52
def validate_site(site_info,
                  seq):
    "Validate site position residue match with site residue."
    pos=int(site_info[1:])-1 # python index starts from zero
    if pos >= len(seq) or pos < 0: 
        return int(False)
    return int(seq[pos]==site_info[0])

# %% ../nbs/00_core.ipynb 55
def validate_site_df(df, 
                     site_info_col,
                     protein_seq_col): 
    "Validate site position residue match with site residue in a dataframe."
    return df.apply(lambda r: validate_site(r[site_info_col],r[protein_seq_col]) , axis=1)

# %% ../nbs/00_core.ipynb 58
def onehot_encode(sequences, transform_colname=True, n=20):
    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
    encoded_array = encoder.fit_transform([list(seq) for seq in sequences])
    colnames = [x[1:] for x in encoder.get_feature_names_out()]
    if transform_colname:
        colnames = [f"{int(item.split('_', 1)[0]) - 20}{item.split('_', 1)[1]}" for item in colnames]
    encoded_df = pd.DataFrame(encoded_array, columns=colnames)
    return encoded_df

# %% ../nbs/00_core.ipynb 64
def multiply_func(values, # list of values, possibilities of amino acids at certain positions
             factor=17, # scale factor
            ):
    
    "Multiply the possibilities of the amino acids at each position in a phosphorylation site"
    

    # Using the logarithmic property: log(a*b) = log(a) + log(b)
    # Compute the sum of the logarithms of the values and the scale factor
    log_sum = np.sum(np.log2(values)) + (len(values) - 1) * np.log2(factor)

    return log_sum

# %% ../nbs/00_core.ipynb 68
def multiply(values, kinase, num_dict=Data.get_num_dict()):
    "Multiply values, consider the dynamics of scale factor, which is PSPA random aa number."

    # Check if any values are less than or equal to zero
    if np.any(np.array(values) == 0):
        return np.nan
    else:
        # Retrieve the divide factor from the dictionary
        divide_factor = num_dict[kinase]

        # Using the logarithmic property: log(a*b) = log(a) + log(b)
        # Compute the sum of the logarithms of the values and the divide factor
        log_sum = np.sum(np.log2(values)) + (len(values) - 1) * np.log2(divide_factor)

        return log_sum

# %% ../nbs/00_core.ipynb 71
def sumup(values, # list of values, possibilities of amino acids at certain positions
          kinase=None, 
         ):
    "Sum up the possibilities of the amino acids at each position in a phosphorylation site sequence"
    return sum(values)

# %% ../nbs/00_core.ipynb 74
def STY2sty(input_string: str):
    "Replace all 'STY' with 'sty' in a sequence"    
    return input_string.replace('S', 's').replace('T', 't').replace('Y', 'y')

# %% ../nbs/00_core.ipynb 76
def get_dict(input_string:str, # phosphorylation site sequence
            ):
    
    "Get a dictionary of input string; no need for the star in the middle; make sure it is 15 or 10 length"

    center_index = len(input_string) // 2
    center_char = input_string[center_index]

    result = []

    for i, char in enumerate(input_string):
        position = i - center_index

        if char.isalpha():
            result.append(f"{position}{char}")

    return result

# %% ../nbs/00_core.ipynb 79
def predict_kinase(input_string: str, # site sequence
                   ref: pd.DataFrame, # reference dataframe for scoring
                   func: Callable, # function to calculate score
                   to_lower: bool=False, # convert capital STY to lower case
                   to_upper: bool=False, # convert all letter to uppercase
                   verbose=True
                   ):
    "Predict kinase given a phosphorylation site sequence"
 
    input_string = check_seq(input_string)

    if to_lower:
        input_string = STY2sty(input_string)

    if to_upper:
        input_string = input_string.upper()
    
    results = []
    
    for kinase, row in ref.iterrows():
        
        # Convert the row into a dictionary, excluding NaN values, to create a PSSM dictionary for a kinase
        r_dict = row.dropna().to_dict()
        
        # Extract position+amino acid name from the input string and filter them against the name in PSSM
        pos_aa_name = get_dict(input_string)
        pos_aa_name = [key for key in pos_aa_name if key in r_dict.keys()]
    
        # Collect corresponding PSSM values for these positions and amino acids
        pos_aa_val = [r_dict[key] for key in pos_aa_name] # Further checks for NaN values
        
        # Calculate the score for this kinase using the specified function
        score = func(pos_aa_val, kinase)
        results.append(score)
    
    if verbose:
        print(f'considering string: {pos_aa_name}')

    out = pd.Series(results, index=ref.index).sort_values(ascending=False)
        
    return out.round(3)

# %% ../nbs/00_core.ipynb 82
def Params(name=None):
    params = {
        "PSPA_st": {'ref': Data.get_pspa_st_norm().astype('float32'), 'func': multiply},
        "PSPA_y": {'ref': Data.get_pspa_tyr_norm().astype('float32'), 'func': multiply},
        "PSPA": {'ref': Data.get_pspa_all_norm().astype('float32'), 'func': multiply},
        "CDDM": {'ref': Data.get_cddm().astype('float32'), 'func': sumup},
        "CDDM_upper": {'ref': Data.get_cddm_upper().astype('float32'), 'func': sumup, 'to_upper': True},
    }
    
    if name is None:
        print("Available parameter sets:")
        return list(params.keys())
    
    if name in params:
        return params[name]
    
    raise ValueError(f"Unknown parameter set: {name}. Use Params() to list available options.")

# %% ../nbs/00_core.ipynb 87
def cut_seq(input_string: str, # site sequence
            min_position: int, # minimum position relative to its center
            max_position: int, # maximum position relative to its center
            ):
    
    "Extract sequence based on a range relative to its center position"
    
    # Find the center position of the string
    center_position = len(input_string) // 2

    # Calculate the start and end indices
    start_index = max(center_position + min_position, 0)  # Ensure start_index is not negative
    end_index = min(center_position + max_position + 1, len(input_string))  # Ensure end_index does not exceed string length

    # Extract and return the substring
    return input_string[start_index:end_index]

# %% ../nbs/00_core.ipynb 90
def predict_kinase_df(df, seq_col, ref, func, to_lower=False, to_upper=False):
    
    print('input dataframe has a length', df.shape[0])
    print('Preprocessing')
    
    df = df.copy()
    
    df[seq_col] = check_seq_df(df, seq_col)
    
    if to_lower:
        df[seq_col] = df[seq_col].apply(STY2sty)
        
    if to_upper:
        df[seq_col] = df[seq_col].str.upper()
        
    # Adjust sequence lengths to match the reference matrix's expected inputs
    max_value = ref.columns.str[:-1].astype(int).max() # Get the highest position index from the reference columns
    min_value = ref.columns.str[:-1].astype(int).min() # Get the lowest position index
    df[seq_col] = df[seq_col].apply(partial(cut_seq, min_position=min_value, max_position=max_value))
    
    print('Finish preprocessing')
    
    
    # wide form to long form
    df['keys'] = df[seq_col].apply(get_dict)
    input_keys_df  = df[['keys']].explode('keys').reset_index()
    input_keys_df.columns = ['input_index', 'key']
    
    
    ref_T = ref.T
    
    input_keys_df = input_keys_df.set_index('key')
    
    
    print('Merging reference')
    merged_df = input_keys_df.merge(ref_T, left_index=True, right_index=True, how='inner')

    print('Finish merging')
    
    if func == sumup:
        grouped_df = merged_df.groupby('input_index').sum()
        out = grouped_df.reindex(df.index)
         
    elif func==multiply:
        # Get the list of kinases and num_dict
        kinases = ref_T.columns
        num_dict = Data.get_num_dict()
        
        out = {}
        for kinase in tqdm(kinases):
            divide_factor = num_dict[kinase]
            # Extract data for this kinase
            kinase_df = merged_df[['input_index', kinase]].copy()
            kinase_df = kinase_df.rename(columns={kinase: 'value'})

            # Compute log_value
            kinase_df['log_value'] = np.log2(kinase_df['value'].where(kinase_df['value'] > 0))

            # Group by 'input_index' and compute sum and count
            grouped = kinase_df.dropna().groupby('input_index')
            sum_log_values = grouped['log_value'].sum()
            len_values = grouped['log_value'].count()

            # Compute log_sum using the formula
            log_sum = sum_log_values + (len_values - 1) * np.log2(divide_factor)

            # Find all 'input_index' where 'log_value' is NaN
            nan_input_indices = kinase_df.loc[kinase_df['value']==0, 'input_index'].unique()
            # Set log_sum at those indices to NaN
            log_sum.loc[nan_input_indices] = np.nan

            # Assign the computed values to the results DataFrame
            out[kinase] = log_sum

        out = pd.DataFrame(out).reindex(df.index)
        
    else:
        grouped_df = merged_df.drop(columns=['key']).groupby('input_index').agg(func)
        out = grouped_df.reindex(df.index)
        
    # Get results as a DataFrame
    return out

# %% ../nbs/00_core.ipynb 95
def get_pct(site,ref,func,pct_ref):
    
    "Replicate the precentile results from The Kinase Library."
    
    # As here we try to replicate the results, we use site.upper(); consider removing it for future version.
    score = predict_kinase(site.upper(),ref=ref,func=func)
    
    percentiles = {}
    for kinase in score.index: 
        # Get the values from `ref` for this kinase
        ref_values = pct_ref[kinase].values
        # Calculate how many values in `ref` are less than the new score
        less = np.sum(ref_values < score[kinase])
        # Calculate how many values are equal to the new score
        equal = np.sum(ref_values == score[kinase])
        # Calculate the percentile rank
        percentile = (less + 0.5 * equal) / len(ref_values) * 100
        percentiles[kinase] = percentile
        
    pct = pd.Series(percentiles)
    final = pd.concat([score,pct],axis=1)
    final.columns=['log2(score)','percentile']
    return final

# %% ../nbs/00_core.ipynb 101
def get_pct_df(score_df, # output from predict_kinase_df 
               pct_ref, # a reference df for percentile calculation
              ):
    
    "Replicate the precentile results from The Kinase Library."

    # Create an array to hold percentile ranks
    percentiles = np.zeros(score_df.shape)
    
    # Calculate percentiles for each column in a vectorized manner
    for i, kinase in tqdm(enumerate(score_df.columns),total=len(score_df.columns)):
        ref_values = np.sort(pct_ref[kinase].values)
        
        # Use searchsorted to find indices where the scores would be inserted to maintain order
        indices = np.searchsorted(ref_values, score_df[kinase].values, side='right')
        
        # Calculate percentile ranks
        percentiles[:, i] = indices / len(ref_values) * 100

    # Convert the array to a DataFrame with appropriate indices and columns
    percentiles_df = pd.DataFrame(percentiles, index=score_df.index, columns=score_df.columns).astype(float).round(3)
    
    return percentiles_df

# %% ../nbs/00_core.ipynb 105
def phosphorylate_seq(r,site_info_col='site', sub_seq_col='substrate_sequence'):
    "Phosphorylate whole sequence based on phosphosites"
    seq = list(r[sub_seq_col])

    for pos in r[site_info_col]:
        char = pos[0] 
        position = int(pos[1:]) - 1 # substract 1 as python index starts from 0

        if 0 <= position < len(seq):
            if seq[position] == char:
                seq[position] = char.lower()  
            else:
                raise ValueError(f"Mismatch at position {position+1}: expected {char}, found {seq[position]}")
        else:
            raise IndexError(f"Position {position+1} out of range for sequence length {len(seq)}")

    return ''.join(seq)

# %% ../nbs/00_core.ipynb 110
def phosphorylate_seq_df(df,
                         id_col='substrate_uniprot', 
                         site_info_col='site',
                         sub_seq_col='substrate_sequence'
                        ):
    "Phosphorylate whole sequence based on phosphosites in a dataframe"
    df_seq = df.groupby(id_col).agg({site_info_col:lambda r: r.unique(),sub_seq_col:'first'}).reset_index()
    df_seq['substrate_phosphoseq'] = df_seq.apply(lambda r: phosphorylate_seq(r,site_info_col,sub_seq_col),axis=1)
    return df_seq

# %% ../nbs/00_core.ipynb 113
def extract_site_seq(df: pd.DataFrame, # dataframe that contains protein sequence
                     seq_col: str, # column name of protein sequence
                     site_info_col: str, # column name of site information (e.g., S10)
                     n=7, # length of surrounding sequence (default -7 to +7)
                    ):
    "Extract -n to +n site sequence from protein sequence"
    
    data = []
    for i, r in tqdm(df.iterrows(),total=len(df)):
        position = int(r[site_info_col][1:]) - 1
        start = position - n
        end = position + n +1

        # Extract the subsequence
        subseq = r[seq_col][max(0, start):min(len(r[seq_col]), end)]

        # Pad the subsequence if needed
        if start < 0:
            subseq = "_" * abs(start) + subseq
        if end > len(r[seq_col]):
            subseq = subseq + "_" * (end - len(r[seq_col]))

        data.append(subseq)
        
    return np.array(data)

# %% ../nbs/00_core.ipynb 118
def get_prob(df: pd.DataFrame, col: str, aa_order=[i for i in 'PGACSTVILMFYWHKRQNDEsty']):
    """Get the probability matrix of PSSM from phosphorylation site sequences."""
    
    site = check_seq_df(df, col)
    
    site_array = np.array(site.apply(list).tolist())
    seq_len = site_array.shape[1]
    
    position = list(range(-(seq_len // 2), (seq_len // 2)+1)) # add 1 because range do not include the final num
    
    site_df = pd.DataFrame(site_array, columns=position)
    melted = site_df.melt(var_name='Position', value_name='aa')
    
    grouped = melted.groupby(['Position', 'aa']).size().reset_index(name='Count')
    grouped = grouped[grouped.aa.isin(aa_order)].reset_index(drop=True)
    
    pivot_df = grouped.pivot(index='aa', columns='Position', values='Count').fillna(0)
    pssm_df = pivot_df / pivot_df.sum()
    
    pssm_df = pssm_df.reindex(index=aa_order, columns=position, fill_value=0)
    pssm_df = pssm_df.rename(index={'s': 'pS', 't': 'pT', 'y': 'pY'})
    
    return pssm_df

# %% ../nbs/00_core.ipynb 122
def pssm_to_seq(pssm_df, 
                thr=0.4, # threshold of probability to show in sequence
                contain_sty=True, # keep only s,t,y values (last three) in center 0 position
                ):
    "Represent PSSM in string sequence of amino acids"
    
    pssm_df = pssm_df.copy()
    if contain_sty:
        pssm_df.loc[pssm_df.index[:-3], 0] = 0  # keep only s,t,y in center 0 position

    pssm_df.index = pssm_df.index.map(lambda x: x.replace('pS', 's').replace('pT', 't').replace('pY', 'y'))

    consensus = []
    for i, col in enumerate(pssm_df.columns):
        top = pssm_df[col].nlargest(3)
        passing = [aa for aa, prob in zip(top.index, top.values) if prob > thr]

        if not passing:
            symbol = '.'
        elif len(passing) == 1:
            symbol = passing[0]
        else:
            symbol = f"[{'/'.join(passing)}]"

        if col == 0:  # center position
            if symbol.startswith('['):
                symbol = symbol[:-1] + ']*'
            else:
                symbol += '*'

        consensus.append(symbol)

    return ''.join(consensus)

# %% ../nbs/00_core.ipynb 124
def recover_pssm(flat_pssm:pd.Series,aa_order=list('PGACSTVILMFYWHKRQNDEsty')):
    "Recover 2D pssm from flat pssm Series"
    df = flat_pssm.copy().reset_index()
    df.columns=['info','value']
    df['Position']=df['info'].str.extract(r'(-?\d+)').astype(int)
    df['aa']=df['info'].str.extract(r'-?\d+\s*(.*)')
    df = df.pivot(index='aa',columns='Position',values='value').fillna(0)
    return df.reindex(index=aa_order).rename(index={'s': 'pS', 't': 'pT', 'y': 'pY'})

# %% ../nbs/00_core.ipynb 128
def process_pssm(pssm_df):
    "Keep only s,t,y values in center 0 position; normalize per position"
    pssm_df=pssm_df.copy()
    pssm_df.columns= pssm_df.columns.astype(int)
    pssm_df.loc[pssm_df.index[:-3], 0] = 0
    pssm_df = pssm_df/pssm_df.sum()
    return pssm_df

# %% ../nbs/00_core.ipynb 130
def pssm2dict(pssm_df):
    "Convert pssm dataframe to dict"
    pssm_df=pssm_df.copy()
    pssm_df = pssm_df.unstack().reset_index(name='value')
    pssm_df['position_residue']=pssm_df.iloc[:,0].astype(str)+pssm_df.iloc[:,1]
    return pssm_df.set_index('position_residue')['value'].round(5).to_dict()

# %% ../nbs/00_core.ipynb 133
def js_divergence(p1, # pssm 
                  p2, # pssm
                  mean=True):
    "p1 and p2 are two arrays (df or np) with index as aa and column as position"
    assert p1.shape==p2.shape
    mask = (p1 + p2) > 0 #skip those with double 0
    p1,p2 = p1[mask], p2[mask]
    
    m = 0.5 * (p1 + p2)
    js = 0.5 * np.sum(p1 * np.log(p1 / m + 1e-10), axis=0) + \
         0.5 * np.sum(p2 * np.log(p2 / m + 1e-10), axis=0)
    return np.mean(js) if mean else js

# %% ../nbs/00_core.ipynb 135
def js_divergence_flat(p1_flat, # pd.Series of flattened pssm
                       p2_flat, # pd.Series of flattened pssm
                       ):

    "p1 and p2 are two flattened pd.Series with index as aa and column as position"

    js = js_divergence(p1_flat,p2_flat,mean=False)
    total_position = len(p1_flat.index.str.extract(r'(-?\d+)').drop_duplicates())
    return js/total_position

# %% ../nbs/00_core.ipynb 139
def entropy(pssm_df,# a dataframe of pssm with index as aa and column as position
            return_min=False, # return min entropy as a single value or return all entropy as a series
            exclude_zero=False, # exclude the column of 0 (center position) in the entropy calculation
            contain_sty=True, # keep only s,t,y values (last three) in center 0 position
            ): 
    "Calculate entropy per position (max) of a PSSM surrounding 0"
    pssm_df = pssm_df.copy()
    pssm_df.columns= pssm_df.columns.astype(int)
    if 0 in pssm_df.columns:
        if exclude_zero:
            pssm_df = pssm_df.drop(columns=[0])
        if contain_sty:                       
            pssm_df.loc[pssm_df.index[:-3], 0] = 0
    pssm_df = pssm_df/pssm_df.sum()
    per_position = -np.sum(pssm_df * np.log2(pssm_df + 1e-9), axis=0)
    return per_position.min() if return_min else per_position

# %% ../nbs/00_core.ipynb 141
@delegates(entropy)
def entropy_flat(flat_pssm:pd.Series,**kwargs): 
    "Calculate entropy per position of a flat PSSM surrounding 0"
    pssm_df = recover_pssm(flat_pssm)
    return entropy(pssm_df,**kwargs)

# %% ../nbs/00_core.ipynb 142
def get_IC_standard(pssm_df):
    """Calculate the standard information content (bits) from frequency matrix, 
    using the same number of residues log2(len(pssm_df)) for all positions"""
    
    entropy_position=entropy(pssm_df)
    
    # information_content = max_entropy - entropy --> log2(N) - entropy
    IC_position = np.log2((len(pssm_df))) - entropy_position
    scaled_df = pssm_df.mul(IC_position)
    return scaled_df

# %% ../nbs/00_core.ipynb 143
@delegates(entropy)
def get_IC(pssm_df,**kwargs):
    """Calculate the information content (bits) from a frequency matrix,
    using log2(3) for the middle position and log2(len(pssm_df)) for others."""
    
    entropy_position = entropy(pssm_df,**kwargs)
    
    max_entropy_array = pd.Series(np.log2(len(pssm_df)), index=pssm_df.columns)
    
    max_entropy_array[0] = np.log2(3)

    # information_content = max_entropy - entropy --> log2(N) - entropy
    IC_position = max_entropy_array - entropy_position
    return IC_position

# %% ../nbs/00_core.ipynb 144
@delegates(get_IC)
def get_IC_flat(flat_pssm:pd.Series,**kwargs):
    """Calculate the information content (bits) from a flattened pssm pd.Series,
    using log2(3) for the middle position and log2(len(pssm_df)) for others."""
    
    pssm_df = recover_pssm(flat_pssm)
    return get_IC(pssm_df,**kwargs)

# %% ../nbs/00_core.ipynb 145
def get_scaled_IC(pssm_df):
    """For plotting purpose, calculate the scaled information content (bits) from a frequency matrix,
    using log2(3) for the middle position and log2(len(pssm_df)) for others."""
    
    IC_position = get_IC(pssm_df)
    
    return pssm_df.mul(IC_position, axis=1)

# %% ../nbs/00_core.ipynb 147
def get_pvalue(df,
              columns1, # list of column names for group1
              columns2, # list of column names for group2
              test_method = 'mann_whitney', # 'student_t', 'mann_whitney', 'wilcoxon'
              FC_method = 'median', # or mean
             ):

    "Performs statistical tests and calculates difference between the median or mean of two groups of columns."

    group1 = df[columns1]
    group2 = df[columns2]

    # Compute median values for each gene in both groups
    if FC_method == "median":
        m1 = group1.median(axis=1)
        m2 = group2.median(axis=1)
    elif FC_method == "mean":
        m1 = group1.mean(axis=1)
        m2 = group2.mean(axis=1)

    # As phosphoproteomics data has already been log transformed, we can directly use subtraction
    FCs = m2 - m1

    # Perform the chosen test and handle NaN p-values
    if test_method == 'student_t': # data is normally distributed, non-paired
        test_func = ttest_ind
    elif test_method == 'mann_whitney': # not normally distributed, non-paired, mann_whitney considers the rank, ignore the differences
        test_func = mannwhitneyu
    elif test_method == 'wilcoxon': # not normally distributed, paired
        test_func = wilcoxon

    t_results = []
    for idx in tqdm(df.index, desc=f"Computing {test_method} tests"):
        try:
            if test_method == 'wilcoxon': # as wilcoxon is paired, if lack a paired sample, just give nan, as default nanpolicy is propagate (gives nan if nan in input)
                stat, pvalue = test_func(group1.loc[idx], group2.loc[idx])
            else:
                stat, pvalue = test_func(group1.loc[idx], group2.loc[idx], nan_policy='omit')
        except ValueError:  # Handle cases with insufficient data
            pvalue = np.nan
        t_results.append(pvalue)

    # Exclude NaN p-values before multiple testing correction
    p_values = np.array(t_results, dtype=float)  # Ensure the correct data type
    valid_p_values = p_values[~np.isnan(p_values)]

    # Adjust for multiple testing on valid p-values only
    reject, pvals_corrected, _, _ = multipletests(valid_p_values, alpha=0.05, method='fdr_bh')

    # Create a full list of corrected p-values including NaNs
    full_pvals_corrected = np.full_like(p_values, np.nan)
    np.place(full_pvals_corrected, ~np.isnan(p_values), pvals_corrected)

    # Adjust the significance accordingly
    full_reject = np.zeros_like(p_values, dtype=bool)
    np.place(full_reject, ~np.isnan(p_values), reject)

    # Create DataFrame with results
    results = pd.DataFrame({
        'log2FC': FCs,
        'p_value': p_values,
        'p_adj': full_pvals_corrected
    })

    results['p_value'] = results['p_value'].astype(float)

    def get_signed_logP(r,p_col):
        log10 = -np.log10(r[p_col])
        return -log10 if r['log2FC']<0 else log10

    results['signed_logP'] = results.apply(partial(get_signed_logP,p_col='p_value'),axis=1)
    results['signed_logPadj'] = results.apply(partial(get_signed_logP,p_col='p_adj'),axis=1)

    return results

# %% ../nbs/00_core.ipynb 148
def get_metaP(p_values):
    
    "Use Fisher's method to calculate a combined p value given a list of p values; this function also allows negative p values (negative correlation)"

    logs = [math.log(abs(p))*-1 if p<0 else math.log(abs(p)) for p in p_values]
    chi_square_stat = -2 * sum(logs)
    degrees_of_freedom = 2 * len(p_values)
    score = chi2.sf(abs(chi_square_stat), degrees_of_freedom)*-1 if chi_square_stat<0 else chi2.sf(abs(chi_square_stat), degrees_of_freedom)

    return score

# %% ../nbs/00_core.ipynb 151
def raw2norm(df: pd.DataFrame, # single kinase's df has position as index, and single amino acid as columns
             PDHK: bool=False, # whether this kinase belongs to PDHK family 
            ):
    
    "Normalize single ST kinase data"
    columns_to_exclude = ['S', 'T', 'C', 't', 'y']
    
    if PDHK:
        columns_to_exclude.append('Y')
        divisor = 16
    else:
        divisor = 17
    
    s = df.drop(columns=columns_to_exclude).sum(1)
    df2 = df.div(s, axis=0)
    df2.C = df2.C / (df2.C.median() * divisor)
    df2['S'] = df2.drop(columns=columns_to_exclude).median(1)
    df2['T'] = df2.drop(columns=columns_to_exclude).median(1)
    df2 = round(df2, 4)
    
    return df2

# %% ../nbs/00_core.ipynb 153
def get_one_kinase(df: pd.DataFrame, #stacked dataframe (paper's raw data)
                   kinase:str, # a specific kinase
                   normalize: bool=False, # normalize according to the paper; special for PDHK1/4
                   drop_s: bool= True, # drop s as s is a duplicates of t in PSPA
                  ):
    "Obtain a specific kinase data from stacked dataframe"
    
    p = pd.DataFrame(df.loc[kinase],columns = [kinase]).reset_index().rename(columns={'index':'substrate'})
    p['position'] = p.substrate.str.extract('(-?\d+)')
    p['aa'] = p.substrate.str[-1]
    p.position = p.position.astype(int)
    pp = p.pivot(index='position', columns='aa', values=kinase)
    if drop_s:
        if 's' in pp.columns:
            pp = pp.drop(columns=['s'])

    if normalize:
        pp = raw2norm(pp, PDHK=True if kinase == 'PDHK1' or kinase == 'PDHK4' else False)
    return pp
