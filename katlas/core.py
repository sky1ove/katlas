"""Core functions in Katlas library"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_core.ipynb.

# %% auto 0
__all__ = ['Data', 'CPTAC', 'check_seq', 'check_seq_df', 'validate_site', 'validate_site_df', 'onehot_encode', 'STY2sty',
           'cut_seq', 'get_dict', 'multiply_func', 'multiply', 'sumup', 'predict_kinase', 'get_param',
           'predict_kinase_df', 'get_pct', 'get_pct_df', 'get_unique_site', 'phosphorylate_seq', 'phosphorylate_seq_df',
           'extract_site_seq', 'get_prob', 'pssm_to_seq', 'recover_pssm', 'flatten_pssm', 'get_freq', 'js_divergence',
           'js_divergence_flat', 'entropy', 'entropy_flat', 'get_IC_standard', 'get_IC', 'get_IC_flat', 'get_scaled_IC',
           'get_pvalue', 'get_metaP', 'raw2norm', 'get_one_kinase']

# %% ../nbs/00_core.ipynb 3
import math, pandas as pd, numpy as np
from tqdm import tqdm
from scipy.stats import chi2
from typing import Callable
from functools import partial
from scipy.stats import ttest_ind, mannwhitneyu, wilcoxon,chi2
from statsmodels.stats.multitest import multipletests
from functools import lru_cache
from sklearn.cluster import KMeans
from sklearn.preprocessing import OneHotEncoder
from fastcore.meta import delegates

# %% ../nbs/00_core.ipynb 6
class Data:
    """A class for fetching various datasets."""
    
    @staticmethod
    @lru_cache(maxsize=None)
    def fetch_data(url: str) -> pd.DataFrame:
        """
        Fetch data from the given URL and return a DataFrame.
        Renames 'Unnamed: 0' column to 'kinase' if present.
        """
        df = pd.read_parquet(url)
        if "Unnamed: 0" in df.columns:
            df = df.rename(columns={"Unnamed: 0": "kinase"})
        return df

    @staticmethod
    @lru_cache(maxsize=None)
    def fetch_csv(url: str) -> pd.DataFrame:
        """
        Fetch data from the given URL and return a DataFrame.
        Renames 'Unnamed: 0' column to 'kinase' if present.
        """
        df = pd.read_csv(url)
        if "Unnamed: 0" in df.columns:
            df = df.rename(columns={"Unnamed: 0": "kinase"})
        return df
    

    @staticmethod
    def _convert_numeric_columns(df: pd.DataFrame) -> pd.DataFrame:
        """
        Convert column names that are numeric strings into integers,
        but only if they are still strings.
        """
        df.columns = [int(col) if isinstance(col, str) and col.lstrip('-').isdigit() else col for col in df.columns]
        return df

    #--------------------------- Kinase and PSPA ---------------------------
    KINASE_INFO_URL = "https://github.com/sky1ove/katlas/raw/main/dataset/kinase_info.csv"
    PSPA_TYR_NORM_URL = "https://github.com/sky1ove/katlas/raw/main/dataset/PSPA/pspa_tyr_norm.parquet"
    PSPA_ST_NORM_URL = "https://github.com/sky1ove/katlas/raw/main/dataset/PSPA/pspa_st_norm.parquet"
    PSPA_ALL_NORM_URL = "https://github.com/sky1ove/katlas/raw/main/dataset/PSPA/pspa_all_norm.parquet"
    PSPA_ST_PCT_URL = "https://github.com/sky1ove/katlas/raw/main/dataset/PSPA/pspa_pct_st.parquet"
    PSPA_TYR_PCT_URL = "https://github.com/sky1ove/katlas/raw/main/dataset/PSPA/pspa_pct_tyr.parquet"
    PSPA_NUM_RANDOM_URL = "https://github.com/sky1ove/katlas/raw/main/dataset/PSPA/pspa_divide_num.csv"

    @staticmethod
    def get_kinase_info() -> pd.DataFrame:
        """Return kinase information."""
        return Data.fetch_csv(Data.KINASE_INFO_URL)

    @staticmethod
    def get_pspa_tyr_norm() -> pd.DataFrame:
        """Return PSPA tyrosine kinase normalized data."""
        return Data.fetch_data(Data.PSPA_TYR_NORM_URL)

    @staticmethod
    def get_pspa_st_norm() -> pd.DataFrame:
        """Return PSPA serine/threonine kinase normalized data."""
        return Data.fetch_data(Data.PSPA_ST_NORM_URL)

    @staticmethod
    def get_pspa_all_norm() -> pd.DataFrame:
        """Return PSPA combined normalized data for serine/threonine and tyrosine kinases."""
        return Data.fetch_data(Data.PSPA_ALL_NORM_URL)

    @staticmethod
    def get_pspa_st_pct() -> pd.DataFrame:
        """Return PSPA scoring for serine/threonine kinases."""
        return Data.fetch_data(Data.PSPA_ST_PCT_URL)

    @staticmethod
    def get_pspa_tyr_pct() -> pd.DataFrame:
        """Return PSPA scoring for tyrosine kinases."""
        return Data.fetch_data(Data.PSPA_TYR_PCT_URL)

    @staticmethod
    def get_num_dict() -> dict:
        """Return a dictionary mapping kinase to number of random amino acids."""
        num = pd.read_csv(Data.PSPA_NUM_RANDOM_URL)
        return num.set_index("kinase")["num_random_aa"].to_dict()

    #--------------------------- CDDM ---------------------------
    KS_DATASET_URL = "https://github.com/sky1ove/katlas/raw/main/dataset/CDDM/ks_datasets_20250407.parquet"
    CDDM_URL = "https://github.com/sky1ove/katlas/raw/main/dataset/CDDM/ks_main.parquet"
    CDDM_UPPER_URL = "https://github.com/sky1ove/katlas/raw/main/dataset/CDDM/ks_main_upper.parquet"
    CDDM_OTHERS_URL = "https://github.com/sky1ove/katlas/raw/main/dataset/CDDM/ks_others.parquet"
    CDDM_OTHERS_INFO_URL = "https://github.com/sky1ove/katlas/raw/main/dataset/CDDM/ks_others_info.parquet"

    @staticmethod
    def get_ks_dataset() -> pd.DataFrame:
        """Return kinase substrate dataset with numeric columns converted."""
        df = Data.fetch_data(Data.KS_DATASET_URL)
        return Data._convert_numeric_columns(df)

    @staticmethod
    def get_cddm() -> pd.DataFrame:
        """Return the primary CDDM dataset."""
        return Data.fetch_data(Data.CDDM_URL)

    @staticmethod
    def get_cddm_upper() -> pd.DataFrame:
        """Return the upper CDDM dataset."""
        return Data.fetch_data(Data.CDDM_UPPER_URL)

    @staticmethod
    def get_cddm_others() -> pd.DataFrame:
        """Return CDDM data for other kinases with mutations."""
        return Data.fetch_data(Data.CDDM_OTHERS_URL)

    @staticmethod
    def get_cddm_others_info() -> pd.DataFrame:
        """Return additional information for CDDM 'others' dataset."""
        return Data.fetch_data(Data.CDDM_OTHERS_INFO_URL)

    # Combined PSPA and CDDM
    COMBINE_URL = "https://github.com/sky1ove/katlas/raw/main/dataset/combine_main.parquet"

    @staticmethod
    def get_combine() -> pd.DataFrame:
        """Return the combined PSPA and CDDM dataset."""
        return Data.fetch_data(Data.COMBINE_URL)

    #--------------------------- Amino Acid ---------------------------
    AA_INFO_URL = "https://github.com/sky1ove/katlas/raw/main/dataset/amino_acids/aa_info.parquet"
    AA_RDKIT_URL = "https://github.com/sky1ove/katlas/raw/main/dataset/amino_acids/aa_rdkit.parquet"
    AA_MORGAN_URL = "https://github.com/sky1ove/katlas/raw/main/dataset/amino_acids/aa_morgan.parquet"

    @staticmethod
    def get_aa_info() -> pd.DataFrame:
        """Return amino acid information."""
        return Data.fetch_data(Data.AA_INFO_URL)

    @staticmethod
    def get_aa_rdkit() -> pd.DataFrame:
        """Return RDKit representations of amino acids."""
        return Data.fetch_data(Data.AA_RDKIT_URL)

    @staticmethod
    def get_aa_morgan() -> pd.DataFrame:
        """Return Morgan fingerprint representations of amino acids."""
        return Data.fetch_data(Data.AA_MORGAN_URL)

    #--------------------------- Phosphoproteomics ---------------------------
    CPTAC_KB_URL = "https://github.com/sky1ove/katlas/raw/main/dataset/phosphosites/linkedOmicsKB_ref_pan.parquet"
    CPTAC_UNIQUE_URL = "https://github.com/sky1ove/katlas/raw/main/dataset/phosphosites/cptac_unique_site.parquet"
    CPTAC_URL = "https://github.com/sky1ove/katlas/raw/main/dataset/phosphosites/linkedOmics_ref_pan.parquet"
    PSP_HUMAN_URL = "https://github.com/sky1ove/katlas/raw/main/dataset/phosphosites/psp_human.parquet"
    OCHOA_URL = "https://github.com/sky1ove/katlas/raw/main/dataset/phosphosites/ochoa_site.parquet"
    COMBINE_PSP_OCHOA_URL = "https://github.com/sky1ove/katlas/raw/main/dataset/phosphosites/combine_site_psp_ochoa.parquet"
    P_COMBINE_PSP_OCHOA_URL = "https://github.com/sky1ove/katlas/raw/main/dataset/phosphosites/phosphorylated_combine_site.parquet"
    P_COMBINE_PSP_OCHOA20_URL = "https://github.com/sky1ove/katlas/raw/main/dataset/phosphosites/phosphorylated_combine_site20.parquet"
    
    @staticmethod
    def get_cptac_ensembl_site() -> pd.DataFrame:
        """Return CPTAC dataset with unique EnsemblProteinID+site."""
        return Data.fetch_data(Data.CPTAC_KB_URL)

    @staticmethod
    def get_cptac_unique_site() -> pd.DataFrame:
        """Return CPTAC dataset with unique site sequences."""
        return Data.fetch_data(Data.CPTAC_UNIQUE_URL)

    @staticmethod
    def get_cptac_gene_site() -> pd.DataFrame:
        """Return CPTAC dataset with unique Gene+site."""
        return Data.fetch_data(Data.CPTAC_URL)

    @staticmethod
    def get_psp_human_site() -> pd.DataFrame:
        """Return PhosphoSitePlus human dataset (Gene+site)."""
        return Data.fetch_data(Data.PSP_HUMAN_URL)

    @staticmethod
    def get_ochoa_site() -> pd.DataFrame:
        """Return dataset from Ochoa et al."""
        return Data.fetch_data(Data.OCHOA_URL)

    @staticmethod
    def get_combine_site_psp_ochoa() -> pd.DataFrame:
        """
        Return the combined dataset from Ochoa and PhosphoSitePlus,
        converting numeric column names where applicable.
        """
        df = Data.fetch_data(Data.COMBINE_PSP_OCHOA_URL)
        return Data._convert_numeric_columns(df)

    @staticmethod
    def get_combine_site_phosphorylated() -> pd.DataFrame:
        """
        Return the combined phosphorylated dataset from Ochoa and PhosphoSitePlus,
        with numeric column names converted.
        """
        df = Data.fetch_data(Data.P_COMBINE_PSP_OCHOA_URL)
        return Data._convert_numeric_columns(df)


    @staticmethod
    def get_human_site() -> pd.DataFrame:
        """
        Return the combined phosphorylated dataset from Ochoa and PhosphoSitePlus,
        with numeric column names converted.
        """
        df = Data.fetch_data(Data.P_COMBINE_PSP_OCHOA20_URL)
        return Data._convert_numeric_columns(df)




# %% ../nbs/00_core.ipynb 10
class CPTAC:
    
    "A class for fetching CPTAC phosphoproteomics data."
    @staticmethod
    def _fetch_data(cancer: str, # cancer type CPTAC
                    is_Tumor: bool=True, # tumor tissue or normal
                    is_KB: bool=False, # whether it is for LinkedOmicsKB or LinkedOmics
                   ):
        "Fetches the data from the given URL and returns a DataFrame"
        
        # URL of ID and data
        sample_type = "Tumor" if is_Tumor else "Normal"
        ID_URL = f"https://zenodo.org/records/8196130/files/bcm-{cancer.lower()}-mapping-gencode.v34.basic.annotation-mapping.txt.gz"
        DATA_URL = f"https://cptac-pancancer-data.s3.us-west-2.amazonaws.com/data_freeze_v1.2_reorganized/{cancer.upper()}/{cancer.upper()}_phospho_site_abundance_log2_reference_intensity_normalized_{sample_type}.txt"

        # Load ID data
        ref = pd.read_csv(ID_URL, compression='gzip', sep='\t')[['protein','gene','gene_name']].drop_duplicates().reset_index(drop=True)
        
        # Load CPTAC phosphoproteomics data
        try:
            raw = pd.read_csv(DATA_URL, sep='\t')
        except Exception as e:
            print(f'{cancer} has {e}')
        else:
            info = pd.DataFrame({'gene':raw.idx.str.split('|').str[0],
                                 'site':raw.idx.str.split('|').str[2],
                                 'site_seq':raw.idx.str.split('|').str[3]})

            print(f'the {cancer} dataset length is: {info.shape[0]}')

            # Merge ensembl ID with gene name
            info = info.merge(ref,'left')
            print(f'after id mapping, the length is {info.shape[0]}')

            print(f'{info.gene_name.isna().sum()} sites does not have a mapped gene name')

            info['gene_site'] = info['gene_name'] + '_' + info['site']
            info['protein_site'] = info['protein'].str.split('.').str[0] + '_' + info['site']
            
            info = info.drop_duplicates(subset="protein_site" if is_KB else "gene_site").reset_index(drop=True)
            print(f'after removing duplicates of protein_site, the length is {info.shape[0]}')

            return info
    
    
    @staticmethod
    def list_cancer():
        "Get available CPTAC cancer type"
        return ['HNSCC','GBM','COAD','CCRCC','LSCC','BRCA','UCEC','LUAD','PDAC','OV']

    @staticmethod
    def get_id(cancer_type: str,
               is_Tumor: bool=True, # tumor tissue or normal
               is_KB: bool=False, # whether it is for LinkedOmicsKB or LinkedOmics
              ):
        "Get CPTAC phosphorylation sites information given a cancer type"
        assert cancer_type in CPTAC.list_cancer(), "cancer type is not included, check available cancer types from CPTAC.list_cancer()"
        return CPTAC._fetch_data(cancer_type,is_Tumor, is_KB)

# %% ../nbs/00_core.ipynb 17
def check_seq(seq):
    """Convert non-s/t/y characters to uppercase and replace disallowed characters with underscores."""
    acceptor = seq[len(seq) // 2]
    assert acceptor.lower() in {'s', 't', 'y'}, f"{seq} has {acceptor} at position {len(seq) // 2}; need to have one of 's', 't', or 'y' in the center"

    allowed_chars = set("PGACSTVILMFYWHKRQNDEsty")
    return "".join(char if char in {'s', 't', 'y'} else (char.upper() if char.upper() in allowed_chars else '_') for char in seq)

# %% ../nbs/00_core.ipynb 20
def check_seq_df(df,col):
    "Convert non-s/t/y to upper case & replace with underscore if the character is not in the allowed set"
    assert len(df[col].str.len().value_counts())==1, 'inconsistent sequence length detected'
    return df[col].apply(check_seq)

# %% ../nbs/00_core.ipynb 22
def validate_site(site,seq):
    "Validate site position residue match with site residue."
    pos=int(site[1:])-1 # python index starts from zero
    if pos >= len(seq) or pos < 0: 
        return int(False)
    return int(seq[pos]==site[0])

# %% ../nbs/00_core.ipynb 25
def validate_site_df(df,site_col,seq_col): 
    "Validate site position residue match with site residue in a dataframe."
    return df.apply(lambda r: validate_site(r[site_col],r[seq_col]) , axis=1)

# %% ../nbs/00_core.ipynb 27
def onehot_encode(sequences):
    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
    encoded = encoder.fit_transform([list(seq) for seq in sequences])
    return encoded

# %% ../nbs/00_core.ipynb 30
def STY2sty(input_string: str):
    "Replace 'STY' with 'sty'"    
    return input_string.replace('S', 's').replace('T', 't').replace('Y', 'y')

# %% ../nbs/00_core.ipynb 32
def cut_seq(input_string: str, # site sequence
            min_position: int, # minimum position relative to its center
            max_position: int, # maximum position relative to its center
            ):
    
    "Extract sequence based on a range relative to its center position"
    
    # Find the center position of the string
    center_position = len(input_string) // 2

    # Calculate the start and end indices
    start_index = max(center_position + min_position, 0)  # Ensure start_index is not negative
    end_index = min(center_position + max_position + 1, len(input_string))  # Ensure end_index does not exceed string length

    # Extract and return the substring
    return input_string[start_index:end_index]

# %% ../nbs/00_core.ipynb 34
def get_dict(input_string:str, # phosphorylation site sequence
            ):
    
    "Get a dictionary of input string; no need for the star in the middle; make sure it is 15 or 10 length"

    center_index = len(input_string) // 2
    center_char = input_string[center_index]

    result = []

    for i, char in enumerate(input_string):
        position = i - center_index

        if char.isalpha():
            result.append(f"{position}{char}")

    return result

# %% ../nbs/00_core.ipynb 37
def multiply_func(values, # list of values, possibilities of amino acids at certain positions
             factor=17, # scale factor
            ):
    
    "Multiply the possibilities of the amino acids at each position in a phosphorylation site"
    

    # Using the logarithmic property: log(a*b) = log(a) + log(b)
    # Compute the sum of the logarithms of the values and the scale factor
    log_sum = np.sum(np.log2(values)) + (len(values) - 1) * np.log2(factor)

    return log_sum

# %% ../nbs/00_core.ipynb 41
def multiply(values, kinase, num_dict=Data.get_num_dict()):
    "Multiply values, consider the dynamics of scale factor, which is PSPA random aa number."

    # Check if any values are less than or equal to zero
    if np.any(np.array(values) == 0):
        return np.nan
    else:
        # Retrieve the divide factor from the dictionary
        divide_factor = num_dict[kinase]

        # Using the logarithmic property: log(a*b) = log(a) + log(b)
        # Compute the sum of the logarithms of the values and the divide factor
        log_sum = np.sum(np.log2(values)) + (len(values) - 1) * np.log2(divide_factor)

        return log_sum

# %% ../nbs/00_core.ipynb 44
def sumup(values, # list of values, possibilities of amino acids at certain positions
          kinase=None, 
         ):
    "Sum up the possibilities of the amino acids at each position in a phosphorylation site sequence"
    return sum(values)

# %% ../nbs/00_core.ipynb 47
def predict_kinase(input_string: str, # site sequence
                   ref: pd.DataFrame, # reference dataframe for scoring
                   func: Callable, # function to calculate score
                   to_lower: bool=False, # convert capital STY to lower case
                   to_upper: bool=False, # convert all letter to uppercase
                   verbose=True
                   ):
    "Predict kinase given a phosphorylation site sequence"
 
    input_string = check_seq(input_string)

    if to_lower:
        input_string = STY2sty(input_string)

    if to_upper:
        input_string = input_string.upper()
    
    results = []
    
    for kinase, row in ref.iterrows():
        
        # Convert the row into a dictionary, excluding NaN values, to create a PSSM dictionary for a kinase
        r_dict = row.dropna().to_dict()
        
        # Extract position+amino acid name from the input string and filter them against the name in PSSM
        pos_aa_name = get_dict(input_string)
        pos_aa_name = [key for key in pos_aa_name if key in r_dict.keys()]
    
        # Collect corresponding PSSM values for these positions and amino acids
        pos_aa_val = [r_dict[key] for key in pos_aa_name] # Further checks for NaN values
        
        # Calculate the score for this kinase using the specified function
        score = func(pos_aa_val, kinase)
        results.append(score)
    
    if verbose:
        print(f'considering string: {pos_aa_name}')

    out = pd.Series(results, index=ref.index).sort_values(ascending=False)
        
    return out.round(3)

# %% ../nbs/00_core.ipynb 49
def get_param(name=None):
    params = {
        "PSPA_st": {'ref': Data.get_pspa_st_norm().astype('float32'), 'func': multiply},
        "PSPA_y": {'ref': Data.get_pspa_tyr_norm().astype('float32'), 'func': multiply},
        "PSPA": {'ref': Data.get_pspa_all_norm().astype('float32'), 'func': multiply},
        "CDDM": {'ref': Data.get_cddm().astype('float32'), 'func': sumup},
        "CDDM_upper": {'ref': Data.get_cddm_upper().astype('float32'), 'func': sumup, 'to_upper': True},
    }
    
    if name is None:
        print("Available parameter sets:")
        return list(params.keys())
    
    if name in params:
        return params[name]
    
    raise ValueError(f"Unknown parameter set: {name}. Use get_param() to list available options.")

# %% ../nbs/00_core.ipynb 53
def predict_kinase_df(df, seq_col, ref, func, to_lower=False, to_upper=False):
    
    print('input dataframe has a length', df.shape[0])
    print('Preprocessing')
    
    df = df.copy()
    
    df[seq_col] = check_seq_df(df, seq_col)
    
    if to_lower:
        df[seq_col] = df[seq_col].apply(STY2sty)
        
    if to_upper:
        df[seq_col] = df[seq_col].str.upper()
        
    # Adjust sequence lengths to match the reference matrix's expected inputs
    max_value = ref.columns.str[:-1].astype(int).max() # Get the highest position index from the reference columns
    min_value = ref.columns.str[:-1].astype(int).min() # Get the lowest position index
    df[seq_col] = df[seq_col].apply(partial(cut_seq, min_position=min_value, max_position=max_value))
    
    print('Finish preprocessing')
    
    
    # wide form to long form
    df['keys'] = df[seq_col].apply(get_dict)
    input_keys_df  = df[['keys']].explode('keys').reset_index()
    input_keys_df.columns = ['input_index', 'key']
    
    
    ref_T = ref.T
    
    input_keys_df = input_keys_df.set_index('key')
    
    
    print('Merging reference')
    merged_df = input_keys_df.merge(ref_T, left_index=True, right_index=True, how='inner')

    print('Finish merging')
    
    if func == sumup:
        grouped_df = merged_df.groupby('input_index').sum()
        out = grouped_df.reindex(df.index)
         
    elif func==multiply:
        # Get the list of kinases and num_dict
        kinases = ref_T.columns
        num_dict = Data.get_num_dict()
        
        out = {}
        for kinase in tqdm(kinases):
            divide_factor = num_dict[kinase]
            # Extract data for this kinase
            kinase_df = merged_df[['input_index', kinase]].copy()
            kinase_df = kinase_df.rename(columns={kinase: 'value'})

            # Compute log_value
            kinase_df['log_value'] = np.log2(kinase_df['value'].where(kinase_df['value'] > 0))

            # Group by 'input_index' and compute sum and count
            grouped = kinase_df.dropna().groupby('input_index')
            sum_log_values = grouped['log_value'].sum()
            len_values = grouped['log_value'].count()

            # Compute log_sum using the formula
            log_sum = sum_log_values + (len_values - 1) * np.log2(divide_factor)

            # Find all 'input_index' where 'log_value' is NaN
            nan_input_indices = kinase_df.loc[kinase_df['value']==0, 'input_index'].unique()
            # Set log_sum at those indices to NaN
            log_sum.loc[nan_input_indices] = np.nan

            # Assign the computed values to the results DataFrame
            out[kinase] = log_sum

        out = pd.DataFrame(out).reindex(df.index)
        
    else:
        grouped_df = merged_df.drop(columns=['key']).groupby('input_index').agg(func)
        out = grouped_df.reindex(df.index)
        
    # Return results as a DataFrame
    return out

# %% ../nbs/00_core.ipynb 57
def get_pct(site,ref,func,pct_ref):
    
    "Replicate the precentile results from The Kinase Library."
    
    # As here we try to replicate the results, we use site.upper(); consider removing it for future version.
    score = predict_kinase(site.upper(),ref=ref,func=func)
    
    percentiles = {}
    for kinase in score.index: 
        # Get the values from `ref` for this kinase
        ref_values = pct_ref[kinase].values
        # Calculate how many values in `ref` are less than the new score
        less = np.sum(ref_values < score[kinase])
        # Calculate how many values are equal to the new score
        equal = np.sum(ref_values == score[kinase])
        # Calculate the percentile rank
        percentile = (less + 0.5 * equal) / len(ref_values) * 100
        percentiles[kinase] = percentile
        
    pct = pd.Series(percentiles)
    final = pd.concat([score,pct],axis=1)
    final.columns=['log2(score)','percentile']
    return final

# %% ../nbs/00_core.ipynb 62
def get_pct_df(score_df, # output from predict_kinase_df 
               pct_ref, # a reference df for percentile calculation
              ):
    
    "Replicate the precentile results from The Kinase Library."

    # Create an array to hold percentile ranks
    percentiles = np.zeros(score_df.shape)
    
    # Calculate percentiles for each column in a vectorized manner
    for i, kinase in tqdm(enumerate(score_df.columns),total=len(score_df.columns)):
        ref_values = np.sort(pct_ref[kinase].values)
        
        # Use searchsorted to find indices where the scores would be inserted to maintain order
        indices = np.searchsorted(ref_values, score_df[kinase].values, side='right')
        
        # Calculate percentile ranks
        percentiles[:, i] = indices / len(ref_values) * 100

    # Convert the array to a DataFrame with appropriate indices and columns
    percentiles_df = pd.DataFrame(percentiles, index=score_df.index, columns=score_df.columns).astype(float).round(3)
    
    return percentiles_df

# %% ../nbs/00_core.ipynb 67
def get_unique_site(df:pd.DataFrame = None,# dataframe that contains phosphorylation sites
                    seq_col: str='site_seq', # column name of site sequence
                    id_col: str='gene_site' # column name of site id
                   ):
    "Remove duplicates among phosphorylation sites; return df with new columns of acceptor and number of duplicates"
    
    unique = df.groupby(seq_col).agg(
        {id_col: lambda r: '|'.join(r.unique())} )
    unique['num_site'] = unique[id_col].str.split('|').apply(len) 
    unique = unique.reset_index()
    position = len(unique[seq_col][0])//2
    unique['acceptor'] = unique[seq_col].str[position]
    
    return unique

# %% ../nbs/00_core.ipynb 71
def phosphorylate_seq(r,site_col='site', seq_col='substrate_sequence'):
    "Phosphorylate whole sequence based on phosphosites"
    seq = list(r[seq_col])

    for pos in r[site_col]:
        char = pos[0] 
        position = int(pos[1:]) - 1 # substract 1 as python index starts from 0

        if 0 <= position < len(seq):
            if seq[position] == char:
                seq[position] = char.lower()  
            else:
                raise ValueError(f"Mismatch at position {position+1}: expected {char}, found {seq[position]}")
        else:
            raise IndexError(f"Position {position+1} out of range for sequence length {len(seq)}")

    return ''.join(seq)

# %% ../nbs/00_core.ipynb 72
def phosphorylate_seq_df(df,
                         id_col='substrate_uniprot', 
                         site_col='site',
                         seq_col='substrate_sequence'
                        ):
    "Phosphorylate whole sequence based on phosphosites in a dataframe"
    df_seq = df.groupby(id_col).agg({site_col:lambda r: r.unique(),seq_col:'first'}).reset_index()
    df_seq['substrate_phosphoseq'] = df_seq.apply(lambda r: phosphorylate_seq(r,site_col,seq_col),axis=1)
    return df_seq

# %% ../nbs/00_core.ipynb 74
def extract_site_seq(df: pd.DataFrame, # dataframe that contains protein sequence
                     seq_col: str, # column name of protein sequence
                     position_col: str, # column name of position 0
                     length=7, # length of surrounding sequence (default -7 to +7)
                    ):
    "Extract -7 to +7 site sequence from protein sequence"
    
    data = []
    for i, r in tqdm(df.iterrows(),total=len(df)):
        position = r[position_col] - 1
        start = position - length
        end = position + length +1

        # Extract the subsequence
        subseq = r[seq_col][max(0, start):min(len(r[seq_col]), end)]

        # Pad the subsequence if needed
        if start < 0:
            subseq = "_" * abs(start) + subseq
        if end > len(r[seq_col]):
            subseq = subseq + "_" * (end - len(r[seq_col]))

        data.append(subseq)
        
    return np.array(data)

# %% ../nbs/00_core.ipynb 79
def get_prob(df: pd.DataFrame, col: str, aa_order=[i for i in 'PGACSTVILMFYWHKRQNDEsty']):
    """Get the probability matrix of PSSM from phosphorylation site sequences."""
    
    site = check_seq_df(df, col)
    
    site_array = np.array(site.apply(list).tolist())
    seq_len = site_array.shape[1]
    
    position = list(range(-(seq_len // 2), (seq_len // 2)+1)) # add 1 because range do not include the final num
    
    site_df = pd.DataFrame(site_array, columns=position)
    melted = site_df.melt(var_name='Position', value_name='aa')
    
    grouped = melted.groupby(['Position', 'aa']).size().reset_index(name='Count')
    grouped = grouped[grouped.aa.isin(aa_order)].reset_index(drop=True)
    
    pivot_df = grouped.pivot(index='aa', columns='Position', values='Count').fillna(0)
    pssm_df = pivot_df / pivot_df.sum()
    
    pssm_df = pssm_df.reindex(index=aa_order, columns=position, fill_value=0)
    pssm_df = pssm_df.rename(index={'s': 'pS', 't': 'pT', 'y': 'pY'})
    
    return pssm_df

# %% ../nbs/00_core.ipynb 81
def pssm_to_seq(pssm_df, threshold=0.4,contain_sty=True):
    pssm_df = pssm_df.copy()
    if contain_sty:
        pssm_df.loc[pssm_df.index[:-3], 0] = 0  # keep only s,t,y in center 0 position

    pssm_df.index = pssm_df.index.map(lambda x: x.replace('pS', 's').replace('pT', 't').replace('pY', 'y'))

    consensus = []
    for i, col in enumerate(pssm_df.columns):
        top = pssm_df[col].nlargest(3)
        passing = [aa for aa, prob in zip(top.index, top.values) if prob > threshold]

        if not passing:
            symbol = '.'
        elif len(passing) == 1:
            symbol = passing[0]
        else:
            symbol = f"[{'/'.join(passing)}]"

        if col == 0:  # center position
            if symbol.startswith('['):
                symbol = symbol[:-1] + ']*'
            else:
                symbol += '*'

        consensus.append(symbol)

    return ''.join(consensus)

# %% ../nbs/00_core.ipynb 82
def recover_pssm(flat_pssm:pd.Series):
    "Recover 2D pssm from flat pssm Series"
    df = flat_pssm.copy().reset_index()
    df.columns=['info','value']
    df['Position']=df['info'].str.extract(r'(?P<pos>-?\d+)').astype(int)
    df['aa']=df['info'].str.extract(r'-?\d+\s*(.*)').replace({'s':'pS','t':'pT','y':'pY'})
    return df.pivot(index='aa',columns='Position',values='value').fillna(0)

# %% ../nbs/00_core.ipynb 83
def flatten_pssm(pssm):
    "Flatten pssm dataframe to dict"
    pssm=pssm.copy()
    pssm = pssm.unstack().reset_index(name='value')
    pssm['position_residue']=pssm.iloc[:,0].astype(str)+pssm.iloc[:,1]
    return pssm.set_index('position_residue')['value'].round(5).to_dict()

# %% ../nbs/00_core.ipynb 84
def get_freq(df_k: pd.DataFrame, # a dataframe for a single kinase that contains phosphorylation sequence splitted by their position
             aa_order = [i for i in 'PGACSTVILMFYWHKRQNDEsty'], # amino acid to include in the full matrix 
             aa_order_paper = [i for i in 'PGACSTVILMFYWHKRQNDEsty'], # amino acid to include in the partial matrix
             position = [i for i in range(-7,8)], # position to include in the full matrix
             position_paper = [-5,-4,-3,-2,-1,1,2,3,4] # position to include in the partial matrix
             ):
    
    "Get frequency matrix given a dataframe of phosphorylation sites for a single kinase"
    

    #Count frequency for each amino acid at each position
    melted_k = df_k.melt(
                    value_vars=[i for i in range(-7, 8)],
                    var_name='Position', 
                    value_name='aa')
    
    # Group by Position and Amino Acid and count occurrences
    grouped = melted_k.groupby(['Position', 'aa']).size().reset_index(name='Count')
    

    # Remove wired amino acid
    aa_include = [i for i in 'PGACSTVILMFYWHKRQNDEsty']
    grouped = grouped[grouped.aa.isin(aa_include)].reset_index(drop=True)
    
    # get pivot table
    pivot_k = grouped.pivot(index='aa', columns='Position', values='Count').fillna(0)
    
    # Get frequency by dividing the sum of each column
    freq_k = pivot_k/pivot_k.sum()

    
    # data from the kinase-substrate dataset, and format is Lew's paper's format
    paper = freq_k.reindex(index=aa_order_paper,columns=position_paper,fill_value=0)

    # full pivot data from kinase-substrate dataset
    full = freq_k.reindex(index=aa_order,columns=position, fill_value=0)

    
    return paper,full

# %% ../nbs/00_core.ipynb 88
def js_divergence(p1, p2,mean=True):
    "p1 and p2 are two arrays (df or np) with index as aa and column as position"
    mask = (p1 + p2) > 0 #skip those with double 0
    p1,p2 = p1[mask], p2[mask]
    
    m = 0.5 * (p1 + p2)
    js = 0.5 * np.sum(p1 * np.log(p1 / m + 1e-10), axis=0) + \
         0.5 * np.sum(p2 * np.log(p2 / m + 1e-10), axis=0)
    return np.mean(js) if mean else js

# %% ../nbs/00_core.ipynb 89
def js_divergence_flat(p1_flat, p2_flat,total_aa=23):

    "p1 and p2 are two flattened arrays (df or np) with index as aa and column as position"

    js = js_divergence(p1_flat,p2_flat,mean=False)
    total_position = len(p1_flat)//total_aa
    return js/total_position

# %% ../nbs/00_core.ipynb 91
def entropy(pssm_df,# a dataframe of pssm with index as aa and column as position
            return_min=False, # return min entropy as a single value or return all entropy as a series
            exclude_zero=False, # exclude the column of 0 (center position) in the entropy calculation
            contain_sty=True, # keep only s,t,y values (last three) in center 0 position
            ): 
    "Calculate entropy per position (max) of a PSSM surrounding 0"
    pssm_df = pssm_df.copy()
    pssm_df.columns= pssm_df.columns.astype(int)
    if 0 in pssm_df.columns:
        if exclude_zero:
            pssm_df = pssm_df.drop(columns=[0])
        if contain_sty:                       
            pssm_df.loc[pssm_df.index[:-3], 0] = 0
    pssm_df = pssm_df/pssm_df.sum()
    per_position = -np.sum(pssm_df * np.log2(pssm_df + 1e-9), axis=0)
    return per_position.min() if return_min else per_position

# %% ../nbs/00_core.ipynb 92
@delegates(entropy)
def entropy_flat(flat_pssm:pd.Series,**kwargs): 
    "Calculate entropy per position of a flat PSSM surrounding 0"
    pssm_df = recover_pssm(flat_pssm)
    return entropy(pssm_df,**kwargs)

# %% ../nbs/00_core.ipynb 93
def get_IC_standard(pssm_df):
    """Calculate the standard information content (bits) from frequency matrix, 
    using the same number of residues log2(len(pssm_df)) for all positions"""
    
    entropy_position=entropy(pssm_df)
    
    # information_content = max_entropy - entropy --> log2(N) - entropy
    IC_position = np.log2((len(pssm_df))) - entropy_position
    scaled_df = pssm_df.mul(IC_position)
    return scaled_df

# %% ../nbs/00_core.ipynb 94
@delegates(entropy)
def get_IC(pssm_df,**kwargs):
    """Calculate the information content (bits) from a frequency matrix,
    using log2(3) for the middle position and log2(len(pssm_df)) for others."""
    
    entropy_position = entropy(pssm_df,**kwargs)
    
    max_entropy_array = pd.Series(np.log2(len(pssm_df)), index=pssm_df.columns)
    
    max_entropy_array[0] = np.log2(3)

    # information_content = max_entropy - entropy --> log2(N) - entropy
    IC_position = max_entropy_array - entropy_position
    return IC_position

# %% ../nbs/00_core.ipynb 95
@delegates(get_IC)
def get_IC_flat(flat_pssm:pd.Series,**kwargs):
    """Calculate the information content (bits) from a flattened pssm pd.Series,
    using log2(3) for the middle position and log2(len(pssm_df)) for others."""
    
    pssm_df = recover_pssm(flat_pssm)
    return get_IC(pssm_df,**kwargs)

# %% ../nbs/00_core.ipynb 96
def get_scaled_IC(pssm_df):
    """For plotting purpose, calculate the scaled information content (bits) from a frequency matrix,
    using log2(3) for the middle position and log2(len(pssm_df)) for others."""
    
    IC_position = get_IC(pssm_df)
    
    return pssm_df.mul(IC_position, axis=1)

# %% ../nbs/00_core.ipynb 98
def get_pvalue(df,
              columns1, # list of column names for group1
              columns2, # list of column names for group2
              test_method = 'mann_whitney', # 'student_t', 'mann_whitney', 'wilcoxon'
              FC_method = 'median', # or mean
             ):

    "Performs statistical tests and calculates difference between the median or mean of two groups of columns."

    group1 = df[columns1]
    group2 = df[columns2]

    # Compute median values for each gene in both groups
    if FC_method == "median":
        m1 = group1.median(axis=1)
        m2 = group2.median(axis=1)
    elif FC_method == "mean":
        m1 = group1.mean(axis=1)
        m2 = group2.mean(axis=1)

    # As phosphoproteomics data has already been log transformed, we can directly use subtraction
    FCs = m2 - m1

    # Perform the chosen test and handle NaN p-values
    if test_method == 'student_t': # data is normally distributed, non-paired
        test_func = ttest_ind
    elif test_method == 'mann_whitney': # not normally distributed, non-paired, mann_whitney considers the rank, ignore the differences
        test_func = mannwhitneyu
    elif test_method == 'wilcoxon': # not normally distributed, paired
        test_func = wilcoxon

    t_results = []
    for idx in tqdm(df.index, desc=f"Computing {test_method} tests"):
        try:
            if test_method == 'wilcoxon': # as wilcoxon is paired, if lack a paired sample, just give nan, as default nanpolicy is propagate (gives nan if nan in input)
                stat, pvalue = test_func(group1.loc[idx], group2.loc[idx])
            else:
                stat, pvalue = test_func(group1.loc[idx], group2.loc[idx], nan_policy='omit')
        except ValueError:  # Handle cases with insufficient data
            pvalue = np.nan
        t_results.append(pvalue)

    # Exclude NaN p-values before multiple testing correction
    p_values = np.array(t_results, dtype=float)  # Ensure the correct data type
    valid_p_values = p_values[~np.isnan(p_values)]

    # Adjust for multiple testing on valid p-values only
    reject, pvals_corrected, _, _ = multipletests(valid_p_values, alpha=0.05, method='fdr_bh')

    # Create a full list of corrected p-values including NaNs
    full_pvals_corrected = np.full_like(p_values, np.nan)
    np.place(full_pvals_corrected, ~np.isnan(p_values), pvals_corrected)

    # Adjust the significance accordingly
    full_reject = np.zeros_like(p_values, dtype=bool)
    np.place(full_reject, ~np.isnan(p_values), reject)

    # Create DataFrame with results
    results = pd.DataFrame({
        'log2FC': FCs,
        'p_value': p_values,
        'p_adj': full_pvals_corrected
    })

    results['p_value'] = results['p_value'].astype(float)

    def get_signed_logP(r,p_col):
        log10 = -np.log10(r[p_col])
        return -log10 if r['log2FC']<0 else log10

    results['signed_logP'] = results.apply(partial(get_signed_logP,p_col='p_value'),axis=1)
    results['signed_logPadj'] = results.apply(partial(get_signed_logP,p_col='p_adj'),axis=1)

    return results

# %% ../nbs/00_core.ipynb 99
def get_metaP(p_values):
    
    "Use Fisher's method to calculate a combined p value given a list of p values; this function also allows negative p values (negative correlation)"

    logs = [math.log(abs(p))*-1 if p<0 else math.log(abs(p)) for p in p_values]
    chi_square_stat = -2 * sum(logs)
    degrees_of_freedom = 2 * len(p_values)
    score = chi2.sf(abs(chi_square_stat), degrees_of_freedom)*-1 if chi_square_stat<0 else chi2.sf(abs(chi_square_stat), degrees_of_freedom)

    return score

# %% ../nbs/00_core.ipynb 102
def raw2norm(df: pd.DataFrame, # single kinase's df has position as index, and single amino acid as columns
             PDHK: bool=False, # whether this kinase belongs to PDHK family 
            ):
    
    "Normalize single ST kinase data"
    columns_to_exclude = ['S', 'T', 'C', 't', 'y']
    
    if PDHK:
        columns_to_exclude.append('Y')
        divisor = 16
    else:
        divisor = 17
    
    s = df.drop(columns=columns_to_exclude).sum(1)
    df2 = df.div(s, axis=0)
    df2.C = df2.C / (df2.C.median() * divisor)
    df2['S'] = df2.drop(columns=columns_to_exclude).median(1)
    df2['T'] = df2.drop(columns=columns_to_exclude).median(1)
    df2 = round(df2, 4)
    
    return df2

# %% ../nbs/00_core.ipynb 104
def get_one_kinase(df: pd.DataFrame, #stacked dataframe (paper's raw data)
                   kinase:str, # a specific kinase
                   normalize: bool=False, # normalize according to the paper; special for PDHK1/4
                   drop_s: bool= True, # drop s as s is a duplicates of t in PSPA
                  ):
    "Obtain a specific kinase data from stacked dataframe"
    
    p = pd.DataFrame(df.loc[kinase],columns = [kinase]).reset_index().rename(columns={'index':'substrate'})
    p['position'] = p.substrate.str.extract('(-?\d+)')
    p['aa'] = p.substrate.str[-1]
    p.position = p.position.astype(int)
    pp = p.pivot(index='position', columns='aa', values=kinase)
    if drop_s:
        if 's' in pp.columns:
            pp = pp.drop(columns=['s'])

    if normalize:
        pp = raw2norm(pp, PDHK=True if kinase == 'PDHK1' or kinase == 'PDHK4' else False)
    return pp
