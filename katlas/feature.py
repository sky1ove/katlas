"""A collection of tools to extract features from SMILES, proteins, etc."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/05_feature.ipynb.

# %% auto 0
__all__ = ['def_device', 'remove_hi_corr', 'preprocess', 'standardize', 'get_rdkit', 'get_rdkit_3d', 'get_rdkit_all',
           'get_rdkit_df', 'get_morgan', 'onehot_encode', 'onehot_encode_df', 'kmeans', 'filter_range_columns',
           'get_clusters_elbow', 'get_esm', 'get_t5', 'get_t5_bfd']

# %% ../nbs/05_feature.ipynb 4
import pandas as pd, numpy as np
import torch, re, gc
from tqdm.notebook import tqdm; tqdm.pandas()
from .data import Data
from sklearn.preprocessing import OneHotEncoder, StandardScaler

# Rdkit
from rdkit import Chem
from rdkit.Chem import Descriptors, Descriptors3D, AllChem, rdFingerprintGenerator

# Clustering
from sklearn.cluster import KMeans
from matplotlib import pyplot as plt

from sklearn import set_config
set_config(transform_output="pandas")

# %% ../nbs/05_feature.ipynb 6
def_device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'

# %% ../nbs/05_feature.ipynb 7
def remove_hi_corr(df: pd.DataFrame, 
                   thr: float=0.98 # threshold
                   ):
    "Remove highly correlated features in a dataframe given a pearson threshold"
    
    # Create correlation matrix
    corr_matrix = df.corr().abs()
    
    # Select upper triangle of correlation matrix
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    
    # Find index of feature columns with correlation greater than threshold
    to_drop = [column for column in upper.columns if any(upper[column] > thr)]
    
    # Drop features 
    df = df.drop(to_drop, axis=1)
    
    return df

# %% ../nbs/05_feature.ipynb 11
def preprocess(df: pd.DataFrame,
               thr: float=0.98):
    
    "Remove features with no variance, and highly correlated features based on threshold"
    
    col_ori = df.columns
    df = df.loc[:,df.std() != 0].copy()
    df = remove_hi_corr(df, thr)
    dropping_col = set(col_ori) - set(df.columns)
    print(f'removing columns: {dropping_col}')
    return df

# %% ../nbs/05_feature.ipynb 14
def standardize(df): 
    "Standardize features from a df"
    return StandardScaler().fit_transform(df.copy())

# %% ../nbs/05_feature.ipynb 17
def get_rdkit(SMILES):
    """
    Extract chemical features from SMILES
    Reference: https://greglandrum.github.io/rdkit-blog/posts/2022-12-23-descriptor-tutorial.html
    """
    mol = Chem.MolFromSmiles(SMILES)
    return Descriptors.CalcMolDescriptors(mol)

# %% ../nbs/05_feature.ipynb 18
def get_rdkit_3d(SMILES):
    """
    Extract 3d features from SMILES
    """
    mol = Chem.MolFromSmiles(SMILES)
    mol = Chem.AddHs(mol)
    AllChem.EmbedMolecule(mol, AllChem.ETKDG())
    AllChem.UFFOptimizeMolecule(mol)
    return Descriptors3D.CalcMolDescriptors3D(mol)

# %% ../nbs/05_feature.ipynb 19
def get_rdkit_all(SMILES):
    "Extract chemical features and 3d features from SMILES"
    feat = get_rdkit(SMILES)
    feat_3d = get_rdkit_3d(SMILES)
    return feat|feat_3d

# %% ../nbs/05_feature.ipynb 20
def get_rdkit_df(df,
                 col, # column of SMILES
                 postprocess=True, # remove redundant columns and standardize features for dimension reduction
                 ):
    "Extract rdkit features (including 3d) from SMILES in a df"
    out = df[col].apply(get_rdkit_all).apply(pd.Series)

    if postprocess:
        out = preprocess(out) # remove redundant
        out = standardize(out)
    return out

# %% ../nbs/05_feature.ipynb 24
def get_morgan(df: pd.DataFrame, # a dataframe that contains smiles
               col: str = "SMILES", # colname of smile
               radius=3
              ):
    "Get 2048 morgan fingerprint (binary feature) from smiles in a dataframe"
    mols = [Chem.MolFromSmiles(smi) for smi in df[col]]

    mfpgen = rdFingerprintGenerator.GetMorganGenerator(radius=radius,fpSize=2048)
    morgan_fps = [mfpgen.GetFingerprint(mol) for mol in mols]
    
    fp_df = pd.DataFrame(np.array(morgan_fps), index=df.index)
    fp_df.columns = "morgan_" + fp_df.columns.astype(str)
    return fp_df

# %% ../nbs/05_feature.ipynb 29
def onehot_encode(sequences, transform_colname=True, n=20):
    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
    encoded_array = encoder.fit_transform([list(seq) for seq in sequences])
    colnames = [x[1:] for x in encoder.get_feature_names_out()]
    if transform_colname:
        colnames = [f"{int(item.split('_', 1)[0]) - 20}{item.split('_', 1)[1]}" for item in colnames]
    encoded_df = pd.DataFrame(encoded_array)
    encoded_df.columns=colnames
    return encoded_df

# %% ../nbs/05_feature.ipynb 30
def onehot_encode_df(df,seq_col='site_seq', **kwargs):
    return onehot_encode(df[seq_col],**kwargs)

# %% ../nbs/05_feature.ipynb 35
def kmeans(onehot,n=2,seed=42):
    "Take onehot encoded and regurn the cluster number."
    kmeans = KMeans(n_clusters=n, random_state=seed,n_init='auto')
    return kmeans.fit_predict(onehot)

# %% ../nbs/05_feature.ipynb 38
def filter_range_columns(df, # df need to have column names of position + aa
                         low=-10,high=10):
    positions = df.columns.str[:-1].astype(int)
    mask = (positions >= low) & (positions <= high)
    return df.loc[:,mask]

# %% ../nbs/05_feature.ipynb 44
def get_clusters_elbow(encoded_data,max_cluster=400, interval=50):

    wcss = []
    for i in range(1, max_cluster,interval):
        kmeans = KMeans(n_clusters=i, random_state=42)
        kmeans.fit(encoded_data)
        wcss.append(kmeans.inertia_)

    # Plot the Elbow graph
    plt.figure(figsize=(5, 3))
    plt.plot(range(1, max_cluster,interval), wcss)
    plt.title(f'Elbow Method (n={len(encoded_data)})')
    plt.xlabel('# Clusters')
    plt.ylabel('WCSS')

# %% ../nbs/05_feature.ipynb 47
def get_esm(
    df: pd.DataFrame, # DataFrame containing protein sequences
    col: str, # column with amino acid sequences
    model_name: str = "esm2_t33_650M_UR50D",
    batch_size: int = 1, # Number of sequences per batch
):
    "Extract ESM2 embeddings (mean pooled per sequence)."

    # model, alphabet = esm.pretrained.load_model_and_alphabet(model_name)
    model, alphabet = torch.hub.load("facebookresearch/esm:main", model_name)
    model = model.to(def_device)
    model.eval()

    batch_converter = alphabet.get_batch_converter()

    # Infer repr layer
    match = re.search(r"_t(\d+)_", model_name)
    if not match:
        raise ValueError(f"Cannot infer repr layer from {model_name}")
    layer = int(match.group(1))

    print(f"Using ESM layer {layer}")
    print("Available models:\n"
          "esm2_t48_15B_UR50D\n"
          "esm2_t36_3B_UR50D\n"
          "esm2_t33_650M_UR50D\n"
          "esm2_t30_150M_UR50D\n"
          "esm2_t12_35M_UR50D\n"
          "esm2_t6_8M_UR50D\n")

    sequences = df[col].tolist()
    all_embeddings = []

    for i in tqdm(range(0, len(sequences), batch_size)):
        batch_seqs = sequences[i : i + batch_size]
        data = [(f"seq_{j}", s) for j, s in enumerate(batch_seqs)]

        batch_labels, batch_strs, batch_tokens = batch_converter(data)
        batch_tokens = batch_tokens.to(def_device)

        with torch.no_grad():
            results = model(batch_tokens, repr_layers=[layer], return_contacts=False)

        token_reps = results["representations"][layer]
        batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)

        for j, seq_len in enumerate(batch_lens):
            # skip BOS (0), stop before EOS
            emb = token_reps[j, 1 : seq_len - 1].mean(0)
            all_embeddings.append(emb.cpu().numpy())

        del results, token_reps, batch_tokens
        torch.cuda.empty_cache()
        gc.collect()

    df_emb = pd.DataFrame(
        all_embeddings,
        index=df.index,
        columns=[f"esm_{i}" for i in range(len(all_embeddings[0]))],
    )

    return df_emb

# %% ../nbs/05_feature.ipynb 51
def get_t5(df: pd.DataFrame, 
           col: str = 'sequence'
           ):
    "Extract ProtT5-XL-uniref50 embeddings from protein sequence in a dataframe"
    from transformers import T5Tokenizer, T5EncoderModel
    
    # Reference: https://github.com/agemagician/ProtTrans/tree/master/Embedding/PyTorch/Advanced
    # Load the tokenizer
    tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)

    # Load the model
    model = T5EncoderModel.from_pretrained("Rostlab/prot_t5_xl_half_uniref50-enc").to(def_device)

    # Set the model precision based on the device
    model.half()
    
    def T5_embeddings(sequence):
        seq_len = len(sequence)
        # Prepare the protein sequences as a list
        sequence = [" ".join(list(re.sub(r"[UZOB]", "X", sequence)))]

        # Tokenize sequences and pad up to the longest sequence in the batch
        ids = tokenizer.batch_encode_plus(sequence, add_special_tokens=True, padding="longest")
        input_ids = torch.tensor(ids['input_ids']).to(def_device)
        attention_mask = torch.tensor(ids['attention_mask']).to(def_device)
        # Generate embeddings
        with torch.no_grad():
            embedding_rpr = model(input_ids=input_ids, attention_mask=attention_mask)

        emb_mean = embedding_rpr.last_hidden_state[0][:seq_len].detach().to(torch.float32).cpu().numpy().mean(axis=0)

        return emb_mean

    series = df[col].progress_apply(T5_embeddings)
        

    T5_feature = pd.DataFrame(series.tolist(),index=df.index)
    T5_feature.columns = 'T5_' + T5_feature.columns.astype(str)
    
    return T5_feature

# %% ../nbs/05_feature.ipynb 54
def get_t5_bfd(df:pd.DataFrame, 
               col: str = 'sequence'
               ):
    
    "Extract ProtT5-XL-BFD embeddings from protein sequence in a dataframe"
    # Reference: https://github.com/agemagician/ProtTrans/tree/master/Embedding/PyTorch/Advanced
    from transformers import T5Tokenizer, T5Model
    # Load the tokenizer
    tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_bfd', do_lower_case=False)

    model = T5Model.from_pretrained("Rostlab/prot_t5_xl_bfd").to(def_device)

    model.eval()
    
    def T5_embeddings_bfd(sequence, device = def_device):
        seq_len = len(sequence)

        # Prepare the protein sequences as a list
        sequence = [" ".join(list(re.sub(r"[UZOB]", "X", sequence)))]

        # Tokenize sequences and pad up to the longest sequence in the batch
        ids = tokenizer.batch_encode_plus(sequence, add_special_tokens=True, padding="longest")
        input_ids = torch.tensor(ids['input_ids']).to(def_device)
        attention_mask = torch.tensor(ids['attention_mask']).to(def_device)

        # Generate embeddings
        with torch.no_grad():
            embedding_rpr = model(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids = input_ids)

        emb_mean = embedding_rpr.last_hidden_state[0][:seq_len].detach().to(torch.float32).cpu().numpy().mean(axis=0)

        return emb_mean

    series = df[col].progress_apply(T5_embeddings_bfd)
        

    T5_feature = pd.DataFrame(series.tolist(),index=df.index)
    T5_feature.columns = 'T5bfd_' + T5_feature.columns.astype(str)
    
    return T5_feature
