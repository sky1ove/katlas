# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03_ML_trainer.ipynb.

# %% auto 0
__all__ = ['train_ML', 'xgb_trainer', 'xgb_predict']

# %% ../nbs/03_ML_trainer.ipynb 3
from .core import Data
from .feature import *
from fastbook import *
import xgboost as xgb
import matplotlib.pyplot as plt
from scipy.stats import spearmanr,pearsonr
from sklearn.model_selection import train_test_split
from pathlib import Path
from sklearn.metrics import mean_squared_error
import math
from scipy.stats import spearmanr, pearsonr
import joblib
from sklearn.linear_model import *
from sklearn.svm import *
from sklearn.ensemble import *

# %% ../nbs/03_ML_trainer.ipynb 10
def train_ML(model, 
             X_train, 
             X_test, 
             y_train, 
             y_test, 
             save = None, # file name (.pkl) to save
            ):
    
    # Fit the model
    model.fit(X_train, y_train.values.ravel()) # better convert y_train to numpy array and flatten
    print(model)
    
    if save is not None:
        # Save the model to a file
        joblib.dump(model, save)
    
    # Predict
    y_pred = model.predict(X_test) # X_test is dataframe, y_pred is numpy array
    
    # Prepare pred dataframe
    pred = y_test.copy()
    pred['pred'] = y_pred
    
    # Calculate RMSE
    mse = mean_squared_error(pred.iloc[:,0], pred.pred)
    rmse = math.sqrt(mse)
    print(f'rmse is {rmse:.4f}')
    
    # Calculate the Spearman correlation coefficient
    spearman_corr, _ = spearmanr(pred.iloc[:,0], pred.pred)

    # Calculate the Pearson correlation coefficient
    pearson_corr, _ = pearsonr(pred.iloc[:,0], pred.pred)

    print(f"Spearman correlation coefficient: {spearman_corr:.4f}")
    print(f"Pearson correlation coefficient: {pearson_corr:.4f} ")
    
    return pred

# %% ../nbs/03_ML_trainer.ipynb 15
def xgb_trainer(df,
                feature_col,
                target_col,
                test_index=None,
                xgb_params = { 
                            'max_depth':7, #from 4 to 7
                            'learning_rate':0.001, #from 0.001
                            'subsample':0.8,
                            'colsample_bytree':1, # from 0.2 to 1, because need to take all features
                            'eval_metric':'rmse',
                            'objective':'reg:squarederror',
                            'tree_method':'gpu_hist',
                            'predictor':'gpu_predictor',
                            'random_state':123
                        },
                model_file='xgb_model.bin',
                split_seed = 123, # seed of random split
               ):
    
    X = df[feature_col]
    y = df[target_col]
    
    print(f'xgb params is: {xgb_params}')
    
    if test_index is None:
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=split_seed)
    else:
        X_train,y_train = X.loc[~X.index.isin(test_index)],y.loc[~X.index.isin(test_index)]
        X_test, y_test = X.loc[test_index],y.loc[test_index]

        
    print(X_train.shape,y_train.shape,X_test.shape, y_test.shape)
    print(y_test.index)
    #prepare matrix for xgb
    dtrain = xgb.DMatrix(X_train, y_train)
    dtest = xgb.DMatrix(X_test, y_test)
    
    model = xgb.train(xgb_params, 
            dtrain=dtrain,
            evals=[(dtrain,'train'),(dtest,'valid')],
            num_boost_round=9999,
            early_stopping_rounds=100,
            verbose_eval=100,)
    
    # Save the model
    path = Path(model_file)
    
    # Make a directory if not exists
    path.parent.mkdir(exist_ok=True)
        
    model.save_model(model_file)
    print(f'Model saved to {model_file}')
    
    # Prepare the pred/target df
    pred = model.predict(dtest)
    
    out = np.vstack([np.ravel(y_test),np.ravel(pred)]).T
    pred_df = pd.DataFrame(out,index=y_test.index, columns = ['target','pred'] )
    
    spearman_corr, _ = spearmanr(pred_df.target, pred_df.pred)
    print(f'Spearman correlation: {spearman_corr:.2f}')
    pearson_corr, p_value = pearsonr(pred_df.target, pred_df.pred)
    print(f'Pearson correlation: {pearson_corr:.2f}')


    
    fig, ax = plt.subplots()
    ax.scatter(pred_df.target, pred_df.pred)
    ax.set_xlabel('True values')
    ax.set_ylabel('Predicted values')
    ax.set_title('Scatter plot of true versus predicted values')
    plt.show()
    plt.close()
    
    
    dd = model.get_score(importance_type='gain')
    gain = pd.DataFrame({'feature':dd.keys(),f'gain_importance':dd.values()}).set_index('feature').sort_values(by='gain_importance',ascending=False)
    gain[:10].plot.barh()
    plt.show()
    plt.close()
        
    dd = model.get_score(importance_type='weight')
    weight = pd.DataFrame({'feature':dd.keys(),f'weight_importance':dd.values()}).set_index('feature').sort_values(by='weight_importance',ascending=False)
    weight[:10].plot.barh()
    plt.show()
    plt.close()
    
    return pred_df, gain, weight

# %% ../nbs/03_ML_trainer.ipynb 18
def xgb_predict(df, # a dataframe that contains ID and features for prediction
                feature_col, #feature column name
                ID_col = "ID", #ID column name
                model_file='xgb_model.bin'):
    # Load the XGBoost model
    model = xgb.Booster()
    model.load_model(model_file)
    
    # Prepare data for prediction
    X = df[feature_col]
    dtest = xgb.DMatrix(X)
    
    # Make predictions
    preds = model.predict(dtest)
    
    # Combine predictions with IDs into a DataFrame
    result_df = pd.DataFrame({ID_col: df[ID_col], 'preds': preds})
    
    return result_df
